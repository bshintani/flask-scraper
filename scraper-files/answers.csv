Post ID,Answer ID,Answer,Vote Score,Date,User ID
1,1,"
If your devices need Internet access, use a three router Y-configuration to separate your 'ordinary' (home/work) network from the one containing your IOT devices.
There are plenty of resources about that on the Internet.
This article goes in-depth describing the disadvantages of simpler configurations, leading up to the Y concept. The author even has a walkthrough showing how he set this up for himself.
Note: Many modern router devices let you mimic this configuration with one box only, but talking about three physical ones makes you understand it better. You may even have some old routers laying around that you now have a purpose for.
",1,2018-02-05 20:59:37Z,0
1,2,"
You start by not giving them access to the Internet.  If your Intranet of Things devices have no direct contact with the outside world (eg. your IoT sprinkler controller is firewalled off from the Internet and needs to go through a proxy when requesting a weather forecast), it becomes much harder for an attacker to do anything.  If there's no connection whatsoever, any attacker needs to be physically present.
",2,2018-02-02 22:42:21Z,3
1,3,"
You didn't mention the first rule of IoT security. Change the admin password. If possible make it complex. I have witnessed hundreds, if not thousands, of penetration attempts and most of them are attempting to compromise some form of administration account with guessable passwords. 
Another thing that you can do is register it with a directory. You may have AD available to you but there are also dozens of online cloud versions available too.
If you are going to have multiple devices this can simplify any administrative
activities as well.
",1,2018-02-05 17:36:33Z,4
1,4,"
IoT is a loaded term.  I'm going to assume the topic is about ""How to keep self-configured IoT devices secure"" and disregard cloud-based IoT devices.
The best way to learn about securing IoT devices is to learn how generic security works.  IoT devices are by definition, internet connected devices.  They are no different than computers, servers, network printers, routers, switches, etc.  They face all the same problems that those devices face when it comes to security.
You can find a ton of information on how to secure network infrastructure online.  There are many other resources as well, such as academic classes.
I personally, run a firewall on my router that segments my network and white-list locations I access it from.
To disregard my disregard, cloud-IoT devices are at the mercy of their cloud services.
",1,2018-02-05 20:10:00Z,2
1,5,"

Lastly as an importart part of the threat model is physical security, because if I put a sensor outside with a solar panel to power it, how do I stop mischievous kids or crooks from destroying or stealing the technology?

You, essentially, have to build a better mouse trap. Consider your physical security situation - is the street side of your house safe, but kids out of view of the streets will wreck things? Perhaps the opposite is true - position the sensors within view of the street (or vice versa). Next, consider replace-ability - is this a cheap sensor or a $150 weather station type deal? Consider pairing down the features of outdoor sensors to lower the cost of replacement. If it takes you five minutes and five dollars to replace the sensor, who cares if it's broken or stolen every three months. Thirdly, consider hardening - is your sensor on the top of a 30 ft roof? Is it resistant to a baseball or rock? Do you need to walk up to it and wail on it with a golf club for a few minutes to break it? Can you make it that resistant? 
All security is about making compromise ""not worth it"" for the attacker. It's hard to overcome the ""worth it"" factor for teenagers - the destruction itself is the reward, and making a harder target makes the destruction more fun. You'll have to think critically about what you need to do to make the sensors unlikely to be damaged, or make them easier to replace when damage occurs.
",1,2018-02-05 20:34:31Z,5
2,1,"
Be very, very careful. It's not KRACK that is the problem, it is a lax attitude to security and privacy in general. So called ""smart"" consumer products can often be hijacked, accessed from the internet, or monitored. As a customer, it is hard to know if any specific product is safe or not.
The Norwegian Consumer Council has been on the case for a while, and produced a few horror stories. From a report, aptly titled #ToyFail, on three ""smart"" dolls:

When scrutinizing the terms of use and privacy policies of the connected toys, the NCC found a general disconcerting lack of regard to basic consumer and privacy rights. [...]
Furthermore, the terms are generally vague about data retention, and reserve the right to terminate the service at any time without sufficient reason. Additionally, two of the toys transfer personal information to a commercial third party, who reserves the right to use this information for practically any purpose, unrelated to the functionality of toys themselves. 


[I]t was discovered that two of the toys have practically no embedded
  security. This means that anyone may gain access to the microphone and speakers
  within the toys, without requiring physical access to the products. This is a
  serious security flaw, which should never have been present in the toys in the
  first place.

And from an other of their reports, again aptly named #WatchOut, on ""smart"" watches for kids:

[T]wo of the devices have flaws which could allow a potential attacker to take control of the apps, thus gaining access to children’s real-time and historical location and personal details, as well as even enabling them to contact the children directly, all without the parents’ knowledge.
Additionally, several of the devices transmit personal data to servers located in North America and East Asia, in some cases without any encryption in place. One of the watches also functions as a listening device, allowing the parent or a stranger with some technical knowledge to audio monitor the surroundings of the child without any clear indication on the physical watch that this is taking place.

And the FBI agrees:

Smart toys and entertainment devices for children are increasingly incorporating technologies that learn and tailor their behaviours based on user interactions. These features could put the privacy and safety of children at risk due to the large amount of personal information that may be unwittingly disclosed.

So unless you have a real need (other than ""this is cool"") for these kinds of products, I would say that your best approach is to simply stay away from them.
",92,2017-10-31 15:13:39Z,9
2,2,"
It really depends on your threat model.  I wouldn't be particularly worried about a particular sexual predator in your local area having the technical skills necessary to utilize Krack to inject voice into the toy. Unless it uses the vulnerable Linux driver, the key clearing won't work and the partial nature of the compromise for a general reset would make voice injection nearly impossible.  
Similarly, as a client device, it doesn't offer a whole lot of security risk other than possibly as a listening device, depending on if it is always on or activated by pushing a button.  Krack wouldn't make it usable as an entry point in to your network directly, so I don't see it as a particularly riskier device than any other IOT device.
As always in security, it comes down to your risk aversion though.  Personally, if I thought it would be valuable to my child (who is also 3) I don't think I would consider the local security implications as a reason not to get it for my home environment.  I'd be more concerned about the controls and security on the web side.  
My main concern for IOT devices isn't the local compromise so much as the web connected remote compromise.  The chances of a sufficiently skilled and motivated malicious individual in your direct proximity is pretty low.  The chances of a motivated and malicious user on the Internet trying to remotely access the IOT device is significantly higher and it's important to understand what holes the devices punch in your network protections.
Also, as Michael was kind enough to point out, the interests of such a broad hacker are much less likely to be concerned with your privacy and much more likely to either be interested in attacks on your other computers or on the computational capabilities of the device as an attack bot.
",15,2017-10-31 15:04:35Z,11
2,3,"
Welcome to the Internet of Things(IoT). This is a... thing. Therefore, it can be assimilated

Mirai is a type of malware that automatically finds Internet of Things devices to infect and conscripts them into a botnet—a group of computing devices that can be centrally controlled.

And

One reason Mirai is so difficult to contain is that it lurks on devices, and generally doesn't noticeably affect their performance. There's no reason the average user would ever think that their webcam—or more likely, a small business's—is potentially part of an active botnet. And even if it were, there's not much they could do about it, having no direct way to interface with the infected product.

The problem is that security is seldom a consideration when making toys like this. The technology to make all this work is fairly simple, but the companies aren't paid to think about this. It's a child's toy. It's meant to be cheap and easy. And you get what you pay for. 
Earlier this year, it was found that a similar child's toy had no security at all (emphasis mine)

A maker of Internet-connected stuffed animal toys has exposed more than 2 million voice recordings of children and parents, as well as e-mail addresses and password data for more than 800,000 accounts.
The account data was left in a publicly available database that wasn't protected by a password or placed behind a firewall, according to a blog post published Monday by Troy Hunt, maintainter of the Have I Been Pwned?, breach-notification website. He said searches using the Shodan computer search engine and other evidence indicated that, since December 25 and January 8, the customer data was accessed multiple times by multiple parties, including criminals who ultimately held the data for ransom. The recordings were available on an Amazon-hosted service that required no authorization to access.

I'm going to be honest. These things are scary powerful in what they can do. Even if it doesn't expose your messaging, it could still be used for something malicious like a DDOS attack. If I were you, I'd pass on anything like this unless there's something explicit about security.
",12,2017-10-31 18:09:50Z,13
2,4,"
This is pretty much the same kind of toy as CloudPets. Those were toys that allowed talking with the children (toy) by using a mobile app. The security was terrible. It turned out that both the user details and the pet recordings were available on databases with no password. And the company didn't even answer to the mails alerting them of the vulnerabilities.
You can view about this terrifying story on Troy Hunt blog: https://www.troyhunt.com/data-from-connected-cloudpets-teddy-bears-leaked-and-ransomed-exposing-kids-voice-messages/
Now, Talkies may actually have made the right choices (it's hard to do so many things wrong as CloudPets did!), but this showcases the level of security of this sector.
So no, I wouldn't trust this toy with my kids data. Not to mention how the toy itself could be compromised (eg. like Hello Barbie).
In fact, Germany went so far to ban the internet connected Cayla dolls over fears they could be exploited to target children: https://www.telegraph.co.uk/news/2017/02/17/germany-bans-internet-connected-dolls-fears-hackers-could-target/
",6,2017-11-01 17:29:50Z,14
2,5,"
Internet-enabled anything poses a risk.  As a rule, security is an expense and consumers as a whole really don't consider product security when making purchasing decisions.  For example, there was a thread on Reddit recently about a couple who got divorced and she didn't change the password on the Nest thermostat.  So while she was out, he would crank up the air conditioning or heat and cause massive utility bills.  We also know of baby monitors that have been used to listen in on neighbors without their consent.  I've attended IT security demos of internet-connected light switches, showing how easy it was to attack them.
krack is important, definitely, but when compared to a non-existent security posture is irrelevant.  Quite simply, if someone is concerned about security, I'd suggest not purchasing networkable anything unless they can identify a need for it to have a network connection and they have the skills to properly secure both it and their network.
WRT your trusted circle of phones: how often would you plan on managing that list?  What is the means to join that trusted circle?  Do you know when your friends sell their phones back so you can decommission them from your circle? (If your answer is not ""no"" to the last one, you're probably not being realistic with yourself.)
Encourage creativity.  Build skills.  Get the kid a bunch of building blocks or a train.  Get a Spirograph.  Play cards/games with them.  Find something that they will play with for hours that doesn't require your constant attention.  
",4,2017-10-31 17:22:04Z,15
2,6,"
This puts the ""IOT"" in ""IDIOT!"".
Most of the companies that make these have no clue how to prevent hackers from taking them over, sometimes programming comically stupid/obvious exploits into them.
The KRACK exploit might be irrelevant half the time since most of these manufacturers wouldn't figure out how to implement some form of encryption.
Any type of internet-enabled voice recording is potentially creepy and downright dangerous invasion of privacy. These devices likely use the cloud for sound processing and storage considering they are almost certainly based on low-grade ARM chips and minimal cheap flash storage at most.
Even if the device is properly made, there's no similar guarantee on the cloud app it uses. You'd be surprised how often researchers happen to stumble on valuable leftover data in the cloud that the previous user of a logical machine instance failed to clean up.
",2,2017-11-01 04:44:14Z,16
3,1,"
Some people argue that code which is open source can be audited by many and therefor contains little bugs. On the other hand, attackers have the same easy access and also look for these same vulnerabilities. There is definitely a tradeoff here which is not correctly described in previous answers.
Others mention that code should be inherently secure and therefor requires no obfuscation/encryption/hiding. It is true that a system should be designed to be secure even if you know how it works. This doesn't mean that this is always the case AND the implementation is flawless. In practice, code is never 100% secure. (Take a look at web app security: Why do we need security headers to protect us against XSS and CSRF attacks if there are no vulnerabilities in the web application?) Additional security measures can be taken by trying to hide the code through encryption and obfuscation. In the mobile world, reverse engineering is even seen as a serious risk: OWASP Mobile Top 10 risks. 
As no system is 100% secure we can only try to increase the effort required to break it. 
So now, the tradeoff between open source/easily available code VS encrypted and obfuscated code. Allowing public review on you source code can help reduce the number of bugs. However, if you are a small company where the public has little incentive to freely audit your code, there is no benefit from publishing your code as nobody will look at it with good intentions. However, it becomes much easier for attackers to discover vulnerabilities. (We are not talking about the newest iOS version which every security researcher is trying to crack)..
In this case we aren't even talking about open sourcing the code for public review. We are talking about encrypting the firmware in transit. Security researchers are not likely going to buy your device to obtain the code to discover and publish vulnerabilities. Therefor the chance of having the good guys finding the vulnerabilities VS the bad guys finding them decreases.
",5,2017-09-05 07:50:19Z,18
3,2,"
No. You should not rely upon the obscurity of your firmware in order to hide potential security vulnerabilities that exist regardless of whether or not you encrypt/obfuscate your firmware.
I have a radical suggestion: do the exact opposite. Make your firmware binaries publicly available and downloadable, freely accessible to anyone who wants them. Add a page on your site with details on how to contact you about security issues. Engage with the security community to improve the security of your product.
",86,2017-09-01 13:38:54Z,21
3,3,"
Doubtful it would be beneficial. It is by far a better option to push it open-source than closed source. It might seem silly and even controversial at first, but opening up a project to the public has plenty of benefits.
While there are people with malicious intents, there are also people wanting to help and make the internet a better place. Open source allows more eyes to look over the project, not only to view about potential features, bugs, and issues but also increase security and stability of the ""thing""
And to agree with Polynomial's answer, engaging in a community and building a base of people that help you out with security, will increase the client base by a significant margin.
",11,2017-09-01 13:52:35Z,22
3,4,"
A well-designed firmware should rely on the strength of its access key rather than relying on the attacker's ignorance of the system design. This follows the foundational security engineering principle known as Kerckhoffs's axiom: 

An information system should be secure even if everything about the
  system, except the system's key, is public knowledge.

The American mathematician Claude Shannon recommended starting from the assumption that ""the enemy knows the system"", i.e., ""one ought to design systems under the assumption that the enemy will immediately gain full familiarity with them"".
You may be interested to know that prior to the late Nineteenth Century, security engineers often advocated obscurity and secrecy as valid means of securing information. However, these knowledge-antagonistic approaches are antithetical to several software engineering design principles — especially modularity.
",6,2017-09-01 18:33:32Z,24
3,5,"
Are you sure you are not confusing two cryptographic methods?
You should certainly sign your firmware updates for security reasons. This allows the device to verify they are from you.
To encrypt them adds a little bit of obscurity and that's it. Since the decrypting device is not under you control, someone sooner or later will hack it, extract the decryption key and publish it on the Internet.
",3,2017-09-02 05:49:21Z,25
3,6,"
You need to ask yourself this question.  Is someone clever enough and interested enough to download your firmware and start looking for vulnerabilities going to be deterred by an additional firmware encryption layer where the key must be revealed?
This is just another hoop to jump through, no different than figuring out what disk format your firmware image is in.  This isn't even a particularly difficult hoop to jump through.  Keep in mind that all FAR more sophisticated methods of what amounts to DRM have all been broken.
Odds are someone determined enough to hack your internet connected coffee maker/Dishwasher isn't going to be deterred by an additional encryption layer.
",0,2017-09-01 19:40:07Z,26
3,7,"
In the sense of ""will the encryption of firmware prevent the detection of vulnerabilities in my code?"" other answers have addressed the core of it: although it may discourage some attackers, security through obscurity leads to a false feeling of invulnerability that is counterproductive.
However, I'd like to add an extra bit on the basis of my experience. I have seen that the firmware packages are sometimes encrypted, but the motivation for that is only to preserve a company's intelectual property, rather than be a control against attackers.
Of course, hackers often find ways around this ""control"", but that's a different story.
",0,2017-09-08 06:53:14Z,27
3,8,"
Several people said you shouldn't rely on obfuscating the code by encrypting it, but to just make it secure. Quite recently the encryption of some rather critical software in Apple's iPhones was cracked (meaning that hackers can now see the actual code, no more). It stopped anyone from examining the code for three years, so the time from release to first crack has been increased by three years. That seems like some very successful obfuscation. 
And encryption goes very well together with code signing. So when your device is given new firmware, it can reject any fake firmware. Now that part isn't just recommended, that is absolutely essential. 
",-3,2017-09-01 21:03:46Z,28
4,1,"
Almost all IoT devices of the sort we are talking here (ie, consumer level) are deployed behind home grade routers (or ARE home grade routers) and are using IPv4 and NAT/uPNP to reach the internet.  That means they are sharing an IP with legitimate traffic for one and for another are using a dynamic IP, not a static IP, meaning there's even more opportunities for innocent parties to be caught up in such a blacklist.
Once the glorious world of IPv6 comes, we can likely do just that - assign each one a routable IP address and blacklist them as they get shown to be vulnerable.
One legitimate thing that CAN be done is for ISPs to perform source IP filtering - that is, don't let spoofed packets out, only let packets from IPs within the network.  That will help. Some.
",14,2016-10-26 17:08:31Z,33
4,2,"

wouldn't it make since to start giving IOT devices a static IP

No, because IPs only work in the proper network context.
An device's IP needs to be routable, which is to say, sitting on a network with routers that know how to communicate with other networks and are willing to do so on behalf of the device.
First of all, if you hardwire IPs and send them out, you'd have to use RFC1918 IPs (non-routable) to avoid conflicting with IPs owned by other people.  So you're already stuck with a narrow slice of addresses you can use.  And those IPs would need to be translated when they hit the Internet, thus removing the ability of the target to filter on them.
Second of all, you'd be handing people devices with IPs that may or may not match their networks.  They wouldn't want to rework their networks to accommodate your cheap IOT devices.
In short, a device's IP is set to allow it to work on the network.  Setting the IP and expecting the network to work with it breaks things.
",6,2016-10-26 17:07:52Z,35
4,3,"
The way IP addressing is done for IOT devices doesn't matter in a DDOS attack. Here is the reason why.

IOT devices may not always be exposed to network with an external IP address. 
It can also be device behind a NAT which has only an internal IP
address.  
So irrespective of whether it is DHCP or some static IP, provided by  the router to the device, from the DDOS attack victim's
perspective, the source of the attack is the external IP of the
router. 
In other words, if you have an IP cam at home, The packets from your IP cam as well as your laptop will have the same source IP address, ie the public IP address of your router. 
So filtering the packets based on source IP of the packet can deny service for your laptop as well.

",4,2016-10-26 18:06:32Z,36
4,4,"
No, as dynamic addressing isn't the attack vector when IoT devices get abused. It's poorly written software! On the DoS side, we have the 'underpowered' device vs. powerful computer problem, and on the connectivity side we have the ease of use vs. security problem.
Ironically, the only way to do proper IoT is to not connect them to the internet! At least a gateway plus locally configured outgoing tunnel would be needed before they can be connected at some sort of safe level.
",1,2016-10-26 17:49:55Z,37
4,5,"
A static IP would not help; as crovers mentions, due to ipv4 exhaustion, its just not practical. Although IPV6 provides a large enough address space for each device to have a staic IP address, there will still be a lit of overlap with the IPV4 world - where a server will just see lots of traffic arising from a point of presence for IPV6 nodes.
But there are a lot of things manufacturers and governments can do (to make up for the lack of skills on the part of the buying public). Blocking traffic ther than from/to the local network on an appliance by default, forcing a change of admin password before the device, using encryption appropriately, supporting open standards.
The goverment part is more tricky - the obvious solution isproviding an accreditation framework which is easily auditable but minimally invasive with minimum standards, the problem is that they tend to listen to either the ""experts"" advocating (for example) that all such devices should be fips-140 certified or the vendors bleating about customer freedom and over-regulation rather than seeking pragmatic solutions.
Maybe someone might set up an IETF to look at the classification problem.
",1,2016-10-26 18:12:42Z,38
5,1,"
I would suggest that rather than trying to protect the camera from the outside world with a firewall that using a black hole router as the only gateway for these devices would be more restrictive - also if you have a physical device at the router's address then it would make monitoring of egress attempts more visible. Also, point the device to the same black hole for DNS queries.
If you need remote access then use a VPN with NAT.
",3,2016-10-25 15:28:05Z,38
5,2,"
The following controls should help secure your device:

Change the root password (if possible: See @crover's comment for more info)
Segment the network so that only the host administering the camera can connect to it
Block egress from the network from the camera's IP
Apply any vendor supplied patches or configuration changes

",2,2016-10-25 15:17:20Z,40
5,3,"
There are a few things to do to protect your camera:

Make sure you’re not using the default password. This way, even if you’re discovered, it’ll be harder to break in. Especially change any root passwords, where possible.
Incapsula has a Mirai vulnerability scanner, which scans your IP to see what may be at risk. https://www.incapsula.com/mirai-scanner.html They also offer a Web Application Firewall (WAF) to put your device behind.
Disable any unnecessary remote connections. 
Run the most up-to-date firmware to ensure discovered security vulnerabilities are patched. But realize that there are many unpatched vulnerabilities that remain. This is something that we are less likely to do for devices than our own home/work computer. 
Reboot and make sure everything still works.

And, as you mentioned, in the wake of Mirai, understand that your device is at risk even if you personally  aren’t a target.
",1,2016-11-13 13:19:44Z,42
5,4,"
Other than nagging your vendor for an update or replacing the cameras with something more secure, the only other option would be putting them on their own network, with limited and controlled access via another router.  You can do this with some higher-end switches which support segmentation or you can just plug them into their own switch, with a firewall/router between, which limits access to those cameras only to the 'secured' ports.
",0,2016-10-25 15:17:33Z,33
5,5,"
I would put them on a separate network segment with no access to the Internet. The only access would be through a VPN gateway. You not only protect them from attacks but also secure your connection to them (as those cheap cameras don't support HTTPS).
In the future I would also recommend buying cameras from a reputable vendor with a good security track record (a network company such as Cisco/Ubiquiti).
",0,2016-11-13 14:32:07Z,43
6,1,"
The devices are designed to be accessible from outside the home. To offer this service to their owners, they make themselves accessible through the homeowner's router/firewall. The way they do this is by sending a UPnP packet to the owner's router that tells the router to open a port that connects back to them. They then listen for connections that arrive directly from the internet.
In other words, the devices first hacked their owner's routers by design, which exposed their own vulnerabilities.  (This has nothing to do with secured, private, or open WiFi, other than many IoT devices connect via WiFi; UPnP exposes the exact same vulnerabilities on wired devices connected by Ethernet cables, too.)
To protect yourself, disable UPnP on your router.
",84,2016-10-25 02:59:31Z,48
6,2,"
Your understanding of the attack is not as clear as you think. In this article, Krebs mentioned that the attackers didn't really have to hack the devices. The vulnerability was well known, they just had to scan the internet for those devices.
Sure, if SSH/Telnet to the devices was disabled, the problem would have been solved easily. To make the matter worse, the hard coded credentials present in the hardware were not even visible to the web interface for the administrator.
Yes, it is absolutely imperative to know what are the devices present in your network and what are the services that you do/do not need.
EDIT : After @tlng05 's clarification about the question.
As already mentioned in other answers, you should disable UPnP on your router to absolutely make sure that your device is not straight forward configurable from the outside world.
",15,2016-10-25 03:03:16Z,51
6,3,"
Your misconception is here:

secured private wifi networks

Whilst many home WiFi networks are secured against unauthorised wireless devices connecting directly, many are wide open to access from the wider Internet.  It's this access (that's demanded by the IoT devices to perform their legitimate functions) that can be abused (and on a much bigger scale than physically visiting many WiFi networks).
The attack surface of a router is on both all networks!
",10,2016-10-25 09:00:47Z,53
6,4,"

What I don't understand is although easily hacked, most IoT devices are connected to secured private wifi networks.

Yes they are connected to your private wifi networks, But are they secured? Well not so much as pointed by you these device are unprotected by firewalls, IPSs unlike the enterprise networks. Some of them have ancient firmwares, which haven't been updated since ages. And yes some have default passwords still working, So that anyone can easily take access and exploit them for attacks.

So is it assumed that these thousands of IoT devices' networks were hacked first, then the device itself was hacked?

Well not necessarily, Although it may be possible in some cases. But mostly these devices are intentionally left exposed to the internet because they are needed to be accessed from anywhere around the world.
As pointed out by many examples above, If you want the CCTV footage of your house mostly you would want it live streamed on your handheld device and that is why they are needed to be accessible over internet. They are N number of other examples. 
Conclusion: To use IoT devices to attack, one doesn't need access to your network. These devices can be directly accessed from internet. What we need to do is protect these devices from such un-authorized accesses and keep our devices safe without having to use expensive devices like firewalls and IPSs.
",2,2016-10-25 08:26:14Z,55
6,5,"
UPnP can be an issue, but everybody seems to be missing the point that many of these devices make persistent standard outgoing NAT connections to the vendors' servers. All the attacker has to do is hack into the vendor's site to gain control of all of the attached IoT devices, and from there, since they are now inside home networks, to attack other computers inside the network or launch DDoS attacks. Direct HTTP, SSH or other UPnP-enabled access through your router isn't necessarily a requirement. 
",2,2016-10-25 14:45:22Z,57
6,6,"
While IoT devices are indeed within secure networks, they are largely made such that they are accessible from the internet. For example, the temperature setting of your home is accessible from your phone app when you're at work. This is enabled by a connection being opened up to the internet. This answers why they're able to access the outside world. 
Now, most IoT devices, or botnets, are not well patched and use loose security configurations. Parts 1 and 2 of the article found here explain this in detail, but to summarize, these devices are infected with malware. They are able to send outgoing messages to the internet (the outside world). And thus, they end up sending the ""DoS"" message to the target. 
",1,2016-10-25 03:14:10Z,58
6,7,"
Most IoT devices are on networks that are connected to the Internet by conventional SoHo NAT routers that typically have very limited firewall capabilities or where the firewalls are not enabled or maintained. There is a common myth that NAT is a security layer, it is not.

""NAT and firewalling are completely orthogonal concepts that have nothing to do with each other. Because some NAT implementations accidentally provide some firewalling, there is a persistent myth that NAT provides security. It provides no security whatsoever. None. Zero."" -- How Important is NAT as a security layer?

",1,2016-10-25 18:46:34Z,30
6,8,"
It may be worthwhile thinking about terminology and what is meant when people say that IoT things have been 'hacked'. In many cases, the devices have not been hacked at all - they are performing as designed. 
Broadly speaking, there are two types of network connections. The first type is a fully connected type connection where both parties need to be fully connected. Similar to a phone call, you need to have someone on both ends. With this type of connection, the initiating system makes an initial connection to the destination system and the destination system connects back to the initiating system. This type of connection is what normally occurs when it is important to be able to coordinate communications, track data packet order and request re-sending of any lost data. 
The other type of connection is more like a messaging connection (think of SMS or some other messaging In this type of connection, you don't have a bi-directional connection. The originating system sends a message to the destination system and, depending on the message, the receiving system may send back a response to the sender address in the initial message. This type of communication is good when order of data, loss of some data etc is not critical. 
The thing is, while fully connected connections are great for things like data integrity and because of the bi-directional nature, are difficult to spoof, they are more expensive in terms of resources and overhead. The second type of connection has less integrity and is easier to spoof because there is no bi-directional connection, but they are cheap - require less resources and have lower system overheads to process. 
Many IoT systems are small, lightweight and need to be efficient. They typically have less memory and less powerful processes and therefore tend to favour designs which use connecitonless protocols rather than more expensive connected protocols. However, this also means that it is easier for rogue systems to 'lie' and do things like spoof IP addresses. This is like me sending you a message, where the return address is false. When you reply to the message your reply will go to the address in the message, but that is not the real originating address. 
In effect, what is happening is that the IoT devices are being folled into sending data/responses to an innocent bystander who has not requested anything. The system has not been 'hacked', only fooled. 
Often, the situation can be made worse by using amplification techniques. There are some connectionless type services out there which, when asked a vary simple/short question, will respond with a vary long answer i.e. answers with lots of data. This can make it vary easy to create a situation where suddenly, a victim site (such as a DNS) suddently starts receiving large amounts of data it was not expecting or did not ask for. 
to do this, all you need to do is identify devices on the internet which support a connectionless protocol, send these devices a message which requests something which is likely to involve a large data response and spoof the IP address of the targeted victim. 
to make it worse, the targeted system doesn't even need to know or understand the data being sent to it. The idea is to just send so much data that the system becomes overwhelmed - that could happen when the system is forced to look at large amounts of incoming data simply to make a decision to discard it and take no further action. With enough data, even that process of working out you need to just ignore it can be enough to prevent the system from being able to process legitimate connections.  The fact that this data is comming from multiple different source systems i.e. all the IoT deices means you cannot just block an IP address because there are simply too many. 
So, while it is vary true there are far too many IoT devices which ahve been poorly designed and lack sufficient security controls, a part of the problem is the conflicting requirements to implement a light-weight resource efficient solution on one hand, but somehow deal with a world with too many malicious agents who want to exploit your good intentions. There is certainly a lot IoT vendors could do to improve the situation, but for most of them, this would just increase production costs and the reality is, most consumers are not aware of the issues, so failing to invest in the better solution doesn't affect market share and therefore doesn't result in sufficient financial benefit.  
",1,2016-10-27 23:11:01Z,60
6,9,"
XM actually disabled telnet/ssh on many of their IoT devices (they supply many for new DvRs, webcams, etc.) more than a year ago. So anyone who had actually updated the firmware (who knows) or had bought a more recent model of (IoT device whatever) since then likely would have been immune from that sort of attack.
My understanding is the Mirai (not sure about the other popular one Bashlight) connected to most IoT devices through GRE - an IP point-to-point virtual tunnel. GRE is kind of like a VPN of packet delivery - it can pass data through public network privately - without the actual data/headers being identifiable and with almost no protocol overhead. So once you have a master list of exploitable cams, home sec, connected whatever devices and the models, you can scan the whole internet and tunnel IPs accessible through open ports, run against passwords, etc etc. Hard for people to see it coming because GRE looks like regular IP transmission between devices calling home or streaming home video to app, etc. This is just my take...
",0,2016-10-26 01:31:37Z,61
6,10,"
New in this forum I thought I'd chime in from a hobby IoT device maker's perspective. I may very well be off topic, not least since I am not entirely sure what you guys even consider to BE an IoT device, but for what it's worth:
The ""IoT devices"" I create, which do useless things such as report whether or not someone has moved within a certain area in a certain time, could easily be ""hacked"" without accessing my WiFi. You could probably just put up a receiver of the right sort (we could be talking 433MHz) and eavesdrop all day. Then you could craft your own messages and send them to my stupid device and/or the server that collects that information, and have me running home in panic since my not-so-smart-home system says it's 200 degrees Centrigrade in my fridge and five thousand people have passed into my garage but noone came out.
Basically what I'm saying is that whatever flaws the IoT devices hardware exposes directly, and it's software doesn't guard against, could be an entry port for a hacker. Heck, based on where the device is placed you could even attach your own hardware to it and start making trouble. ""Here's my WiFi enabled ESP8266, go ahead and upload your own software to it via USB."" But I guess that's really out of scope.
",0,2016-10-27 11:40:59Z,62
7,1,"

... but they used their own infrastructure

It's not really their own infrastructure what they use. They use instead botnets consisting of  hijacked systems. These are systems which they p0wn but definitely not own. And thus it is very cheap for them.
Apart from that any VPS provider who would rent their VPS for DDoS attacks would quickly lose reputation and thus proper customers. And if a VPS provider then specializes on providing VPS for DDoS attacks to make up for the loss of normal customers it would be more easy to block such DDoS because they all origin from the same networks, i.e. simply cut off this provider from having access to major networks.
",40,2017-04-25 04:54:20Z,66
7,2,"
Cloud based DoS attacks are possible, and they do happen from time to time. But it's not a very popular option for a couple of reasons:

Initial setup - Deploying hundreds of VMs is not an easy feat, and paying for them isn't simple either. However if you're using someone else's VM, then this makes things a lot easier. 
Detection - Many providers including Azure monitor their services to check for any malicious activity. In fact, launching attacks from their systems violates their ToS, and will have you shut down very quickly.

However, the ability to have thousands of machines spread over the world, each generating some traffic can be very powerful. If you want to take it a step further, tunnel your traffic through the Tor network, to make it nearly impossible for a defender to stop. 
It's been done before though:

In 2012 group of cyber-criminals exploite  the CVE-2014-3120 Elasticsearch 1.1.x vulnerability, followed by the use of Linux DDoS Trojan Mayday and with that, they compromised several Amazon EC2 Virtual Machines. Although this vulnerability was not unique to cloud-based systems and could have been used against any server, including non-cloud based systems, it did open up some interesting opportunities to the attackers. They were able to launch a UDP based DDoS attack from the compromised cloud instances. They utilized the outbound bandwidth of the Cloud Service Provider, Amazon in this case. 
  Source: Infosec Institute

Getting caught - this is more difficult. Creating and deploying VMs these days is as easy as signing up for an anonymous email ID, registering and deploying machines. However, providers will notice large amounts of traffic from a system. Since you're​ violating their ToS, they will nearly always shut you down immediately. However since you're not ever revealing your actual identity (assuming you're accessing their services through an anonymizer and using stolen credit cards (no morals ;] )), they mostly will not be able to discover your real identity. But this does mean that you're flushing your money down the drain - which is why it's just simpler to set up your own infrastructure and offer it as a service. 
",9,2017-04-25 04:48:58Z,70
7,3,"
The funny thing is that what you describe is available right now. It's called Mirai, it's more or less open source, and chances are you've already been affected by it

Mirai is a type of malware that automatically finds Internet of Things devices to infect and conscripts them into a botnet—a group of computing devices that can be centrally controlled. From there this IoT army can be used to mount distributed denial of service (DDoS) attacks in which a firehose of junk traffic floods a target’s servers with malicious traffic. In just the past few weeks, Mirai disrupted internet service for more than 900,000 Deutsche Telekom customers in Germany, and infected almost 2,400 TalkTalk routers in the UK. This week, researchers published evidence that 80 models of Sony cameras are vulnerable to a Mirai takeover.
These attacks have been enabled both by the massive army of modems and webcams under Mirai’s control, and the fact that a hacker known as “Anna-senpai” elected to open-source its code in September. While there’s nothing particularly novel about Mirai’s software, it has proven itself to be remarkably flexible and adaptable. As a result, hackers can develop different strains of Mirai that can take over new vulnerable IoT devices and increase the population (and compute power) Mirai botnets can draw on.

There's lots and lots of IoT devices flooding the market. Everyone wants ""smart"" technology. But, as is usually the case, security is an afterthought. So we put that device out there and it's on the Internet for anyone to contact and use as they see fit. And most users (and indeed ISPs) probably won't notice 

Fast forward another 45 minutes. The router was reset, and the network was set up again. By the time I was done messing around, Peakhour had my traffic clocked at 470GB. But I'd gotten rid of the problem (or so I thought). The next morning, before I left for the weekend, I checked: the total traffic was at around 500GB. Maybe I'd defeated the hackers.
That night, I heard from Donna. She'd been monitoring traffic, which was now over 3TB. And, just to make sure we had no doubt, devices were dropped off the network again.

The tipoff that something was amiss? His phone used all of it's 4G allotment despite his being at home. His ISP never batted an eyelash at 3TB of bandwidth consumed

Ultimately, you can count on people who are technically illiterate and companies that don't care to provide all the botnet devices you'll ever need.
",2,2017-04-25 16:38:18Z,13
7,4,"
Cloud providers generally require their customer's identity. If an enterprising young hacker wished to rent Amazon Web Services or the like, they would have to provide a credit card number (or more) to the service, which can be traced back to the owner. Cloud services don't want to engage in DDOS because their networks would be blocked, and it would cost them money in bandwidth. 
There are services where you can rent a VPS anonymously in Bitcoin, but they are generally smaller and they also don't want to be blocked by their uplink or peers. 
So that is why it isn't common. DDOS generally considered anti-social behavior, and sociopaths only make up 4% of the population.
",2,2017-04-25 17:11:10Z,72
7,5,"
The key to your question is in the first 'D'. 
What makes a DDoS attack so effective is the distributed nature of that
attack. With an old style DoS attack, the victim would usually experience a
large number of requests or connections to a specific server or resource
originating from a single or small number of sources. To mitigate the attack,
you could simply block the traffic from the attacking systems. Often, this could
be done by the local firewall or similar. 
Under the DDoS attack, the victim is flooded with requests from a large number
of different sources. The number is too high to block individually and the
volume of attackers will typically overload all local infrastructure such as
firewalls and network switches. In this scenario, you typically need to work
with your ISP to have traffic to the target system sent to a 'black hole', which
will reduce the volume and allow local infrastructure to recover, but typically
does mean that the DDoS on the specific system is successful (because traffic to
that system is being sent to the black hole). The key point is that because the
attack is distributed, it is very difficult to block the attacking systems. 
So, with respect to your question regarding cloud based DDoS services - this to
some extent depends on your definition of cloud. One definition would be any
service that is not on your own infrastructure and is delivered from 'the
cloud'. In this sense, DDoS attacks are already cloud based. They don't use the
attackers own infrastructure, but instead, use hosts the attacker has either
compromised or hosts the attacker has identified which have either poorly
configured services or lack sufficient controls to prevent them from being used
as part of a DDoS attack. For example, one of the reasons there is so much
concern surrounding IoT is that many of these IoT devices include services which
can be exploited as part of a DDoS attack and lack sufficient controls to
prevent this exploitation by unknown remote uses. 
If you define cloud to be just IaaS, PaaS and SaaS providers, the situation is
slightly different. You are unlikely to see these services being used to perform
the actual attack simply because the DDoS attack relies on high numbers of
attackers and being able to use that number of cloud providers is prohibitive -
remember that the cloud providers are not going to welcome this sort of use of
their infrastructure, so you will have to do it in a 'stealthy' manner, which is
becoming increasingly difficult as cloud providers lock down what is considered
appropriate use of their infrastructure (remember, they have a reputation to
maintain - if they become known as a host for 'bad actors', ISPs and others will
just block traffic from their IPs). 
This doesn't mean attackers don't use cloud services. What  you will often find
is that DDoS service providers will use cloud services as the command and
control centre for their DDoS agents/bots. They still need to do this in a
stealthy manner as most reputable cloud services will deactivate any users they
detect doing such things, but this is much harder to detect and they only need a
few cloud providers. The agents/bots they use to actually perform the attacks
are usually compromised desktops and servers, often in home systems which have
poorer security controls and increasingly IoT devices, many of which are also in
home or small office environments which lack enterprise security measures or
skilled system administrators etc. 
",0,2017-04-28 01:22:35Z,60
8,1,"
I answered a similar question here.  For a normal hardware solution, a hardware security module (HSM) does this.  For software, .NET offers the Secure String mechanism to encrypt and protect sensitive data in RAM. Other platforms may offer something similar.  For an AWS solution, Amazon offers CloudHSM to do pretty much what you are asking for, I think.
From AWS:

HSM is short for Hardware Security Module. It is a piece of hardware —
  a dedicated appliance that provides secure key storage and a set of
  cryptographic operations within a tamper-resistant enclosure. You can
  store your keys within an HSM and use them to encrypt and decrypt data
  while keeping them safe and sound and under your full control. You are
  the only one with access to the keys stored in an HSM.
The AWS CloudHSM service brings the benefits of HSMs to the cloud. You
  retain full control of the keys and the cryptographic operations
  performed by the HSM(s) you create, including exclusive, single-tenant
  access to each one. Your cryptographic keys are protected by a
  tamper-resistant HSM that is designed to meet a number of
  international and US Government standards including NIST FIPS 140-2
  and Common Criteria EAL4+.

",6,2016-05-03 18:14:43Z,74
8,2,"
While everyone else has suggested a hardware solution, I'll suggest a software solution (though I don't mean to imply that it's any better or worse than using an HSM - that you'll have to decide on your own based on your own needs). This solution is a kernel patch which encrypts processes.
RamCrypt is a project which encrypts the majority of the memory of individual processes with AES128 in XEX mode, so all secrets in the processes are safe in memory, even if all of the memory is forensically acquired. The memory is encrypted with TRESOR, so the encryption key is never present in RAM. It keeps the key in the x86 debug registers, so they are out of memory, and does all the AES computation using AES-NI, general registers, and SSE registers, so it never enters main memory or CPU cache. The key is then used to encrypt and decrypt memory pages for an entire process. Only 4 pages are kept unencrypted, and all others are encrypted. When an encrypted page needs to be accessed, it is decrypted using the AES key, and one of the 4 unencrypted pages is encrypted. RamCrypt comes as a kernel patch, and you can adjust the number of pages which remain unencrypted to choose a trade-off between performance and security. See the project's main page, and the associated research paper.
",3,2016-05-03 22:17:47Z,76
8,3,"
I'm not sure if I get you right. But at some point the plain secret has to be in some sort of memory (most likely the RAM). If the attacker can read the memory at that point and if he is able to find the secret he can decrypt the data afterwards.
I think theoretically there can't be a way (maybe with some special security module that contains your program code) to save the secret in memory without any chance the attacker can find it. There is always an algorithm that can find the key (there has to be because you want to use the key in your own code). But as pointed out in the paper it can get really really hard to find the key. 
Just a side note: I also don't see the use case of generating a random secret that no one knows and then not store it anywhere.
Cheers
",1,2016-05-03 07:21:37Z,78
8,4,"
Fundamentally, there is nothing that can solve this issue in the absolute. You're running into the DRM problem, which is that you need plaintext access to some data on a system in order to fulfill some functionality, but you're also placing that system under the control of a party which is untrusted. The best you can do is make it incredibly difficult for all but the most determined and skilled attackers, essentially making it a poor a cost/benefit trade-off for them.
In terms of solutions which do make it hard, devices such as Hardware Security Modules (HSMs) should be pretty much top of your list. They utilise a range of features designed to make it exceedingly difficult to recover information from them illegitimately. For example, a common feature is to encrypt data with a key stored in volatile memory (e.g. DRAM) on a separate board, physically attached to the upper casing of the device. The power and data connections for this board are supplied via a contact connector (often made from conductive polymer or foam) via pads on the main board, which is physically attached to the lower casing of the device. If you attempt to open the device, you separate the boards, thus disconnecting the power from the DRAM and losing the keys. Additional sensors such as light, temperature, pressure, acceleration and position (incl. GPS) and even magnetic fields or radiation can be present in order to detect different types of tampering.
One additional potential system which may, in some circumstances, be of use to you is the concept of homomorphic cryptosystems. Homomorphism, in laymen's terms, is essentially the property of a cryptosystem that allows certain operations to be performed on data in its encrypted form, without needing to first decrypt the data. There are a variety of schemes available, some of which are even practical for certain types of scenarios, but they are usually quite complex in nature and can be particularly slow. I am unaware of any existing cryptosystem which provides homomorphism in a way that would be conducive to cloud storage applications.
",1,2016-05-03 19:20:38Z,21
8,5,"
It depends.
Other posters suggest an HSM.  This can be a good solution for a limited set of purposes where you temporarily need access to a key, it's erased from active memory after, and the key to the HSM itself isn't stored in memory long-term.  
It sounds like you need the secret to be active in memory all the time, and not stored in an HSM most of the time.  In that case, I don't see much value in the key storage capabilities of an HSM, since it can't protect keys already stored in memory.
And HSM can also perform the cryptographic functions in the module itself.  Whether this is useful to you or not would depend on your application.
",1,2016-05-03 21:09:27Z,26
9,1,"

Google has access (obviously).
The police will have access if they have a valid search warrant.
A national security letter will give the FBI secret access.
Various three-letter agencies may have access, depending on how they're doing at circumventing Google's encryption.  (Google started encrypting its internal traffic after it was revealed that the NSA was monitoring it.  Modern encryption, properly applied, is believe to be sufficient protection against three-letter agencies -- all known attacks are against the ""properly applied"" part rather than the encryption itself.)

As for deletion, Google uses a highly distributed storage system.  I don't believe they will intentionally keep data after you delete it, but because of how Google's storage works, residual copies may stick around for a while.
",48,2014-07-07 08:25:35Z,3
9,2,"
Not instantly. Although, that's what I want to believe. What you could do is the following. Download the Truecrypt version 7.1a and create an encrypted storage file (option 1 from the wizard) and choose 3 algorithm based encryption with a SHA-512 key. Put all your sensitive files in here and upload the encrypted file to Google Drive. When you want to work with your files. THIS IS IMPORTANT: Copy the encrypted file OUT of Google Drive, perform your work, encrypt and put the encrypted storage file back on Google Drive. This should keep your data safe.
If you want to delete it, just delete the encrypted storage file, nothing they can recover without your key.
PS: Since a short time you can ask Google to explicitly wipe your search data. If you just delete your search history through your user control panel in google than it is still used for advertisement. (scumbag, I know)
",15,2014-07-07 08:29:02Z,85
9,3,"
Any data you upload to Google Drive (or Skydrive, or Dropbox for that matter) should be considered duplicated by the NSA. Apart from arbitrary queries from the aforementioned secret service, law enforcement agencies from any country may gain access to them through legal means (subpoenas and so on). And of course, Google engineers could in theory browse your files.
As for what happens when you delete them, who knows? It is a good working assumption to consider that any data uploaded will stay on their servers forever. Realistically, I suppose it does get deleted at some point, but considering how much replication they must have, it may be a while. Your guess is as good as mine.
Bottom line, it is not trivial to set up off-site backups for confidential data. The only thing you can trust (or hope to trust) is cryptography. Look into duplicity and other backup tools which support strong encryption!
",10,2014-07-07 08:28:07Z,87
9,4,"
What about hosting those files on other hosting services which have client side encryption like:

Tarsnap, which has open source client and does client side encryption.
Tahoe-LAFS
SparkleShare, which has GUI clients for all operating systems.
more on https://prism-break.org/

",6,2014-07-08 15:03:40Z,90
9,5,"
Cryptography is a form of smart obfuscation,
it does not make things ""secure"", just ""secure enough, for now"".
If someone REALLY is out to get you, he would store your encrypted data until a later date when encryption can be broken - anything from stronger computers to software vulnerabilities like ""heartbleed"" will do the trick to decrypt your stuff in the long game.
Also, chances are, if someone really is out to get you it would probably try to find a way to hack your computer, not your cloud account, there's plenty of such software around - some of it available to random 3rd world governments under shareware trial licenses for free.
",2,2014-07-07 18:33:36Z,91
9,6,"
To answer one of your questions: any Google employee who has administrative access to the hosts (file servers) on which the drive data is stored will have read-access to your data: this includes the operations engineers, service engineers, system administrators: these type of employees are typically the ones with that type of access.
The answer to your other questions depends on what methods Google uses to remove distributed data across multiple hosts, and whether they are using any backup methods to store copies of the data. Nobody other than Google technicians will know the answer to this question, and of those probably only the engineers who maintain the filers and the data distribution structure. 
To give you an idea of how truly ""privy"" that information is: I used to work at Yahoo as a Unix sysadmin, but I didn't work with the filer infrastructure and so even I don't know what the answer to that question would be for Yahoo data, although I spent a year working in the operations group for two of the departments there.. Only the engineers directly responsible for that infrastructure are going to know if the data is truly ""deleted"" or not (and to what level of deletion, if that makes sense to you).
most likely, the file system pointers to the data are deleted within one or two days (depending on how long it takes to propagate through Google's infrastructure), while the actual data itself continues to reside on hard disk until it is overwritten: meaning the data (or parts of it) could theoretically (and fairly easily if someone made the effort) be recovered many months later. 
Regarding other agencies or companies with access to this data? Nah .... the only way the NSA or somebody could access it is if they have employees working at Google. Unlikely. And even if they do, do they have root access on the servers in question? Not likely. As someone else mentioned, theoretically the government can get access through a subpoena process. But from a practical perspective do these people have willy-nilly access to John Anybody's data? Nah. 
The only people who theoretically intercept your data when you upload it are the system administrators and operations engineers who work at the companies that provide the network backbone for the internet: such as UUNet. I know for a fact these people for many years have scanned internet traffic for illegal content, because years ago when I lived in Virginia I interviewed with UUNet for a job which involved this work. They offered the job to me, but I didn't take it because I accepted a job elsewhere at the time.
",2,2014-07-08 00:06:37Z,94
9,7,"
This is more of a comment than an answer, but apparently I'm not allowed to comment yet. If you're concerned about the privacy of your cloud storage, you may want to consider Tresorit as an alternative to Google Drive. It offers client-side encrypted storage & syncing, so the Tresorit engineers aren't even supposed to be able to tell what files you're storing there, let alone their contents. It's also hosted in Switzerland to take advantage of that country's strong privacy laws.
Of course, it has the same downside as all true encryption, which is that if you lose your password, there's nothing they can do to help you.
",2,2014-07-08 18:26:21Z,95
9,8,"
Apart from statements like NSA has your data when uploading it is based on the assumption that your data being on your disk is not somehow accessible. Like others stated, if your data is encrypted in a strong manner on your disk and uploaded in the same way I would regard the online version more safe in terms of redundancy (google is managing the replication and backup of their cloud). Stealing or breaking your machine with your data then has no effect.
If your data is not encrypted you still assume at this moment, that your data is totally secure on your disk and not accessible from the NSA. Depending on your operating system this might not even be the truth. Even if you can guarantee that your OS is free of any spyware, there is a chance that you cannot say the same for the channels through which you obtained the data. Being uploaded to google certainly allows easier access to data but I would not regard it as a ""none to full exposure"" in that way.
",1,2014-07-07 13:28:53Z,96
10,1,"
Let's clear up a couple of misconceptions:

DDoS mitigation partners cannot work unless they can decrypt traffic

Yes, they can - they just can't do it as well, as the content is not visible, so the solutions which work on traffic flow will still work.

Handing your key to a 3rd party is a bad thing

Not necessarily. In fact this is extremely common for large corporates - who typically have their infrastructure managed by 3rd parties anyway. What it does do is require that you replace some of your technical controls with contractual controls, and that you take greater oversight/audit responsibilities over the 3rd party.
Mitigation of the threat of the 3rd party misusing the key, or not protecting it is a straightforward one to assess and then provide a solution for. 
",5,2013-11-13 15:34:13Z,99
10,2,"
SSL DDoS Attacks should be divided into two - 

Protocol misuse attacks - 
Such attacks exploit the protocol being used and can cause denial of service effect without completing the creation of secure connection (SSL in our case). A good example can be the THC-SSL-DOS which can be used to create repeating 'renegotiation' in the same connection, though never completing the creation of a secure channel.
Protocol attacks, like the one mentioned above, do not require having keys. They can be mitigated with relatively simple protection measures as IPS signature, enforcement of server settings, or even dedicated DDoS appliances Pravail/Riorey/DefensePro/NSFocus/etc. 
SSL Traffic Floods - 
Here I refer to data that is being passed over the created secure channel. Without further information, these magnificent mitigation devices cannot distinguish between valid connections to malicious connections. They cannot even issue web challenge in an attempt to try and assess source legitimacy. So you are either stuck with nothing or some rate-limit protection - prone to false actions.  

So, what can be done - 
Some appliances can be fed with the certificates or be connected to another appliance from same family (DefensPro-Alteon / Pravail-VSS) - where the sister product opens the encryption, and ddos device does what it does and the flow is allowed to continue or is blocked. 
Radware can issue a challenge on the initial encrypted request (and only on it). Arbor can continuously inspect encrypted traffic, and block suspicious traffic (no challenge). As long as you keep these machines at your place, the risks are rather low. 
Some cloud services offer you to give them your keys. Not very recommended practice. 
However, Prolexic lately published they can live with temp-short-lived keys to achieve the same. This might be a sufficient solution, considering all other limitations and restrictions when dealing with secured (encrypted) traffic. 
And last -  is it bad to provide your key? 
In a very basic manner - Your key is important to secure your communications. If someone has your key she can theoretically 'read' your communications, or impersonate to you while communicating with others. 
However, as someone answered before me, you can practice sharing or having copies of your keys at different places. Depends on your infrastructure, on who manages your network, etc. 
Above all it relates to TRUST. If you trust this 'born yesterday' DDoS protection in the cloud service - go a head, give them your keys :). There are companies that can be trusted and that put many efforts into complying with all regulatory requirements. It doesnt provide 100% insurance, but frankly, nothing does.   
",2,2013-11-14 13:14:09Z,100
10,3,"
All the vendors are going to give you the same answer: you need to hand over your key to them either for a cloud-based security product, or for a box that they supply that sits in your network. Either way you cannot completely mitigate the threat that the have your keys. 
The alternative is to mitigate the threats posed by individual attacks by:

Keeping your systems and applications up to date: many previously serious ddos attacks have been eliminated by OS and software updates
Modifying your configurations: you can often tweak OS and applications to be much more resilient
Offload SSL computations: there are several solutions in this area, often load balancers have crypto engines that can do the work

What you really need to do is perform (or have somebody perform) a threat analysis to find out your biggest concerns, then look for solutions to those. It may be a vendor solution is your best way forward, or you may be able to do just fine using tactical, zero or low cost solutions. 
",1,2013-11-13 12:40:02Z,101
10,4,"
Ichilov's answer is more relevant.
Two non-vulnerable SSL Attacks

Send garbage data in place of pre-master secret (SSL Handshake flood)
Continuously re-negotiate the per-master secret from the handshake again & again

Hope Ichilov's explination is quite enough
Mitigation:

Client puzzle can help to mitigate DDoS attacks. The client puzzle
mechanism could be developed for protocol. It's based on
Proof-Of-Work systems. For SSL an IETF Draft is available at
http://www.ietf.org/id/draft-nygren-tls-client-puzzles-00.txt .Client-Puzzle mechanism is an intent-based security mechanism that
doesn't simply eliminate malicious clients, but increases the
attacker's cost. Also these mechanism are more effective against
signature based mitigation, which employs IDS/IPS systems with
enormous amount of rules.
The other method is to outsource the computation of modular exponent,
a highly resource intensive operation. A partial computation can be
outsourced to a daemon system(s). Otherwise, a system with
crypto-chip can be used. Outsourcing can improve the performance of
the server, but doesn't eliminating malicious client.

I already developed client-puzzle mechanism to work with HTTP. Also I planned to develop for SSL. Hope, you might feel the article interesting.
https://blog.cloudflare.com/keyless-ssl-the-nitty-gritty-technical-details/
",1,2015-07-23 05:19:03Z,102
10,5,"
There are ways and means to monitor TCP and SSL handshakes to determine bad behaviour ahead of actually decrypting / offloading.
Look at the client/server hello for non-RFC compliance and consider rate limitation for 'hello' floods based on this.
HTH.
",1,2017-01-23 15:00:07Z,103
11,1,"
You can do network scans from AWS services, but you need to fill out a request form; otherwise, you will breach their acceptable use policy.  You can find more information about the actual request process here https://aws.amazon.com/security/penetration-testing/
",4,2013-02-20 15:25:17Z,104
11,2,"
NMAP is available for Linux, Windows, Mac OS X, BSD, Solaris, and other platforms as well. If the ""cloud service"" can provide one of these environments, you can use it to run nmap. You will also want your own public IP, not something behind a NAT.
I think AWS is just web hosting, but maybe Amazon offers a service that could be used it this capacity. 
EDIT: AWS apparently provides a full virtual server, but it is behind a NAT, not conducive to port scanning. 
",1,2013-02-12 16:44:57Z,106
11,3,"
A VPS (virtual private server) is probably what you are looking for.  This would give you an actual VM to work from in a remote data center for a reasonably cheap price.  A VPN could also work (which would allow you to tunnel to an external IP with your own local hardware).
",0,2013-02-12 17:04:50Z,11
11,4,"
You are probably going to need to ask permission from any cloud provider to do this.  They generally ban this for obvious reasons and you need to get a waiver.  I'm not sure they'd provide a waiver in this direction of scanning.  Usually their customers want to scan the services running in the cloud and the IaaS provider will give them a narrow window to do it in.
",0,2013-02-12 20:50:23Z,108
11,5,"
u do not need a powerful machine to do portscanning, I do very large networks and use something simple like https://clientarea.ramnode.com/cart.php?gid=1 to do it. 
",0,2013-02-15 15:03:59Z,109
12,1,"
Start with a network diagram based on how a corporate network would look like. For example the DMZ is going to contain the webservers and mailservers. Then there will be firewalls to protect the userland from the DMZ, also you are going to have domian controllers, database servers which are going to be part of a secure network. Hope you get an idea on what am trying to descibe. You can find a sample network diagram here on a for a pentest lab. You need not follow the same thing, but use it to design your own.
As for actually building the lab I would suggest using vSphere server which allows you to virtualize the operating systems and build a network. Check this blog on setting up a virtual environment.
Check out the following resources to help you in setting up a lab
http://ist.bk.psu.edu/cvclab/wp-content/uploads/2011/12/BuildingLab1.pdf
http://www.irongeek.com/i.php?page=videos/pen-testing-practice-in-a-box-how-to-assemble-a-virtual-network
http://www.jasonjfrank.com/wp-content/uploads/2012/10/Building-your-Own-Penetration-Testing-Lab-on-the.pptm
",4,2013-01-03 11:47:36Z,119
12,2,"
This presentation at OWASP AppSec USA 2012 is about a collaborative university program that teaches practical security. It is based on OWASP Hackademic Challenges Project. He starts talking about the pentesting lab at 06:30. 
Books about building you own lab:

Build Your Own Security Lab: A Field Guide for Network Testing by Michael Gregg, 2008
Professional Penetration Testing: Volume 1: Creating and Learning in a Hacking Lab by  Thomas Wilhelm, 2009

",3,2013-01-03 15:58:28Z,121
12,3,"
You need to be looking in the direction of vSphere and ESXi, Hyper-V, or the AWS. You are creating a cloud environment that can spawn new pre-configured networks and hosts. The licensing costs can be high but it will accomplish what you want.
Alternatively, you could make VirtualBox and pre-made VMs available to students along with virtual network settings. That way they could work on the labs at home. 
EDIT:
My source at VMWare suggests that vCloud is the preferred approach, because it allows for the segregation you need to isolate student environments (isolated vlans, etc.). Other than that, a VPS (like EC2) can provide what you need cheaply.
Either solution can be scripted for automated management.
EDIT2:
Found this open source virtualization option that allows for complex VLAN networking: ProxMox
",3,2013-01-03 18:52:01Z,123
12,4,"
I would also recommend hackthissite.org.  They have a large selection of free demonstrators that vary in complexity.
",2,2013-01-03 14:35:16Z,11
12,5,"
SANS also provide a paid-for service similar to what you're talking about called Netwars - the site is here https://www.sans.org/cyber-ranges/netwars
",2,2013-01-03 15:56:36Z,124
13,1,"
While Thomas Pornin is correctly pointing out that the only way to trust a host under attack is using fully homomorphic encryption in practice you can try to work around this requirement. 
A potential attacker has full control of CPU, memory and disk. So it is not possible to do any calculations on valuable data in a VM that might not be under your control. On the other hand it is often not necessary. If you want to use your VM as a database or storage / backup service the VM never needs access to unencrypted information. You could store files or entries in a database, store hashes of the encrypted files or file names and e.g. sort for file size or retrieve a certain file where the client supplies the file name as a hash value. 
Such a scheme limits what you can do with your data but given a good encryption it is impossible for an attacker to steal your data. In the worst case they can modify it, so you need encrypted signatures to prevent tampering of the data.
A popular example is boxcryptor, which uses an encrypted container to store files securely at different cloud providers. Another is duplicity, which allows encrypted backups using rsync and GnuPG. 
Encrypting the storage of the VM itself by, e.g. truecrypt or Bitlocker does not increase the security level significantly. It might thwart an attack if the VM is powered off but as soon as you run it the OS needs the key to access the encrypted volume and at that moment a potential attacker can get hold of the key as well.
",8,2012-10-06 16:21:59Z,126
13,2,"
If the host is hostile then resistance is futile.
The host can read the disk, RAM and CPU state of your VM. Only fully homomorphic encryption would save you, but it does not work yet (Science has not uncovered an efficient solution yet; but that's just a factor one billion or so, therefore we can still hope for something... later on).
",7,2012-10-06 13:12:34Z,128
13,3,"
Other people are speaking in absolutes (in theory). Yes, if attacker has access to the host OS, then your VM can no longer be considered secure, in theory. However, to do this in practice, it requires some non-standard hacking work, which implies this is a targeted attack against YOU, because there will be plenty of other VMs that will not be encrypted and would be far easier targets than your VM. Full disk encryption in the cloud will protect you in the crowd of other VMs, but if you are targeted specifically, you're dead. There is no absolutely secure system, just different difficulties in accessing it. Disk encryption increases the difficulty. If you want better security, run your own host, but only if you are competent, otherwise it could be worse.
",4,2012-10-06 14:30:43Z,129
13,4,"
Your problem here is the same old one- an attacker who has access to the physical box, or in this case the host, can gain access to your VM. 
This is one of the risks you need to take into account if you outsource things- you use your contract with the provider to give you the assurance you need. 
",2,2012-10-06 13:08:18Z,99
13,5,"
Split your data across many VM's, hosted by different hosts (and providers, if possible) so data from single VM are useless. Of course that could kill usability and/or performance. Very rough idea.
",0,2012-10-09 19:03:05Z,130
13,6,"
Use full disk encryption if you can, where you can. Full disk encryption will only mature, and precludes physical attack, which is time off in security work for the real issues with logical access. 
By physical attack, I mean from the VM BIOS, keystroke loggers. Also would protect the drive from being mounted from elsewhere if the surrounding infra was stolen and taken off line, as opposed to actively being attacked. I think that particular threat model sounds major enough to me for it to swing the decision to use FDE if possible and if you have covered off any bigger security issues.
",0,2013-10-17 21:08:00Z,131
13,7,"
I believe protecting VM in a remount host is not easy. The best that you need to do as follows:
First: 
Establish mutual trust between you and the visited host.
Second: 
Add verification tools within your VM to detect abnormal behaviors in the Visited host.
Third: 
Add protection mechanisms to  protect the most sensitive data that VM deals with. 
You might need to put ""VM Obfuscation "" under your consideration.
Thanks 
",-1,2014-01-31 21:36:47Z,132
14,1,"
The most sensible approach is to assume you cant rely on their privacy - it isn't their responsibility, although there are some services whose selling point is securing this data.
If you take that stance, as long as you encrypt all data before it goes to the cloud you can be safe (decide on what level of encryption you need in order to be safe)
This approach gives you a very practical backup, just make sure you protect your encryption keys.
",16,2012-04-25 14:08:35Z,99
14,2,"
I would say no its not suitable for storing criticial information,
From the sound of their terms Google essentially owns everyting you upload as well as anything derivitive of your data as well.
Here is an excerpt from the verge.com explaining the differences of the 3 major players, notice Google is very liberal with what they can do with your data.
Dropbox
https://www.dropbox.com/terms
""By using our Services you provide us with information, files, and folders that you submit to Dropbox (together, ""your stuff""). You retain full ownership to your stuff. We don’t claim any ownership to any of it. These Terms do not grant us any rights to your stuff or intellectual property except for the limited rights that are needed to run the Services, as explained below.""
SkyDrive
http://windows.microsoft.com/en-US/windows-live/microsoft-service-agreement?SignedIn=1
""Except for material that we license to you, we don't claim ownership of the content you provide on the service. Your content remains your content. We also don't control, verify, or endorse the content that you and others make available on the service.""
Google Drive
http://www.google.com/intl/en/policies/terms/
""You retain ownership of any intellectual property rights that you hold in that content. In short, what belongs to you stays yours.
When you upload or otherwise submit content to our Services, you give Google (and those we work with) a worldwide license to use, host, store, reproduce, modify, create derivative works (such as those resulting from translations, adaptations or other changes we make so that your content works better with our Services), communicate, publish, publicly perform, publicly display and distribute such content.""
",13,2012-04-25 15:48:19Z,135
14,3,"
Consider this from an Information Management or Information Assurance question rather than an Information Protection question. To the question if a service provider's level of security is ""safe"" (sufficient and appropriate), the answer is YES and NO - depending on the level of protection the specific information requires.
My suggestion is you create three big categories of information, Public, Personal and Private. You can ask yourself how much damage would I be willing to endure if I lost ""personal"" information to help you decide if a specific item of information should go in ""private"" or not.  Private is the category where you need the most burdensome protections, and you don't want to put less valuable information in that category because those extra security measures cost you time, sometimes money, and generally some frustration.
All private information should be stored encrypted. It does not matter if you store it on a local drive or a cloud service, presuming the encryption is the right type (for example AES with 256 bit keys) and the pass-phrase sufficiently complex and is kept private. Odds are your home network is far less protected than networks managed by Microsoft or Google.
All personal information should be stored under the protection of network credentials by a service provider who publishes their security standards.  In the case of Microsoft, all of their SkyDrive infrastructure is now on their Windows Azure platform, which meats very stringent protections such as HIPPA.  Google, by contrast, is very upfront about scanning most information stored to target ads to you.
If you want to store personal information you can use an enhanced service from Microsoft called Office365.  For $6 per month you get a privately segmented Exchange service for email and a full SharePoint (SkyDrive plus many extra features) that is designed to protect your information from being shared outside of your ""namespace"" (think of it as your domain name). Those Microsoft services have been reviewed in depth and (outside of government classified information which generally requires dedicated hardware), Office365 is approved for all highly regulated environments such as SOX, HIPPA, etc.
If a piece of information is not Private and not Personal, it should be public and then any of the services should be safe for you. Microsoft is hyper-vigilant about not losing customer data as they spend billions of dollars on Windows Azure, so my opinion is they are safe as a backup location.
Let me leave you with a way of thinking about information that was shared by a crusty old IT guy that knew (kids avert your eyes):
Public is a picture of your best friend.
Personal is a picture of your wife in lingerie.
Private is a picture of your best friend with your wife in lingerie.
How much you protect each level of information depends on how embarrassed you would be seeing the picture (document, etc.) on the cover of your local newspaper.
",9,2012-09-25 20:05:14Z,138
14,4,"
My concern with any cloud service would be that one day you'll wake up and they'll be gone.
-- so long as they're a backup, not primary storage, it's an ordinary risk.
",5,2012-04-25 18:50:27Z,139
14,5,"
As others have mentioned, general-purpose cloud storage providers, like Microsoft, Google, Apple, and DropBox are not completely safe, since although they encrypt your files, they have copies of the keys (needed so they can index your files for search purposes).
And as Rory points out, you can make this super-secure pretty easily: encrypt the backups yourself before putting them in the cloud. This is the best option.
But there is also a middle-ground. There are some cloud storage providers who focus on security by not keeping copies of your encryption keys. SpiderOak are a popular one for backing up documents. Carbonite are also popular, and also offer an option to back up the whole of the machine (i.e. OS as well as documents.)
",5,2012-04-27 14:37:12Z,141
14,6,"
Why not using a combination of truecrypt volume and those cloud storages! truecrypt for confidentiality and the cloud services for redundancy and backup
",4,2012-04-25 18:42:11Z,142
15,1,"
If you are already settled on using Dropbox, then your only choice at the moment is to use a 3rd party program such as PGP (or free/open GPG) to first encrypt the file and place it into your drop box. I know you said you wanted to use public/private keys, but as an alternative, you could also use 7-zip to create secure archives with AES encryption based on a pre-shared key (password).
If you have not yet decided on Dropbox, you can look at similar online storage systems such as SpiderOak, Cryptoheaven, Mozy or similar, which perform client-side encryption of the files prior to uploading them into the cloud. You control the encryption key, and therefore the storage providers cannot access the data. I am not sure whether you can import existing private or public keys into those solutions or not, however.
",9,2011-12-30 16:01:21Z,148
15,2,"
I personally have this problem which I solved it myself.
I created a program which works with GPG4Win, and encrypt all my files with my PGP Key. I can also specify which folder encrypt with which user key, quite good in my opinion.
It is a 1 way sync + encrypt from my data folder to Skydrive folder (local), then Skydrive will sync to the SkyDrive Cloud. The best part is that this program does not store your PGP key, or any cloud account password.
Experience it!
Feel free to drop a visit @ http://successreality.blogspot.sg/2013/10/encrypt-sync-4-cloud.html
",4,2013-10-30 14:55:00Z,149
15,3,"
True Crypt may suit your needs. It offers the ability to use  keyfiles to encrypt your files which can then be stored in your drop box and shared.  It also offers the ability to use tokens and smart cards to secure your files.  This lets your key be passed in an out of band manner.
In my opinion the best feature of true crypt is the ability to create hidden volumes.  This creates what the creators of true crypt call plausible deniability.  This basically means that there is a second encrypted volume that appears to be comprised of nothing more than random data.  This is created inside of your encrypted file using using the free space at the end of the first volume. It is also worth noting that several different encryption  algorithms are available.  
",2,2011-12-31 02:36:38Z,150
15,4,"
Give Wuala a shot.
Data is encryted on the users device before uploaded to the cloud. It does not come with key-files but passwords. Maybe this is sufficient for you?
",2,2013-02-04 16:51:45Z,152
15,5,"
You can try:
AxCrypt by Axantum. It's free, but provides only AES-128 (not too hard). This tool may be suitable for you because it provides a portable version. Encryption and decryptionis done by two different programms (AxCrypt2Go and AxDecrypt respectively).
SafeBox is a light tool, supporting asymmetric and symmetric enctryption. Has a clear interface and detailed instructions on the site. This tool has more strong encryption algorithms. Files can be shared via any channel that supports text, or wrapped up in a container. 
",2,2017-09-20 11:26:40Z,153
15,6,"
ESecureDOX provides just about exactly what you're looking for. It's a cloud storage system run by a digital certificate authority, which means that each user is assigned a free digital certificate and PKI key set when they sign up, and each document is automatically encrypted by the storage system when you upload it to the cloud. Your document can only be accessed by your user account because of the technology behind the individual encryption (asymmetric PKI), so that the encryption code for each account would have to be separately compromised in order for any data to be compromised. And that's unlikely: the encryption that it uses would take 20 mainframe computers 20 years to brute force, according to NSA estimates. This opposed to something like dropbox, which only uses SSL encryption. What that  means is that if someone were to gain access to the blanket encryption that dropbox uses, every file on the system would be compromised.
As far as your concerns about sharing a document, when you share a document through ESecureDOX, only the single document is decrypted by the system, and that's done in a way that whomever receives your document is unable to edit it. This allows the sharing of documents without the concern that they'll be tampered with, and without allowing access to any files besides the one you want to share.
I apologize if this is overly dumbed down, but I just wanted to make sure that the advantages of the system were very clear. From the user interface ESecureDOX seems almost identical to already present cloud storage systems, but that's only because the PKI encryption and decryption happen behind the scenes. It can do that because the company that runs it is able to issue its own root certificates and integrate them into their storage solution without the user needing any technical knowledge of PKI or requiring them to keep track of their own keys.
Edit: for full disclosure, I do work in marketing for the company that runs ESecureDOX, Image-X Enterprises. But I haven't misrepresented how the service works in any way, and I do believe in its advantages regardless of my affiliation with the product.
",1,2015-04-29 21:49:01Z,154
15,7,"
Use an S3 bucket from AWS; server side you can have it encrypted and then to connect you can provision each other secure keys and use them for auditing and logging. With the use of Transmit you can add the bucket as a device in finder on OSX and treat it as a local drive which stores and pushes straight to the cloud. 
http://aws.amazon.com/s3/
It's the back end of dropbox, but it's dirt cheap and you can even choose the geographical region for your data centre.
Edit; For the purpose of securing communication client side while integrating with an S3 bucket, you can use the S3 client side encryption mechanisms. 
",0,2013-10-30 16:22:41Z,155
15,8,"
I created a small tool to do this. http://www.itsencrypted.com It uses the user's public key to encrypt a key file, which unlocks the encrypted file which is also in your Dropbox. Then later he'll use the program to download it directly and unencrypt it.
",0,2013-11-10 18:56:52Z,157
15,9,"
Another good solution is Boxcryptor. It encrypts all your data before they are send to Dropbox (or other providers). Boxcryptor also allows you to share your data in a confidential way. Therefore it makes a public/private key pair for each user. When you want to share a file with a friend, Boxcryptor downloads the public key of your friend, encrypts the file and your friend receives and decrypts the file.
",0,2013-11-10 20:02:19Z,159
15,10,"
A bit late to the game, but try Turtl. Uses client-side encryption and allows easy sharing/collaboration for both files and notes. Disclosure: I'm the founder.
",0,2014-03-07 08:42:42Z,161
15,11,"
pCloud Transfer is also a good option. It is a simple system for securely transferring files between two parties. The files get temporarily stored in the ""cloud"" but you can be sure that they will be unreadable to anyone who doesn't know the password you have to provide your recipient with. It's free and requires no registration to use it. I've been using for some time now and I'm really happy with it.
",0,2014-07-30 07:02:09Z,162
15,12,"
Even though this is an old question, it's on the first page of search results. Here are some updated free and low-cost options for 2017. 

SendThisFile. FREE forever. No file size limits, no credit card required, no software to install. All transfers include our comprehensive encryption and security, creating end-to-end 128-bit encrypted file transfers, and you won't have to configure a single setting. We use SAS70 type II / SSAE16 compliant data centers to ensure that private data is protected. It is the industry gold standard for the robust delivery and security of data. https://www.sendthisfile.com/
SpiderOak protects your group messaging, file sharing, and file backups with end-to-end encryption to keep you safe from privacy intrusions, ransomware, and data loss.
It keeps your uploaded material safe and private. They can’t even see what you upload, because you are the sole holder of the keys to unlock the material. The only downside is that it is a complicated for a new user, as it does have a steep learning curve.
100GB storage: $5 per month.
https://spideroak.com/one/ 

",0,2017-10-06 12:32:02Z,151
16,1,"
At 16-core GPU maximum, I'd build my own, unless oclHashcat can distribute work loads (at first look it doesn't seem like it does). That is assuming this thing is going to pound passwords all day most days. If you can scale it more (or want to run a lot in parallel) or wont use it all day long, pay for it by the hour.

Now, after some sketching and brain storming we concluded that GPU is the best way to go (contrary to CPU or rainbow tables).

I'd argue that you should probably download and archive some rainbow tables anyway, though it's less likely they'll come in handy, a lot of poorly written software still runs silly stuff like MD5, which will crack a stupid amount of time faster on a rainbow table. Multi-pronged approach for multiple environments.
Barebones EC2 GPU cloud will run you about $7+k on metal, depending on RAM/CPU/Disk requirements:
http://www.google.com/products/catalog?q=1026GT-TF-FM209&um=1&ie=UTF-8&tbm=shop&cid=16863135665868024811&sa=X&ei=4_HDTsPIFcff0QHjh7XmDg&ved=0CDoQ8wIwAQ
http://aws.amazon.com/hpc-applications/

Cluster GPU Quadruple Extra Large
33.5 EC2 Compute Units (2 x Intel Xeon X5570, quad-core “Nehalem” architecture)
22 GB of memory
2 x NVIDIA Tesla ""Fermi"" M2050 GPUs 1690
GB of instance storage 64-bit platform I/O Performance: Very High (10
Gigabit Ethernet. Full-bisection bandwidth with Placement Groups) API
name: cg1.4xlarge

",8,2011-11-16 17:21:47Z,165
16,2,"
This is a simple cost / benefit analysis, so it's a business question rather than a security one since you've already decided the merits of how to approach the security aspect. Write your code, benchmark it, and compare numbers in a spreadsheet.
For the metal
* Cost of metal
* Cost per hour in terms of electric bill (assume under full load) 
* Budget for parts replacement
* Operations per second

For the EC2
* Amazon rate per hour
* Operations per second

Compare the numbers on a month by month or year by year basis. Pick the one that makes more sense based upon expected life. Front-loading cost, net present value, expected utilization, risk, etc are all accounting problems.
",15,2011-11-16 16:49:19Z,170
16,3,"
This link about GPUs and EC2 password cracking gives you much of the information you are looking for:
http://erratasec.blogspot.com/2011/06/password-cracking-mining-and-gpus.html
The upshot is this. Radeon GPUs are about 3 times faster than equivalent nVidia GPUs for password cracking. But, EC2 uses nVidia GPUs, because they are better at most other GPU tasks. Moreover, EC2 doesn't just use nVidia GPUs, but the expensive ""Tesla"" versions. You can buy a $250 Radeon GPU with the same password cracking performance of a $10,000 Tesla system. Whether your buy it or rent it from Amazon, it just doesn't make sense.
Because GPUs can make you password cracking 20 times faster than a normal CPU, there is a great benefit to using one of them. But, because there are decreasing marginal returns, there isn't a lot of benefit to investing a lot of money in GPU number crunching.
The upshot is that it's worthy spending $500 for two Radeon 6850 cards and sticking them in your desktop for use with oclhashcat, but not worth spending more.
",3,2011-11-17 16:59:29Z,171
16,4,"
Did you try the new GPU rainbow tables from freerainbowtables.com ? 
You don't even need a supercomputer / cloud to check the hashes. 
You could donate to the freerainbowtables.com project some hardware and request your custom rainbow tables. (they have over 2000 machines, in their distributed project that generate rainbow tables on the CPU as well as the GPU).
For salted hashes I would go with an array of fpga cracking machines.
I suggest you post this question on the forum ( http://freerainbowtables.com/phpBB3/ ), there are a few tech freaks like Sc00bz, that would provide a very insightful answer. 
",3,2011-11-22 13:42:18Z,173
16,5,"
It is about risk management and cost/benefit analysis. The more risk that you are willing to take, the less it will cost you to run your application. There are some basic and somewhat obvious threats that could disrupt or compromise your application's operations:

Physical security risk

This refers to the physical security threats to your application's infrastructure, such as unauthorized physical access to hardware, natural disasters, power outages, network disruptions, terrorism, etc. 

IT security risk

IT administrators require physical access to the  hardware, as well virtual access the operating systems running on the hardware. IT administrators will require such access to servers, network equipment, power distribution circuits, monitoring software, etc. There are many security and operational risks associated with having so many different hands on your hardware and software systems.

Technical failure risks

The hardware and software can and will fail from technical reasons and these are everyday problems. When it comes to GPU computing, you will want to have a large cluster of GPUs (as many as you can get). But the more servers you are running, the higher the odds of a hardware failure randomly occurring.
For example, a network card or power supply could burn out. Or a memory leak could crash a server.  
A more subtle but also common problem is electromagnetic interference from the cosmic background radiation that can cause memory corruption.

Capacity risk

As your computational requirements fluctuate, you might have a sudden need to increase the scale up by adding capacity to your GPU cluster (i.e. ingadd more GPUs). There is a risk that your infrastructure will not be able to scale up with your application's requirements.


To help understand the situation, let me give you three examples of different GPU cluster setups with different cost/benefit profiles:

Tier 4 private data center

You build an underground bunker in a friendly sovereign country with strong privacy laws, in region of the planet that is safe from common natural disasters
Your data center is  physically guarded by surveillance systems, and armed security professionals
Bio-metric security access systems in every server rack in the data center ensure that only authorized personnel can ever physically access particular hardware
You have two redundant mains power circuits coming from power generation plants on the grid to the data center
Each power mains circuit is backed by a UPS backup power supply and a backup generator (you have contracts with fuel supply companies to guarantee a supply of fuel to run the generators over an extended period of time)
Each device (server, switch, etc) in your data center has dual redundant power supplies, with each power supply connected to a separate mains circuit
The data center is connected to the outside world through multiple, redundant fiber optic links to backbone telecom carriers
The local network in your data center connecting the servers is fully redundant, with multiple, bound network interface cards in each server, connected to multiple, redundant switches, firewalls, etc.
The local network in your data center connecting the servers is fully redundant, with multiple, bound network interface cards in each server, connected to multiple, redundant switches, firewalls, etc.
Virtualization technology is used to run Operating Systems in Virtual Machines interconnected through Virtual LANs, to protect against physical hardware failures. The physical servers are configured as a Virtual Machine cluster, so that virtual machines can be seamlessly moved from one physical machine to another. 
You only use server hardware with Error Correcting Code (ECC) memory, to protect against electro-magnetic interference from cosmic background radiation. For GPUs, this means that you must use NVIDIA's expensive Tesla series of cards which have ECC memory. The less expensive gaming cards do not have ECC memory so they are prone to data corruption.
You have a 24/7 Security Operations Center, doing things like monitoring network traffic and server activity to detect and respond to intrusion threats
You have a 24/7 Network Operations Center, monitoring and maintaining the status and health of systems, etc
You have redundant cooling systems for the data center
Air filtering and air quality monitoring systems are in place to avoid dust from entering the hardware
Fire detection, fire alarm and fire suppression systems with dry suppression agents are in place
Water pipes and other water systems are avoided - no water near the servers
You keep spare hardware components on-hand in case they need to be replaced
There is plenty of spare rack space, power and cooling capacity to scale up your data center if required
All aspects of the data center are continuously audited by an internal audit team
For added security redundancy, you build one or more similar secondary data center underground bunkers in a different countries and split your cluster across multiple regions. You use Global Server Load Balancing and Failover technology to seamless run your application across multiple data centers.
This is least risky but the most costly solution
Note that there are many other things involved in building a Tier 4 data center (see http://www.adc.com/Attachment/1270711929361/102264AE.pdf)

Third-party data center

Instead of building your own data center, you could co-locate your data in a third-party data center. Co-location in a Tier 4 data center facility would be very expensive (especially if you are co-locating in multiple redundant, data centers), but not as expensive as building and owning your own facility. This is less secure that owning your own private data center, but also less costly. 
You still own your own and run your own hardware that is co-located in the racks. You can secure them yourself and you can upgrade the hardware whenever you need to. There are many Tesla GPU server options available on the market from the different hardware vendors, like IBM, Dell, HP, etc.

Cloud based solution (e.g. Amazon EC2)

You do not have any physical control over the hardware and server virtualization systems. All of your servers are virtual machines running in Amazon's data center. Your application's security is at the mercy of Amazon and its employees.
You can setup a Virtual Private Network within Amazon for additional security. You can also run your own physical connections to Amazon's EC2 data centers.
Amazon offers three types of GPU instances: Spot instances, On-Demand instances and Reserved Instances. Spot instances are the cheapest, but you are not guaranteed capacity - you might bid on 10 GPUs in the spot market and only get 2, and you might lose your GPUs if the spot price exceeds your maximum bid. On-Demand instances are more expensive but cost a fixed rate and you can run them as long as you need to, but capacity is not guaranteed. Reserved instances are the most expensive and require paying an annual reservation fee for each server that you want to reserve, but this guarantees that you will always have access to the number of servers that you require.
The Amazon EC2 cloud is distributed across multiple geographic locations. This allows you to have multiple, redundant data centers 
You don't have the buy and maintain your own hardware, and you pay for metered usage by the hour which can be very cost efficient if your have large fluctuations in your computational demands. If you need 100 GPUs for a few days out of the year, then wit would be much more cost efficient to use the cloud than to purchase and run your own hardware all year round.
Amazon only offers the NVIDIA Tesla C2050 Fermi GPUs. There are more GPU hardware options available if you own your own hardware, including the new Kepler GPUs from NVIDIA that are not yet available on Amazon
Renting Amazon GPUs is very cheap if you go with spot instances (these typically cost 60 cents per hour, with no upfront fee). But if you need reserved instances then the cost rise dramatically to around $5.00 per hour on average.
Working with a cloud like Amazon is more complicated than any other solution. It requires you to spend a lot of time learning on tangential issues about how the Amazon cloud works instead of spending time figuring out how to use GPUs to solve your actual application.

Build your own low-cost, high performance GPU supercomputer in your basement

Let's say that most of the risks I've talked about are not not worth mitigating for you. You are willing to put a GPU cluster in your basement to save on the data center real estate costs, and you are willing to accept risks, such as the of a pipe bursting and flooding your basement ""data center""
Your application is not mission-critical, it does not require 24/7 guaranteed uptime.  If a natural disaster befalls your house then you will probably be worried about bigger things than the security of your GPUs. 
You don't even care about electro-magnetic interference from cosmic background radiation. If your memory gets corrupted and a GPU hangs then you can just reboot the server (and you have a cluster of GPUs anyway, so losing one or two is not a big deal). You can use the latest NVIDIA GeForce Kepler based gaming cards that are  cheaper and faster in single precision that the Tesla cards offered in Amazon. You are such a risk taker that you are willing to do all your computing in single precision instead of double precision, to take advantage of the cheaper gaming cards. So you can buy a high end gaming GPU from an OEM for $400 per card, instead of buying a Tesla GPU from NVIDIA for $2,500 per card and your $400 GPUs will be significantly faster than the more expensive Tesla GPUs, meaning you can buy more GPUs with your budget.
You can take a page out of the Google playbook ... instead of buying fancy servers from companies like IBM or Dell, you can build your own bare-bones, ultra low-cost GPU servers. This means you can buy even more GPUs with your budget. For example, you can skip all the bells and whistles like a case for the server, dual power supplies, hard drives, etc. Since your servers won't have any cases they will be easier to cool. You can also build your server with an ultra low-cost, low-power CPU (instead of spending money on big, expensive multi-core Intel CPUs that you won't use anyway since you are using GPUs), and instead spend money on more GPUs instead of useless CPUs. You can run everything on Linux to completely avoid software licensing fees, leaving you with more money to spend on GPU hardware.
This is not a very secure solution at all. However, this will get you hands-down the fast GPU supercomputer per dollar that you spend. This is your biggest bang for the buck if you're not worried about running a data center in your basement. And it is the easiest environment to maintain and develop on, there is no complicated cloud technology or security hindrances.


CONCLUSION:
For a password cracking machine, I would recommend going with Amazon EC2 if you plan to use large numbers of GPUs for small amounts of time. If you expect that you will only need to crack passwords once in a while, but when you do need to crack a password then you immediately want as many GPUs as you can get, then I would recommend that you go with Amazon EC2 spot or on-demand instances. However, if you plan to be cracking passwords 24/7, i.e. maximizing your utilization of your infrastructure, then I would enthusiastically recommend the DIY route of building a GPU cluster in your basement (just be sure that you have enough power capacity or you will trip a circuit). I should also say that the DIY route is much more fun.
",3,2012-04-05 06:54:18Z,174
16,6,"
You need to run the numbers; however, what are your time constraints?  
As a consulting firm, you might end up with a client that wants to see results by tomorrow. The only way to scale that large is to use Amazon. Building your software so you can scale out to a thousand nodes is the only way you can do that (without owning all that hardware).  
Using 1000 nodes for 1 day is the same cost as 100 nodes for 10 days, but you'll have the results a lot faster.  
",3,2011-11-16 19:29:46Z,108
16,7,"
Brute force password cracking depends on the algorithm in use.  For example are you trying to break a SHA1, SHA2 or BCRYPT password?  Or is the underlying algorithm SCrypt?
It is not possible (as far as I know) to GPU accelerate a SCrypt based password hash, and only CPU optimizations are available due to the additional demands on RAM with the cryptography.
Next, are there libraries that support what you need to do?  OpenCL is popular, but performs differently on NVidia GPUs vs AMD GPUs.  In addition, you may get better performance with vendor-specific APIs.
Finally, know that each video card performs differently and a $700 NVIDIA card is feature rich but not too efficient in SHA2 hashing.  A similar AMD video card will outperform an NVIDA card for SHA2 hashing by about 40% or more depending on the card.
If you choose to use the Amazon GPU cluster, know that the only OS is CentOS and you will have to have python skills (or similar) to get your code up and running.  It may not be worth it in the long run since the GPU farm offered by Amazon includes old NVidia cards that are good for general math offloading, but not efficient for password hashing.
",0,2013-01-02 15:20:59Z,175
17,1,"
Figure out the cost level where you start getting uncomfortable.
Calculate the GB that would need to be transferred to reach that cost level.
See if you think an attacker will be willing to spend that much effort to hurt you for that amount of money.
Every time I run this calculation I end up thinking that if somebody hated me that much, they could certainly find an easier way to hurt me.
For example, a million downloads of your 2MB document is going to cost you about $240 in data transfer charges plus $1 in request charges.  To create this cost to you, the attacker is going to have to download 2,000 GB (2TB).  That's weeks of completely filling up a 10Mbps line.  Just for a measly $240 impact.
Amazon generally doesn't discuss publicly all of the security measures they have in place to stop DOS, DDOS, and other attacks against their customers.  In one whitepaper, Amazon says: ""Proprietary DDoS mitigation techniques are used.""  Of course, it's not always easy to differentiate between a DDOS and a popular resource :-)
You can read more about Amazon Web Services security on their site:

http://aws.amazon.com/security/

",23,2011-11-02 03:04:12Z,181
17,2,"
Agree with Eric.
Another point: DOS/DDOS are nasty attacks that relatively easy to make and hard to protect on application level.
Think about what other option you have.
Let’s say you will go to some other hosting provider and will get same attack, will it be better or worse?
With Amazon at least you can be sure that you will continue to have service and it’s not going to crash under DOS.
Other thing that you may want to check is if you can make your document size smaller (use pdf and not word, change image quality etc.)
",9,2011-11-02 06:13:06Z,182
17,3,"
Although S3 don't allow you to set a cap on costs, there's nothing to stop you setting a cap yourself.
Add into your application soft limits which alert you if you are going well above your historical high points, and hard limits set at a reasonable level loss. That way you get to decide what is reasonable and what isn't, according to your budget.
",6,2011-11-02 13:23:39Z,183
17,4,"
In order to monitor your costs more than once a month you can enable what is called Bucket Logging. With this enabled you get the equivalent of web server access logs for your bucket delivered to you. You can then keep track of how many bytes have been transferred and by whom so there are no surprises at the end of the month.
",6,2011-11-03 15:37:04Z,184
17,5,"
You can add logging to your account, and then monitor the logs for activity spikes. 
Still, though, wish that Amazon had some sort of limits on S3. When I set up a client with their own private S3 account, this is a small problem - users can generate public URLs to say some funny cat video.... - But this has never actually happened.
Another thing you can do is to set up the account with a credit card that has a low credit limit. (Don't use the platinum card).
I have had some small MB sized assets in the public on S3 for a few years, and the bill on them is always microscopic. 
",1,2011-11-10 14:13:01Z,186
18,1,"
PCI compliance is a bit complicated. Firstly, with shared hosting, you need the provider you're on to meet all their relevant requirements. See Has anyone achieved PCI compliance on AWS?
Second, whoever has a terminal that is processing cards is responsible being complaint. It sounds like in this circumstance, you're not the processor and don't have an agreement with a credit card company. In that sense, you don't need to be compliant (HUGE caveat following)
However, the person with the terminal does need to be compliant, and everything they use in the process of handling cards (including you as a 3rd party) needs to achieve compliance. Compliance requires active review -- the depth of which depending on the volume of transactions handled.
Most critically, I wonder what circumstance you're in that you would be storing credit card numbers and not actually processing them.
PCI compliance has to do with volume of transactions processed. Smaller volume shops don't have to worry about a lot. Large volume companies are only PCI compliant if they meet all the standards specified and they are signed off on by a PCI council approved auditor. VISA, for example, specifies those as anyone handling 6 million transactions a year.
There's a guide for merchants at https://www.pcisecuritystandards.org/merchants/how_to_be_compliant.php
",6,2011-08-20 03:15:41Z,170
18,2,"
Absolutely Positively Not
Storing card details is only approved for certain setups, far more advanced than this. 
Doing this as described would not only be not PCI complaint it would be illegal in many countries under various privacy and commerce laws (many of which defer to the PCI SSC for policy and regulation.)
Furthermore it's almost certainly a server breach of the MSA to process payments through a terminal which were obtained in this way. Such a breach may be governed by a remedy of fine or legal action.
",4,2011-08-21 04:29:25Z,189
18,3,"
Its not ideal,  and to be honest it sounds like you are violating OWASP a9.  The checkout process is meaningless,  the problem is spilling your authentication token (THE COOKIE) over plain text. 
",2,2011-08-20 18:58:30Z,190
18,4,"
If you are taking the CC info and then running it through a terminal does that mean you are storing the CVC/CVV? You are not allowed to store that after authorization for ANY reason under the PCI DSS. 
",2,2011-08-21 02:56:24Z,191
18,5,"
It'd complicated, but in short, this setup would not be compliant.  For a start, based on the limited information provided, you are storing the card data in an internet facing zone, rather than an internal system.  There are numerous other things wrong with this setup (management interfaces, key management for encryption etc), but the data storage is a glaring one.
There are also additional requirements for shared hosting providers that would need to be validated.  Under PCI, shared hosting is not really appropriatefor anything other than a merchant page that then redirects to a compliant payment gateway.  Why is your client not doing this?  
",1,2011-09-07 22:20:28Z,192
18,6,"
All good answers. PCI says if you are storing credit card data in any form you need at the very least an ASV to scan your system for vulnerabilities. If you get to a level 1 or 2 (based on your transaction level) you will need a QSA to do it. The bottom line is if you are storing credit card data in any fashion you are a risk to everyone. You can do it but the costs to validate your systems could be prohibitive, depending on the volume you are doing.
",1,2012-03-09 17:34:40Z,193
18,7,"
Unfortunately, as previously stated, the answer is ""No""
To achieve full PCI compliance you need to fulfill many, very specific, requirements. 
Full list can be found here: 
https://www.pcisecuritystandards.org/documents/pci_dss_v2.pdf
Some of these requirements deal with the way you manage your servers, other talk about internal role distribution and so on... 
For example section 9.1.1 states:
""Use video cameras and/or access control mechanisms to monitor individual physical access to sensitive areas. Review collected data and correlate with other entries. Store for at least three months, unless otherwise 
restricted by law."" [this obviously talks about serves access]
I`m choose to focus on this section to show that PCI complicancy is not a ""purely technical"" issue.
Having said that, on a technical site, you probably will want to take notice of section 6.6 that calls for one of the following :

Reviewing public-facing web applications via manual or automated
application vulnerability security assessment tools or  methods, at
least annually and after any change. 
Installing a web-application-firewall in front of public-facing web applications.

This is probably the most demanding section in the whole bill as it requires you to either have a WAF in place or to perform routine checks after (and this is important) EVERY CHANGE made to your web applications.
In other words - pure nightmare...
This section is what drives many SMB website owners away from PCI DDS, and into an arms of a 3rd party billing providers, as the setup and maintenance costs are simply just too high. (WAF will cost thousands of dollars,even before maintenance costs, and routine checks after every change are not an option, and even if it is - it comes with an even greater accumulative cost) 
Recently a affordable solution to this issue was made available via Cloud-based PCI compliant WAF.
The idea here is that of ""shared-usage"" and ""economy of scale"". In this scenario, WAF protection is distributed via Cloud to a community of users (websites) and each member community gets full WAF features and updates but needs to pays only a fraction of the full price.
This cuts heavily on initial setup/purchase costs and also eliminates all additional maintenance costs (as a centered security team operates and updates the WAF for all Cloud users.) 
Also this provides full standardization and promises a very high upkeep quality (very important in an ever-changing security landscape but not achievable without a dedicated security person/team)
(disclaimer - I work for the company which provides this solution)
",1,2012-07-17 07:42:54Z,194
19,1,"
This may not be the answer you will be happy with but how about abstaining from having any undesirable data inside your phone in the first place and instead using the right tool for the job? 
According to Wikipedia:

The app records information about the device it is installed on, including its [...] IMEI, the phone's model and manufacturer, and the phone number. The app searches the phone for images, videos, audio recordings, and files [...]

So, instead of trying to tamper with this spyware in any way (which can get you in a much bigger trouble), simply don't do anything suspicious on this phone and let this app do its job. Prepare against it by not having any photos, videos, audios, file, etc., and instead use the right tool for the job. Use some other secure software/hardware to connect to internet, use encrypted email provider and do all of your communication through the computer where you can do communication safely, and store all of your files somehow in a safe place (encrypted, somewhere on computer or USB, etc). Pretend to be an obedient citizen and use the right tool for the job to do whatever it is you don't want your government to find out. 
Some people may wonder why bother having a phone in the first place (and FYI, I asked the same question under OP's question, for clarification). My answer is: 

to make phone calls (and have conversations which are not going to be considered by Chinese government suspicious, in case they are tracking that too)
to use it as a ""red herring"" - if police asks you to give them your phone you won't have to lie to them that you have no phone, or worry that they will find out that you tampered with app, or get in trouble if you don't have app, etc. You'll just confidently give them phone, with no ""illegal"" information on it, they will check it, and walk away. You may, actually, even have some ""red herring"" files: pictures of nature, shopping list (milk, eggs, etc.), etc., just so that they wouldn't suspect that you deliberately not using your phone for such purposes, and harass you farther. 

I mean, not long ago mobile phones didn't even have the ability to store pictures, videos, files, etc. 
Are you willing to put your life in danger simply because you want to have some files on your phone?
Tough times require tough decisions.
",335,2018-09-24 18:47:56Z,206
19,2,"
Get a phone which doesn't support Android apps.
Why are so many of the answers complex? And not just complex, fragile and suspicious and downright dangerous to the questioner? 
You want to use your phone to send messages and make calls, right? You don't want this app installed, right?
Say hello to your new phone:

Good luck getting an Android app running on this. 
It's probably not illegal to have an old phone.
",177,2018-09-26 11:47:30Z,209
19,3,"
This is a tricky one.  It goes without saying, but it's also a dangerous one.  Attempting to circumvent these restrictions and getting caught doing so will potentially cause a lot of legal trouble.  If they throw people in jail for refusing to install the app, I wouldn't want to figure out what they do to people circumventing the app restrictions.  It is especially relevant because even experts in tech security have gotten caught by their governments despite extensive safeguards (the founder of Silk Road is a great example and is now serving a life sentence).  Granted, evading this app is most likely a much less serious ""crime"", but the Chinese government isn't exactly known for lenience here.  So while I would like to answer your question, please don't take this as me suggesting that you actually do any of this.  I consider myself a tech-expert, but I still wouldn't do it.
Still, to answer your question, you have a few options.  I won't bother mentioning the ""Get a second phone"" option because you've already ruled that out.
1. Virtual Machine/Dual Boot
There are some options for ""dual booting"" android phones.  I don't have any examples to immediately link to (software suggestions are off topic here anyway) but there are options.  If you can get your phone to dual boot then you can install the tracking software on one ROM and then do all your personal stuff on the other.  You may need to put some basic information on the ROM with the tracking app installed just so you don't raise too many flags.
Of course there are still risks here: risks that they might reboot your phone and notice, risks that they might realize you have a completely different system installed next to the tracked one, and the simple risk that you would go out and about and forget to reboot into the ""tracked"" system, allowing a police officer to find and install the tracking app on your actual system.
2. App modification/interceptors
If this app creates enough bad press it is possible that anti-tracking apps or hacked versions of this app may start floating around that try to automatically protect you from it.  I would not expect there to be any general tools already available that would protect you from this, so this is something that would simply take lots of googling or (perhaps) requests to the right people.  This has a major downside that unless you are an expert at reverse engineering, there isn't much to do to make this happen.  It's also hard to estimate what the risks of detection are.  That will obviously vary wildly depending on the skill level of the person who put it together.
3. Server Spoofing
Depending on your level of technological know-how you might be able to put something together yourself (note: this is not for novices).  Based on what I know and my experience in this area, I'm going to try to summarize some details about what a server-spoofing measure might look like.  Again, I'm not summarizing this because I think you should do it, but because understanding how things like this operate can be generally informative and also help understand the risks there-in.
Built-in security
First, we need to understand how this spying app might secure itself.  From all information available so-far, the answer is ""it doesn't"".  This is a pretty simple conclusion to come to because the app communicates exclusively through http.  It is very easy to intercept http requests, either from the device itself (if your phone is rooted) or with network sniffing tools on a computer attached to the same network as the device.  Most likely it is also very possible to easily figure out how the app authenticates itself with the end-server and how the end-server authenticates itself with the app.  In all likelihood there is no authentication in either direction, which means that spoofing requests in either direction is trivially easy.  This might be hard to believe (given that a country like China sets aside lots of resources to invasive technology like this), but the reality is that if the people who developed this app wanted to secure it from outside tampering, using HTTPS for transit would be the very first step to perform.  It is cheap, easy, and very effective.  The lack of HTTPS means that it is very likely that there is no actual security in this ecosystem, which is a plus for anyone trying to evade it.
Sniff all traffic coming out of this app to determine what requests/responses it makes
This is the first step.  By watching the traffic leaving this app (which can be easily intercepted in the network itself since there is no SSL encryption) you can figure out what requests it sends to the destination server and what responses it expects back.  Understanding the underlying API is critical, but easy due to the lack of encryption.  This will also let you know if there is any authentication happening in either direction.  If there is, you can at least see the full request and responses, so you can most likely figure out how to spoof it.  It is possible that there is some hard-to-reverse-engineer authentication going back and forth, but again, given the lack of basic encryption, I doubt there is any such thing built in.
Figure out if the app is talking to a domain name or IP address
The destination server the app is talking to is either found via a DNS lookup or has its IP address hard-coded in the app.  In the event of the former you can edit the DNS for your android phone to repoint it to a different server, including one running on your phone.  In the event of a hard-coded IP address you will similarly have to redirect all traffic to that IP address to your local android phone (presumably you can do this with Android - you can with other operating systems, but you would definitely have to root your phone).
Setup a replacement server
You then setup a local server that responds to all requests just like the server did in your initial spoofing.  You would have to get this server to run on your phone itself, that way it is always available.  This doesn't necessarily have to be complicated (although that depends on how detailed the actual server interaction is), as you don't actually care about keeping any data on hand.  You just need to make sure that you provide valid responses to all requests.
Risks:

The app may auto-update itself (although your mock-server may make this impossible) and point to new domains/ip addresses, suddenly removing your protections
If there is an auto-update functionality and your end up unintentionally killing it (which would be good per point #1 above), a police officer may notice that it is not properly updated, flag you for ""extra"" checking, and discover what you are doing.
They may do server-side tracking and discover what you are doing because they don't find any data on their end for your particular IMEI (because your mock-server acts like a black-hole and sucks up everything).  Even if you send spoofed requests there will be easy ways for them to determine that (imagine the police copy a blacklisted image to your phone and discover that the app doesn't block/report it)
They may have root-checking in the app itself, which will cause you problems

Actually, that's it
I was trying for a longer list but that is really what it all boils down to.  Short of not carrying around a phone or purchasing a separate one, these are about your only options.  For reference, I haven't gone into details about the server spoofing because I think you're necessarily going to go out and do it.  If anything, I've gone through it because it gives opportunity to talk through the risks in more detail, and those should make it clear that there are a lot of risks.  Even if you find a solution from someone, they have to deal with all of these same risks (or ones like it).  Right now this app sounds like it is poorly executed and easily fooled, but depending on how much the Chinese government decides it cares, that could change very quickly.  At that point in time not getting caught basically turns into a cat-and-mouse game with the Chinese government, and that isn't realistically something that someone can continue to win for an extended period of time.  There are a lot of risks, so tread lightly.
",79,2018-09-24 18:55:17Z,214
19,4,"
They can execute code on your device while they have physical access to it. And you can't refuse it. I'm sorry to say that but you are basically doomed. There's no way to trust this device anymore. That's part of the 10 immutable laws of security. In your case the rules #1, #2, #3, #6 and #10 are applicable.
But when you act like you don't trust the device you could raise their suspicion. Maybe. Because nobody knows what they are actually doing with the collected data. Maybe nothing at all. In the ""best"" case it's primary for spreading FUD.
But when they are actually using the data it's easy for them to spot burner phones and all kind of tampering. As far as I know you only get a SIM card by identifying with your ID. Since the spyware reads identifying information like IMEI and IMSI they can simple compare the collected data from the phones with the purchase records. They can combine this with behavior tracking based on metadata collected on the phone (Which apps are used and how often, how long the screen is on etc.) and the mobile network (usage of data, location based on cell tower etc). Since they can do that on a large scale, they can spot strange usage patterns by your usage history or how a ""average"" user is behaving. Of course there's a vast amount of ambiguity and such in this data but they have the ultimate interpretational sovereignty.
You must also keep in mind that you need to keep your measures working all of the time because it could always happen that you get stopped on the street again.
I'd like to emphasize on rule #10. You are basically trying to solve a social problem with technology - just as your government does.
",54,2018-09-24 21:51:43Z,216
19,5,"
Use a custom ROM (two, to be correct).
Android phones can have more than one ROM installed, and you choose one or the other. So install two copies.
On the clean ROM you install the spyware, anything not dangerous, games, whatever you feel clean. On the secure ROM you install things you don't want anyone to know about.
Keep the clean ROM running almost all the time, specially when you are out of home. Boot on the secure ROM only when you really need.
You will need to keep a secure mindset too, to have awareness of what ROM you are using and what content you can create or access. That is the main point of failure. Using a different keyboard on each ROM, or different OS languages can help: Chinese on the clean, English on the secure, for example.
But first you must weight the risk/reward of doing so. If the risk of getting caught plus the mental effort to keep activities and files containerized is worth the benefits of bypassing the spyware, do it. Don't do otherwise.
",28,2018-09-24 20:42:44Z,219
19,6,"
I'd recommend you just go with it. The Chinese police doesn't just stop any random person in the street and asks for their phone. They stop Uyghur.
This happens for reasons which are somewhere in between ""mitigate a real threat"" and ""Woah, no go, dude"", but whatever it is, it's what the government does, so it's legal and ""right"". No benefit of doubt, and no assumption of innocence, no Sir. By Western standards, it's kind of unthinkable, but you cannot draw to the same standards there.
So the situation is that you are easily identified as Uyghur, both from looking at your face, and from the fact that police knows. They know who you are and where you live. And sure they know whether you've been stopped before. Again, you're not being stopped at random. You're stopped because you are already a well-identified target, on their screen.
It isn't even unreasonable to expect that your Internet traffic is monitored (targetted) and even asking about how to circumvent the measures may move your name onto a different, more high priority list.
You can bet that police keeps a list of people where the spyware has been installed (with device IDs), too. If no data comes in from your device, well, guess what. You'll be stopped again by police, and they will look very carefully why this isn't working.
Insofar, it is kind of unwise to try and circumvent (and risking being caught) what police wants. From their point of view, you are a possible criminal, and a possible terrorist. By trying to circumvent the measures you prove that you are a criminal.
The surveillance happens on the base that if you have nothing to hide, then you need not bother if they're watching you. Again, by Western standards, this stance would in no way be acceptable. But whatever, in China it's perfectly acceptable.
I wouldn't want to risk disappearing in a detention camp if I was you. Rather, let them have their spyware, and simply don't do anything that isn't opportune to the system.
",15,2018-09-25 16:46:31Z,221
19,7,"
First of all, I think you should search for solutions that are already implemented by other people. For instance, what do other people in your case do to prevent the spying activities?
One possible solution would be to have a man-in-the-middle implementation analyzing the information that is being sent, altering it, and sending it to the same server and port the spyware is trying to connect to.
I read a bit about the functionality of the app, and the information it gathers is, and I quote from the Wikipedia source you provided:

sent in plaintext

Hence, after doing some tests with a packet sniffer tool and clearly understanding how the spyware and server exchanges made using the HTTP protocol work, you could, if you have root access to your Android phone, redirect the traffic of the spyware app to a process that is running on the background of your Android OS. This process would change the data that is going to be sent to the server the spyware is trying to connect to. That way, you can send data that matches another cellphone (maybe, literally faking the data is a bad idea, because that can trigger alarms).
You should also take into consideration any kind of validation processes that the spyware has implemented so you do not alter them. More specifically, the data that is in the HTTP packets’ headers and that is sent from the spyware app to the server to gracefully initiate an upload.
Of course this is theoretical, but it is a realistic thing to do. Also, you probably will require knowledge of Android programming (mostly in C or Java) and IT.
This approach is stealthy and will not require an uninstall of the spyware app. There is always a risk, but in this case, depending on the data that is actually spoofed, the risk is minimal.
",11,2018-09-24 15:43:41Z,224
19,8,"
Due to the nature of the spyware, they will be able to detect any mitigation techniques which will make you a person of interest to them.
I know you said you can't afford two phones but it really is the best advice - why not clean and refurb an older phone if you have one around?
A burner phone doesn't need to be anything special and even better if it isn't a smartphone.
",9,2018-09-24 13:41:15Z,229
19,9,"
One idea is to think of your phone as a networking device and nothing more.  If you carry a secondary ""tablet"", that is NOT a phone, you can tether through your phone, and use a VPN on the tablet to protect your data.  Now you can hand out your phone to be inspected as all of your actual important data and work is on your tablet which is clearly not a cellphone.  If traditional hotspotting is too dangerous, you may want to consider alternative methods such as using a USB2Go cable or bluetooth pairing. If hotspotting is detected or blocked, you might also be able to use an app like PDANet to bypass those restrictions.
",8,2018-09-25 13:52:59Z,231
19,10,"
Disclaimer: I live in North America.

Knowing that I may be forced to install it sooner or later, what are my options to prepare against it?

Remove illegal items and anything else that you think that the government wouldn't approve of from your property - your clothing, phone, desk at work, home, etc.
If the government can't find you doing anything wrong then you only have to worry about someone planting false evidence on you - you need to search your own stuff from time to time.
When I go through Customs I've made absolutely certain that I have checked everything for anything I shouldn't have, because you know that they will likely check when you go through the port of entry.
As a result of my efforts they've not found anything to object to and have even simply waved me through a few times. Nothing to see here, move along.

Ideally:

Make it appear like the app is installed and working as intended, without having it actually spy on me.


We have different ideas of what's ideal.
Ideally I'd prefer to be paid millions of dollars per hour.


I don't know whether it includes sophisticated anti-tampering features or not. I can't afford two phones nor two contracts, so using a second phone is not a viable option for me.



It wouldn't make any sense for it not to detect tampering.
The APP is probably an excuse.
If they obtain the information by another means (like monitoring the cell phone towers and WiFi, along with all Internet traffic, and then there's your neighbors whom earn a healthy living turning people in) they can say in Court that they obtained the information from the spyware - that way you don't know how the information was actually obtained.
This happens in more places than just where you are, it's different where you are in that roving gangs force you to install the APP. In other places (including North America) they get by without using an APO and rely on other techniques.
Proof: In the last few months people whom have a lot of contact with children (Coaches, High School Principals, etc.) have had their work, home and computers searched for possession of inappropriate images. This appears in the news monthly.

Moral of the story: If you are poor and unsophisticated don't fight the rich, powerful, intelligent army trying to do something that they can find someone to back their actions to prevent you from doing it.
If you can't afford a second phone and contract that's a hint that you couldn't afford trouble in the first place, and the fine.
In your country they are upfront about it and demanding because objections fall on deaf ears. It's not much different in North America, just that they are sneaky to avoid complaints and only focus on major infractions so people don't suspect that they can see the lessor consequential things just as easily.
",4,2018-09-30 13:09:39Z,233
19,11,"
While other answers do provide useful insights, my alternative would be to use the phone as you normally would but isolate the app access to data
One method has been suggested by rooting the device, installing Xposed and corresponding modules of Xprivacy / XPrivacyLua. While I personally use this, rooting the device, managing additional risks due to unlocked bootloader and malicious apps gaining root access is yet another challenge - IMO beyond the scope of average user and hence easy alternative approach below
There is an open source app called Shelter that does two things

Use the ""Work Profile"" available in Android from version 5.0 (Lollipop) onwards
It allows you to install the app of your choice, in this case, your spy app in the work profile. Once it is installed on the work profile (by cloning) , it essentially is on an island and can't access any data that is outside your work profile (your photos, emails, SMS or any other app information). You can safely uninstall the original app and if police check, show them your app, which has a padlock icon indicating it is on your work profile. You can keep the app always in your recents or Overview and pull from there so that they don't even see the icon

In simple terms, the spy app has nothing to spy on!
Download Shelter - an open source app from F-droid or GitHub. Quoting the developer, relevant use case

Run “Big Brother” apps inside the isolated profile so they cannot access your data outside the profile

(Emphasis supplied) 
I am currently using this, in addition to XprivacyLua. For more details see WhatsApp: How to isolate contacts and photos - Privacy concerns 
",3,2018-10-01 09:03:49Z,235
19,12,"
For a rooted phone (without you will have a hard time to stop the app from doing what it is doing), you can use AFWall+ to prevent the app from phoning home and XPrivacy.
XPrivacy on Android up to 6 is better than XPrivacyLua (Android 6 and newer), because the old version can block more different things.
This setup works best, if you can install the app voluntary at a time nobody is watching you configuring the security solutions. If you can only get the app when a policeman stops you, it is a bit harder to hide the security apps.
",2,2018-09-26 09:49:13Z,236
19,13,"
I wouldn't be shocked if there isn't a fake version somewhere--looks exactly real but doesn't actually spy.  Unless the police are actually checking functionality in some fashion this would stop them.
As others have said, though, getting caught fooling the cops would not be a good thing!
",2,2018-09-29 02:59:38Z,237
19,14,"
There are a lot of great answers, but most of them give solutions that are beyond the ability of the average Android user.
I would like to offer a compromise between absolute privacy and getting spied upon.
I assume that your government has other means of acquiring the IMEI and phone metadata. You can stop the app from acquiring other information by denying it permission for:

Body sensors
Calendar
Camera
Contacts
Location
Microphone
Phone
SMS
Storage

Here are instructions on how to do it.
(I am not sure of your Android OS version, but the steps should be similar.)
This way the spyware would not be able to collect information about you, and therefore I assume that it will either not try to send information to servers, or it will send empty information.
However, this means that you install the app before police stops you and makes you install it. You can let the app download, kill the network connection while it's installing, and then remove permissions before it starts.
Risks
You should be aware that if the police does a deeper investigation, turned off permissions would be evidence that you have willfully tampered with the spyware. You should calculate if this risk is worthy.
This will also be a useful method of stopping other spyware (such as your flashlight app that requires contact information) from breaching your privacy.
",1,2018-10-02 04:29:12Z,238
19,15,"
You can simply resolve the servers it attempts to connect to back to localhost. 
This can be accomplished by modifying the /etc/hosts file on your device if you have root. 
Alternatively, you can use a local VPN which doesn't require root, such as this. 
",0,2018-09-25 21:42:49Z,239
19,16,"
Based on your question I cannot be sure whether you are a Chinese resident, or someon who is planning to visit (for instance a tourist).
It was already mentioned earlier that so far it seems that only certain minority citizens people are targeted (and if they were to start targeting foreigners that news is sure to spread very quickly).
Regardless of what I think about that, the following bit may actually help most tourists:
The google play store is currently blocked in China, so though it is easy to overcome, this likely means that you will not be able to get the app through the standard procedure.
",0,2018-09-27 14:53:24Z,240
19,17,"
(radically edited) 
I had proposed that one idea to consider was that

Playing dumb can be a good strategy.  
For example, ask the phone vendor, :""Hi.  My phone is working really slow and keeps crashing.  Can you show me how to do a factory reset?""  

However, I've been informed (see comments below) that even if this led to it being documented that someone else had removed the spyware, the person would likely still get in deep trouble when it was noticed that the spyware was no longer installed.   It sounds like the answer to the question, ""Is there any excuse for not having the spyware installed that would be considered valid?"" is 'NO!'  So it wouldn't matter whether the factory reset is more effective at wiping the system than the spyware is at staying installed.  I would say don't risk it, especially if you think you'd make a high-value target.  
Sadly, the software is effective at repressing dissidents and dissent even if technically it is 100% defective.  
So my answer is that secretly helping to evangelize, build or crowdfund efforts to make it not work or prove it doesn't work well or to break it or make the phone to appear broken when spies try to install spyware would be better than a more direct approach, which would paint a target on your back.  Good luck, and may the force be with you.
",-5,2018-09-25 00:03:02Z,243
19,18,"
I agree with Doomgoose. Get a burner phone for this. Alternatively, get an app called Orbot (it's Tor for Android) which can get you a bit of the privacy you so desire by encrypting your online activities. Another alternative is adding a VPN on top of the equation.
",-15,2018-09-24 15:12:23Z,245
20,1,"

That way the hacker won't know if they have got the login details correct or not.

If the information presented after login has no relationship to the person who the login should be for, then most hackers will quickly recognize that the login is probably not the real one. 
But, in order to show information which looks like it fits the user, considerable effort could be needed. It also needs to be created specifically for each user and show some true information about the user so it does not look fake but not too much so no important information is leaked. 
You cannot expect your provider to do this for you but you might try to do this yourself in many cases, i.e. add another email account, another facebook account etc. 
",66,2018-06-01 12:58:38Z,66
20,2,"
The concept you're describing is called Plausible Deniability and methods to provide it have indeed been implemented in some software, VeraCrypt being one example.
One problem with implementing it in websites, as you suggest, is that it's very hard for the website developer to come up with fake data that is realistic enough to fool an attacker while not giving away any sensitive data about the user. In encryption software like VeraCrypt, that task is shifted to the user, who is obviously in a much better position to do that.
",50,2018-06-01 12:58:54Z,258
20,3,"
Because hackers don't attack login forms
The flaw is that you assume hackers get into accounts by brute-forcing credentials against remote services. But that's futile anyway.
Any website with decent security (the ones without decent security wouldn't care about your idea either) will have a limit imposed on how many failed login attempts can be made in a certain timeframe per IP address, usually something like 5 failed attempts every 6 hours. If security is a bit stronger, accounts might also need action from the owner after a number of failed attempts, and/or the owner might be notified of failed login attempts or even all logins from new devices.
So while brute-force attacks may well be feasible against plain data (such as password hashes exposed in a breach), they are nowhere near feasible against any service with even a bit of security.
For attackers, it is thus much easier to go phishing, or better yet set up a genuine free service themselves and work on the assumption of password reuse:

",31,2018-06-02 04:07:43Z,261
20,4,"
I have never heard of any service or device implementing this either.
The case where an attacker is present and forcing you to login is pretty unlikely. They are more likely to just take your $1000 iPhone and run.
However, it is very plausible for this to happen if the ""attacker"" is a security guard/TSA officer at an airport security checkpoint. Especially if you are in a foreign country. (There was a PHENOMENAL Defcon talk on this subject a few years back.)
Websites
It probably wouldn't make much sense to implement this on a website. If you (the admin) are certain that someone who is attempting to access an account is a hacker, just block them/lock the account. Problem solved.
If the attacker is trying to access multiple accounts, they will probably know something is fishy if they are able to ""successfully"" login to multiple accounts on the first or second try.
Phones
While phones don't allow fake logins (?), but you can set them to lock after the password isn't entered correctly n times.

Attacker/TSA agent tells you to unlock phone. You intentionally enter wrong password on 1st try.
""Oh, oops, wrong password...""
You enter the wrong password again on the 2nd try.
""Sorry, my hands get sweaty when I am nervous...""
You enter wrong password on 3rd try. Phone is now locked for 30 minutes!

This of course will not work if you are reciting the password to the attacker, and they are entering it in the phone. And I think most phone lockouts only last for 30 minutes (?), during which time the attacker/TSA agent will do their best to ""convince"" you to remember the password in a back room.
Laptops
Your suggestion would be relatively easy to implement on a laptop...
Create 2 or more user profiles.
The first profile you name after yourself (first and last name). You set a picture of yourself as the profile picture. This will be your ""fake"" account. Set the password as something simple and easy to remember. Put some ""personal stuff"" in the account (music, pictures of your pet, ""work"" documents, etc).
The second account you give a generic family member name (""hubby"", ""the kids"", ""honey"", etc). Keep the default profile picture. Set a strong password. This will be the account with admin privileges on the laptop, and the account which you will use for your important/confidential work.
Now imagine a scenario in which you are forced to login...

You are in an airport in Oceania, about to fly home to Eurasia. Airport security stop you on your way through the terminal.
Security: ""Give us your passport and laptop!""
You hand them the laptop and passport. They turn on laptop, and try to login to the account which you named after yourself. Upon seeing they need a password, they demand you tell them the password.
You: ""The password is opensea. No spaces.""
The airport security enter the password, and successfully enter your fake account.
After looking around for a few minutes and not finding anything that interests them, they log out and try to login to your real account.
Security: ""Whose account is this? What is the password?""
You: ""That is my kids' account. The password is 123dogs.""
They enter the password, but are unable to login.
Security: ""That password is wrong! Tell us the correct password!""
You act surprised, and ask them to give you the laptop so you can attempt to login. They hand you the laptop, and you start typing in bogus passwords.
You: ""Those darn kids, I told them NOT to change the password! I'm sorry, they were only supposed to use that account for their stupid video games!""
The airport security confer with each other, and then let you go on your way. You safely return to Eurasia without having the confidential information on your laptop compromised.

",14,2018-06-01 21:45:19Z,266
20,5,"
It's not exactly the context you had in mind but there are in fact systems which implemented this idea. I used to work at a (somewhat sensitive) facility where each employee had two codes to disable the alarm system: The regular one and a duress code. If you used the duress code, the system would be disabled so as to not put you in danger but a silent alarm would go off at the monitoring centre. I am reading on Wikipedia that this was also considered for bank ATM in the US but ultimately ruled out.
Another similar concept is the “honeypot“. Some of them might in fact accept any credentials or serve dummy data when attacked, to be able to record what an attacker does next or otherwise exploit the situation (e.g. capture the payload of a worm).
As to why it's not more common in consumer products, online services, etc. there is simply a trade-off between the benefits (how likely a particular attack is, whether it would effectively deter criminals or just prompt them to slightly alter their technique) and the costs (more complex systems to develop, maintain and certify - which also means increased attack surface that could afford an attacker with an actual entry point, bandwidth and operating costs to serve the dummy data to all botnets constantly attacking online services, effort to create credible dummy data to fool more sophisticated attacks).
",13,2018-06-02 09:20:20Z,262
20,6,"
This is called 'Deception Technology' in the cyber world where the solution deceives cyber foes (attackers) with turn-key decoys (traps) that “imitate” your true assets. Hundreds or thousands of traps can be deployed with little effort, creating a virtual mine field for cyber attacks, alerting you to any malicious activity with actionable intelligence immediately. The traps would carry login details, dummy data, dummy system, etc to deceive the attacker by intimating like actual system.
Deception technology is an emerging category of cyber security defense. Deception technology products can detect, analyze, and defend against zero-day 
 (where the attack type/procedure is not known before) and advanced attacks, often in real time. They are automated, accurate, and provide insight into malicious activity within internal networks which may be unseen by other types of cyber defense. Deception technology enables a more proactive security posture by seeking to deceive the attackers, detect them and then defeat them, allowing the enterprise to return to normal operations.
You may refer below link for some solution providers:
https://www.firecompass.com/blog/top-5-emerging-deception-technology-vendors-at-rsa-conference-2017/
",5,2018-06-03 16:32:45Z,268
20,7,"
This has been done in the past, rather successfully, but depends a lot on what the system is. 
At one point it was not uncommon for paid access websites to auto detect when an account was logging in from too many IP addresses or with other suspicious patterns and redirect those users to a version of the site that was primarily ads and affiliate links to other sites. If done well the sharing of stolen login credentials could become a revenue center. 
There are also websites that direct users to different versions based on IP address or other criteria; the simplest version of this is targeted advertising. 
For shell access it's possible to direct a login to a chrooted jail that has only a small section of disk and specially provisioned binaries, which may not necessarily be the same as the general system. 
",2,2018-06-02 05:33:46Z,269
20,8,"
First off, I wouldn't call the attacker in this scenario a hacker. A hacker is trying to get around the security that the website offers, in your scenario the attacker doesn't care how secure your services are, he cares how easily the user is intimidated and possibly what to do with the body afterwards.
Secondly, alternate credentials that change your access has been done, but if it does more than present a restricted view of the truth, is a lot of work and of limited utility.
The reason it is of limited utility, is because your users know about it,  you must presume that any attacker knows about it as well.  Suppose you did this for an ATM card so that it showed a balance that was less than a hundred dollars in order to limit your loss. Either the attacker asks for both (in which case the victim has at best a 50% chance of not loosing more) or simply includes as part of his demands that it produces more than that -- ""if I don't get at least 200 you're dead"".
It's not totally useless, but is only effective against an ignorant attacker.  Relying upon the attacker not knowing something is called security through obscurity, aka ""they got it"".
",2,2018-06-02 12:16:20Z,270
20,9,"
For web and  a remote attack, as many folks here stated before, appart from the difficulty of creating fake user's content  , there is the problem of : how do you know it's a compromised login?
I mean, if you assume there is some sort of suspicious activity , like a brute-force attack, you can just block the login for that IP and maybe for that account itself for a while (until the real owner somehow validates its identity)
The only usefull cases are forced logins, that's another story and a pretty cleaver idea. Here is the implementation I imagine for a social network:

The user creates himself an account with dummy data, and set it as his dummy account.
When there is a forced login going on, like you jelous gf or bf extorting you to login , then you put the dummy password and there is! you are logged in your beautiful self created dummy account.

BUT Its  not a perfect solution either. The attacker probably would know you and, if it's your crazy ex  for instance, she may just check her chatlog with you and will know you just logged on the bogus account. 
This is specially relevant because it will be a public  well known feature of the platform you are , so anybody who forces you would be able to check wheter are you or not.
For banks or other sites, it's a pretty good idea.
",1,2018-06-02 19:10:47Z,253
20,10,"
The problem is that your users have to know about it, thus you must presume that any attacker knows about it as well. 
",-1,2018-06-02 12:53:22Z,271
20,11,"
To me, this sounds as if you were inviting the attacker to dance with you. 
""Hey attacker, you want to hack my website? Well here is a fake login web site for you!""
You certainly do not want to invite the attacker to dance with you because he might find it amusing and challenging which would give him even more motivation to try to hack into your website.
",-2,2018-06-04 15:22:26Z,273
21,1,"
Let's go through the process of what actually happened:

Telegram requires a cell phone number to be linked in order to create an account. 
To verify that the number exists, they send out a verification code for you to enter in the app while creating the account.
This person obviously didn't want their own cell phone number to be linked to the Telegram account, which is an indication they might use it in malicious ways (terrorism, hacking, …), so:
They tried to set you up to get that code.

So to answer your questions:


Was this a scam?

I'd call it gray zone. But yes, someone did try to scam you for a Telegram verification code.

If yes, did I prevent the scam by denying this user the code?

Yes, you did well, using common sense :-)

If not, what's at risk? Is my bank account at risk, for instance? (I only gave the user my cell phone number. No name, no address, nothing else whatsoever)

That depends, how visible are you on the internet? If you have domain names registered with that number, they can find that. Facebook linked? They can likely find that depending on privacy settings. In brief, if you've linked that number to a lot of accounts, they might find those accounts and the information that's publicly in them.

If I am at risk, what do I need to do to prevent any theft of my information, passwords, money, etc.?

""At risk"" is a heavy word in this situation. They just want a Telegram account; I find it likely they just moved on to one of the other 50 people they PMed while talking to you. 

Just keep an eye open for ""strange things"" (like hacking attempts to other accounts etc.), research yourself with that phone number and see if what you can find needs changing. 
",108,2018-03-29 16:41:00Z,276
21,2,"
It could easily have led to a scam. The request was to get access to Telegram using your phone number. 
Works just like Whatsaap, when you register, you enter your number and a code is sent to your phone. 
This could easily be removed by re-registering Telegram on your phone where it would repeat the process but make the other user using it unusable. 
You may not use Telegram but some of your contacts may use it, and may try to contact you because they see you on the app. They may be conned in the process as they could think its you. You were clever to think twice but next time avoid giving your number so freely.
",16,2018-03-29 20:49:58Z,279
21,3,"
There's also the risk of targeted attacks to your smartphone by having your number (e.g. Android or iOS vulnerabilities) through MMS or SMS. But the risk is low.
If changing your number is not to burdensome, you might want to change it.
And kudos on your quick thinking!
",6,2018-03-29 18:32:21Z,284
21,4,"
Contrary to what one of the other answers said, I wouldn't change your number over this. Most of the nuisance stuff from having your number comes from bulk data collection services who resell it. This girl, whomever she is, just wanted to create an account for potentially negative purposes and thought you were a useful patsy (props for having some common sense to not feed her what she wanted). I seriously doubt she'll contact you again, or even share your number with anyone.
Had you given in, I suspect she would have strung you along and tried to get you to do other unwise things (you'd be amazed what people will do for a female voice and a pretty picture, not having met the person).
",5,2018-03-30 12:21:00Z,13
21,5,"
I understood her intention i.e she wants to open a telegram account with your number. Telegram is a good app to maintain privacy but remember criminals also use telegram to maintain their privacy. Hence why she wants to do such a thing so that no one can discover her identity.
Note: Telegram is encrypted so what I mean by identity is her location.
You are completely safe because you didn't give her the code. As I earlier said, telegram is best for maintaining security, privacy.
But next time don't give your number to an unknown person :)
",2,2018-03-31 21:08:59Z,285
22,1,"
Unless you can come up with some other explanation of how this happend, it sounds like your phone has been infected by some malware. It's impossible for us to say if the infection was the result of something the factory did or something you did. Either way, you should be very concerned. I'd recommend the following course of action:

Make a backup of any data you have on the phone. (The backup could be infected, see this question, but if you have no earlier backup you either have to take the risk or lose your data.)
Do a complete factory reset, wiping the phone clean.
Change any passwords that has been stored on or entered on the device.

While you are only seeing one app, it might just be a symptom of a deeper infection.  Just removing the strange chinese app may not be enough.
",103,2018-03-27 17:13:16Z,9
22,2,"
I would be very concerned. One thing I have learnt about security, in all the years I have been trying to understand it, is that if you didn't put something there, somebody else did, and if you don't know what it is doing then the only thing to do is reset, reinstall, and be happy with everything. If you don't know why that software is there, be assured, somebody put it there for a reason and it may be a good reason but it isn't your good reason so if that phone was mine I would save all my data, and only my data, and completely factory reset it.
",31,2018-03-27 18:32:02Z,290
22,3,"
Yes, and be concerned about more than your phone. 
I can’t imagine a situation where an app was installed without your pin/password. without them first jail-breaking or otherwise significantly compromising your phone's OS. I think the only safe assumption is that anything your phone had access to, they also had access to. 
So any account your phone has access to is suspect. Particularly any email account you have setup. Look for password reset emails, check your sent items etc. If you had any kind of financial app ( bank etc) check for odd transactions etc.
If that all looks fine,  I’d follow the advice of others in terms of resetting the phone, but also go beyond the phone and reset all your important passwords etc.
",22,2018-03-28 02:29:18Z,292
22,4,"
The most concerning point is that the repair shop employee claims not to know anything about it.
The app may or may not be okay. If someone in the repair shop installs a new app, i.e. for testing if installing works again, this can be a problem, but doesn't have to be.
But if they tell you they do not know about it, either it was some malware installed it, or they are lying to you. And either way you cannot trust the phone anymore not to be infected with something which should concern you.
",11,2018-03-28 08:50:23Z,236
22,5,"

when the guy in store suggested to just delete it I did it without thinking. Here's the link to icon I found that looks the same, I suppose it's UC Browser. As I mentioned, everything was in Chinese, including name of the app. – Rafi Rosa

Yes, it was UCBrowser. This is a popular browser in China (since Google Play/Services are not available there) most Chinese phones actually come with this browser pre-installed.
I believe it was perhaps a way for the repair guy to test the phone in a familiar app... UCBrowser was found to have several flaws that would allow others to exploit, but mainly they'd only happen if you actually used the browser.

I have done a complete factory reset of my phone and changed all passwords that I used while using it. After that all of apps I have previously installed were restored by iPhone and all of my files were stored on iCloud, is it all I can do? – Rafi Rosa

I believe you did well. Even if it was just an app that the repair guy decided to use for testing, he might have done a lot more than just installing and using the browser. Not being honest/upfront about what they really did when they just needed to repair a battery is certainly not professional.
A factory reset and passwords change should be safe enough regarding your data. If the store does not have ways to profit from replacing other hardware in your phone, you won't notice any difference. However, if you need to be completely sure about its integrity perhaps a certified Apple store may perform a check and tell you what was really changed (not sure if they do this though).
",10,2018-03-29 09:23:21Z,295
22,6,"
I didn't see this addressed in the previous answers, so I thought I'd add:
If you are going to surrender control of your device, you should perform a backup and a factory reset beforehand. You should reset it again and then restore your backup when returned. It only takes a few seconds to copy your files or install malware.
It is too late in this case, but the mystery about ""what happened"" applies every time it leaves your control---not just this time because you noticed something wrong. Intruders usually try to avoid detection, so a good intrusion would be unnoticeable.
",6,2018-03-29 18:37:49Z,296
22,7,"
As already stated, anything done to your phone while out of your sight is definitely cause for concern and probably worth restoring from backup, as well as resetting passwords / 2FA for anything tied to that phone.
However, the exact way this icon got on your home screen would say a lot about how bad of a breach may be involved and how concerned to be. I can think of 2 options not already discussed:

An employee installed the app, but removed it from your list of purchased apps. You could check your purchase history in iTunes for hidden purchases to verify whether this has happened, even for a free app.
The icon was not actually an installed iOS App, but a bookmarked website / Progressive Web App that was pinned to the Home Screen for some reason to help with diagnostics. You could potentially verify this by checking your Safari history for any activity during the time your phone was at the shop. Clearing the history/cookies would erase the tracks for this, but if that's what happened then the 'app' itself isn't really a cause for alarm.

",4,2018-03-28 20:47:56Z,291
22,8,"
Yes you should be concerned. 
Do a factory reset. 
If you have backed up your iPhone since before the app, your in luck. If you didn't, less great but still preferable to wipe it clean and start over.
",2,2018-03-28 01:50:05Z,297
22,9,"
if you gave access to the phone to the store, the new app might not be in the list of your purchased apps, simply because an employee in the store logged off, and installed it with their user.
Nonetheless, I would still handle the phone and passwords used/cached there as potentially compromised.
However, as mentioned in my first paragraph, the app installation might be just routine, and someone forgot to delete it.  
",1,2018-03-30 02:04:04Z,298
22,10,"
I would recommend you to replace your phone instead of resetting it. The store has had the access to the phone's hardware and firmware and could have replaced them with malicious counterparts in addition to installing this app.
",0,2018-03-31 16:12:23Z,299
23,1,"
They will most likely be able to see an itemised bill showing who you called and when. They will also be able to see mobile data usage.
If the phone is enrolled in the organisations mobile management system, they may be able to monitor and control app usage as well as monitor and control internet traffic.
UPDATE: In a worst case scenario they could potentially install full monitoring and install things like key loggers, this is very unlikely however. Additionally, depending on what part of the world you live in, there are restrictions on what they can legally collect, especially without consent.  
The best thing to do is ask your company what you can and can’t use the phone for. If it is a large organisation they should have a security policy and an acceptable use policy.
",33,2017-07-28 10:32:03Z,301
23,2,"
In addition to the other points that have been made, many websites use an SMS message to your mobile phone as a form of two-factor authentication, even though the best practice nowadays is not to do that. So they could at any time divert messages to your number to themselves, and use that to take over your Internet accounts, even accounts that you never use with the phone. 
",5,2017-07-28 14:42:27Z,249
23,3,"
My thoughts about having a company-managed phone or a company-paid contract, and your company Exchange. I include relevant information for iPhone devices (since the amount control varies across mobile platforms).
Phone
If you are using Exchange ActiveSync to access your company email/contact/calendar, note that ith ActiveSync, your company can:

Configure a Mobile Phone for Synchronization
Disable a Mobile Phone for Exchange ActiveSync
Enable a Device for Exchange ActiveSync
View a List of Devices for a User
Configure Device Password Locking
Recover a Device Password
Perform a Remote Wipe on a Mobile Phone
Install SSL Certificates on a Windows Mobile Phone
Configure Mobile Phones to Synchronize with Exchange Server

Apple and Google also have similar centralised management options. There may be other 3rd party services or software that perform similar tasks.
If your phone has been set up with Google G Suite, your company can:

Automatically synchronize email, calendars, and contacts with users’ devices.
Turn on or off features, such as lock screen widgets, Siri, My Photo Stream, Handoff, and iCloud Photo Sharing.
Protect your organization’s managed data by controlling which apps can be used to open documents and attachments.
Control Apple® iCloud® backup and sync, and turn on backup encryption.
Apply device-management controls, such as account wipe, encryption, and screen lock.
Keep work data secure with G Suite apps, such as Gmail, Google Drive, and Calendar. For details, see Get mobile apps for iOS devices. 

If your company was using Apple Device Enrolment, they can access, amongst other things:

Global network proxy for HTTP
Allow iMessage, Game Center, iBooks Store, AirDrop, Find My Friends
Allow removal of apps
Allow user-generated content in Siri
Allow manual installation of configuration files
Allow conﬁguring restrictions
Allow pairing to computers for content sync
Allow account modification
Allow cellular data settings modification
Allow Erase All Content and Settings
Restrict AirPlay connections with whitelist and optional connection passcodes
Enable Siri Profanity Filter
Single App Mode
Accessibility settings

Note that a remote backup includes a lot of your phone information (installed apps, etc). I'm almost certain your company could access this information if they wanted to.
Contract
This depends on whether your employer simply pays for your contract or they are the contract owners and therefore have access to the mobile operator online tools.
If they own the contract, they can definitely access the detailed list of phone calls (including numbers, time and duration). They may also be able to access your Internet data pattern usage (which may reveal some of your habits).
I know of no provider (Europe / Spain) that allows customers to access a detailed list of websites visited, or IPs accessed, but I might be wrong here. I doubt this since it would require your phone provider to do deep packet inspection and maintain quite expensive log and data mining facilities... it is definitely doable but I never heard of this.
General info
Anyone using their phone to access their company resources (even simply checking mail via POP3/IMAP) is usually revealing their approximate geographic position to their company in a periodic fashion.
If you use your company proxy or VPN to access any of their resources, note that your Internet traffic or browser behavior may be being forwarded through your company servers, which would allow them to track which sites you visit (and the content if those sites don't use HTTPS).
If your company has installed custom certificates on your phone, they could potentially also view any HTTPS traffic if you are using their proxy.
TL;DR
In summary, I'd recommend you to:

Find out if your phone has been enrolled to a remote management system.
Check what kind of information your phone company provides to their (business) customers about their contracts.
Check if your company has installed any extra software, proxy/vpn settings or  certificates on your phone (in case you handed your phone to them at any time OR allowed them remote administration).

",5,2017-07-28 14:39:45Z,228
23,4,"
The holder of a contract can almost always see the activity on that contract, that would be numbers called and very possibly the internet usage as well, that is the sites you visit and how much data you have used going to them. In most parts of the world your employer has the right to monitor your activities on business supplied services, and many use that right. Some industries are regulated and have an obligation to monitor what you do, and in these cases your calls may be recorded and your internet traffic actually saved so your actions can be reconstructed - this is a rare case. 
If you control your actual device then they can't see what you are doing on it, however you cannot assume that anything you do on your company phone is private as the connections it makes will give them a great deal of information. You could use TOR, but personally if you do things you really want to keep private then you're better off getting your own contract for your phone. 
",2,2017-07-28 10:40:09Z,101
23,5,"
There are two levels of access (okay, make it three), depending on the carrier-offered options (with ""offered"" meaning ""offered to your employer"").

Certainly all billable traffic gets forwarded to your employer, so:

numbers called and, almost surely but not necessarily, calls received even when not actually billable.
SMS/MMS sent (but not their contents)
network traffic statistics (i.e., volume in gigabytes, but not its contents).

The SIM could route your traffic to a private MPLS group and have it analysed/filtered. How much or how little, it depends on lots of factors. Potentially everything (except for HTTPS encryption, and even then, barring MitM attacks) could be accessible with you being none the wiser. This also includes SMS/MMS contents. Your employer must not simply pay your navigation, he must supply a tailored SIM.
More simply, all the traffic could be routed through a dedicated VPN, no matter what the device settings are. So your navigation effectively happens from a device inside your employer's company. Same rules apply as for #2, but this situation might be apparent through tracerouting or simply checking your apparent source IP address and/or comparing the internal device IP address (e.g. 10.123.45.6) and what appears to a third-party site such as WhatIsMyIp.com (e.g. $IP_IN_YOUR_EMPLOYER_NETWORK). This can be done even with your own SIM inside.

I strongly agree with @TheJulyPlot's answer: ask your employer
a. What is permissible on that device,
b. Whether private (i.e. third-party VPN) navigation is allowed.
If the answer to (b.) is a NO, then you should purchase a second device or look into a dual-SIM device -- but, first, ask whether that is permissible. I had a position once where I was not allowed to bring my own phone - or any other kind of electronic gadget, including my non-networked PDA (!) - inside the premises. You don't want to shell out good money for a device you then can't use.
Final note (especially for case #3): in some circumstances you might be getting flak not just for something you did but for something that was sent to you. So consider who you give (or already gave) that phone number to. ""Moving"" your own SIM with number inside your employer's accounts could backfire spectacularly if they objected to some SMS sent by an Overly Attached Girlfriend or similar.
",1,2017-07-29 13:12:37Z,0
24,1,"
Users should not hash their passwords before sending it to a website. If the user hashes and sends the hashes credentials, the hash is acting exactly like a password and actually defeating the purpose of hashing. If an attacker steals your password hashes, they can just send those to your server to login.
You might want to read about Pass the hash attacks.
You must hash server side.
",7,2016-10-11 01:21:41Z,306
24,2,"

When a user logs into their account on my server, should they send
  their raw data to me, and then I bcrypt compare them, or are they
  supposed to hash and and I directly compare?

You take the data they send you, hash it using the unique salt stored in your database next to their username, and compare that to the salted-hashed password stored in your database next to their username. This is an absolute MUST for security.
It's fine if they hash their password before sending it to you (assuming they always do that; probably because the client does it automatically). This is a nice step if you are trying to protect your password-reusing users, but the hashed-before-it-gets-sent-to-the-server password is really just acting as a pseudo-randomly-generated password.

I ask this because someone posted about hashing on mobiles being slow. It would make send to take load off of the server, and make sense about the slowing down brute forcing, and also protect the user from snooping.

Hashes aren't ""slow"" by any definition for users. They take a very small fraction of a second. Unless you're getting hundreds of login attempts per minute you shouldn't need to worry about the server load from hashing passwords.
Hashing the password on the mobile doesn't do anything for the security of this account (all it does is keep the original password from being transmitted over the wire, which is only terribly useful if the wire can be intercepted and the user is sharing passwords between services (in which case the user's other accounts are now compromised). If the user isn't reusing passwords and the wire can be intercepted then it doesn't matter if the original password or a hashed version is being sent, because what is being sent is what's used to authenticate the user to this server).
When we talk about a hash being used to slow down brute forcing, it's with the assumption that the attacker stole your database (containing the salts and hashed passwords) and is now trying to hash common passwords against each of the salts and looking for matches. A hash that's slow because it's happening on a mobile doesn't do anything to protect the user, since the attacker will undoubtedly be using faster hardware. A slower hashing algorithm and/or one with more rounds of hashing means the attacker has to wait a tiny tiny bit longer for each crack attempt. So a slow hash that takes, say, 0.0001 second instead of 0.00001 seconds would mean they could only try one-tenth as many hashes in a given amount of time. Note that if you hash the user's password at the client and then again on the server, that's just adding one round to the overall hash time (presumably the attacker grabbed a copy of the client code and has noticed that the passwords get hashed before being submitted and has written their attack code to take that step, too).

If I were to do this, wouldn't I run into the exact same problem -
  slow experience for mobile users, if so is it something that is just
  expected as part of logging in. Also, wouldn't the user generate a
  random hash which wouldn't match the one stored in my database?

The hashing on the mobile side is still pretty fast, like I said, so it shouldn't have a noticeable delay for the user.
The hashed password becomes the password used to access your server, so to your server it just looks like everyone is really good at generating random passwords. But that's why you still need to hash the ""password"" they sent you with a salt (and that will match what you've stored in your database) because otherwise once someone gets your database they can just directly send those hashes to you (bypassing the hash function in the client) to authenticate to your server.
",1,2016-10-11 05:15:27Z,307
24,3,"
This probably is a duplicate question. But I seem to keep seeing people recommend hashing passwords client-side while ignoring the true complexity of the issue and the near impossibility that anyone other than an information security scientist is going to get it right.
Bottom line, if you're making a web or mobile app, use HTTPS, configure it correctly and hash passwords on the server. Use bcrypt or scrypt, or if it's the easiest library for you to use, use pbkdf2. Those libraries are well designed. You're not going to invent better. BUT; if you use them to hash the password on the client, then send the hash, then you just made the hash your password. And if you're a mobile app developer, don't you dare set the flag in your HTTPS setup that tells it to skip checking the certificate chain.
It's dang complicated
Occasionally people mention using nonces, etc. Pass-the-hash, challenge-response authentication is nothing new. And various half-baked pass-the-hash schemes have been broken more than once. Consider Microsoft Lan Manager: LM, NTLM and NTLM with session security are all broken. NTLMv2 is apparently still secure, but it's the fourth in the chain. Microsoft has a few resources to throw at the problem. You're going to do better on your own? Kerberos is an example of a solidly secure authentication and authorization system with a lot of science behind it, which generates tickets and applies nonces and passes those around instead of transmitting the passwords.
But Kerberos is quite complex, and relies on computers having trust relationships with each other, and on computer clocks being in sync. It's complicated.
Half-baked attempts to pull off something similar between a web browser and web server, unless you're using the actual Kerberos protocol supplied by a competent, well-vetted vendor, are bound to end in tears.
Security is complicated, subtle and hard.
HTTPS is pretty darn good
Perfect? Maybe not, but current versions aren't broken. If the server is using a strong certificate (2048 bit key) and strong cryptography (TLS 1.x or greater), it's pretty darn solid. Sending your password via HTTPS is not the same as passing it in plain text.
Users should not use the same password for multiple sites
Really, seriously, if they do, it's ultimately on them. They've been warned so many times now. Even the Today Show anchors know not to use the same password on multiple sites now, for crying out loud.
Don't make the hash the actual password, believing you have implemented security
If you just hash the password on the client side and store that value in the database for comparison, then the hash itself is the password. Period.
Are you going to build a full challenge-response pass-the-hash system using nonces, and do it without introducing fatal bugs? Do you really have the time, budget and expertise for that?
There may still be a benefit for the user to hash their password before sending it. If the hash is intercepted, the attacker couldn't use it to log in to OTHER sites.
However; if that is done, and you don't have the chops to create the actual equivalent of Kerberos or NTLMv2 just for your app, it should be done completely independent of anything the server does or expects.
The server must compute the hash
Given all the preceding, the server still needs to hash whatever is passed to it (call it a hash, call it a password, same thing in this context), with salt that is stored on the server, completely blind to anything the client is doing. 
In other words, if the client is sending a hash, great, but the server doesn't know or care. It's just a password.
You're not trying to prove that the client knows the hash value. You're trying to prove that the client knows the original secret (aka ""the password"").
So the server hashes the incoming password fresh each time, using salt that is stored on the server, and compares the computed hash to the stored hash. If they match, authentication has succeeded. Otherwise, authentication fails.
So very many things can go wrong if you try to be clever
You're going to get it wrong. You just are.
Imagine this example:

Your client hashes passwords
Your server stores said hashed passwords
Attacker social engineers your CEO and steals your account database
Attacker has a database full of passwords, not hashes, but actual passwords to actual accounts on your system.

Part of my point, I guess, is that the very question ""Should I hash on the client or the server"" belies the true complexity of the issue and far too many answers either fail to address the complexity at all, or gloss over it in ways that make a system less safe, not safer.
",1,2016-10-11 06:05:48Z,57
24,4,"
Preferably both should be done, for different purposes.

The user hashes their password with a salt specific to the website, to prevent the (possibly hacked or misconfigured) server stealing their password if it is too similar to the user's password on other websites.
If you want better security, you can let the user hash their password more times, in case the server is hacked and the hacker was listening to the communication, and doesn't want announce the fact it is hacked.
The server hashes the password with a salt before saving to or comparing with the database. So it won't be too disastrous if the database file is stolen.
In addition, someone could hash the password using a slow algorithm, the earliest the possible, to slow down brute-forcing if the user is using a weak password.

But in practise, web browsers don't have the support to do 1 and 2 right. And for 1, even if a service doesn't use web login, it doesn't help much if only one service is doing that. So usually only 3 and sometimes 4 is implemented.
The user could use password management softwares to do 1. But most people don't. So sometimes we think 3 is also mitigating the problem 1 could solve. That may lead to some confusion.
For your question, it makes sense to move 4 to the client side. But 3 on the server side should be considered more important, and shouldn't be removed. In this case you are going to hash the password using a slow algorithm on the client side, and hash the result again using a fast algorithm on the server side. But I guess most services just don't use an algorithm which is that slow that is worth the hassle.
",0,2016-10-11 04:26:13Z,308
24,5,"
Already answered on StackOverflow. You want hash passwords on the client device because you want to touch the user's password as little as possible. 
1) If the user ever complains about being hacked you can be confident that your server didn't give up the user's password, because your server never sees the user password.
2) You want to minimize the amount of time that a user's password spends on any storage device- yours or theirs. If you copy the plaintext to your device then their password exists in your memory and disks. You want to avoid this.
3) Transport security isn't perfect, so if you transmit the user's password you're making them vulnerable in the event of a compromise. If you hash on the client side then you ensure that a man-in-the-middle will only ever get hashes. This is enough to allow the attacker to break into your server, but your user is protected in the (common) event that they re-use their password on multiple sites and services.
4) You can address Stoud's concern by reversible-hashing the user's hash while in transport. 
5) Then hash it again once it gets to your server. 
https://stackoverflow.com/questions/3391242/should-i-hash-the-password-before-sending-it-to-the-server-side
",-2,2016-10-11 03:17:40Z,303
25,1,"
It might improve security but it's a very strange way of doing things.
What you are suggesting is to start with an image with ""unnecessary"" software installed and remove some of the stuff you don't need (bash, echo,...). This does not only risk breaking things, it also increases container size because the base layer with the unnecessary tools is still shipped.
A better approach is to start with a minimal image. Not alpine, that's still a full os. The more minimal approach is to use a base image that only contains the required language runtime like googles distoless images https://github.com/GoogleCloudPlatform/distroless . For static binaries, you can even start with an empty image.
That way you get an image that is as small as possible and does not provide the ""attack surface"" of bash.
",6,2018-03-22 13:59:51Z,309
25,2,"
You shouldn't remove bash or any executables that exists in the base image. A lot of executables are interlinked, and removing one executables may cause other apparently unrelated executables to fail to run correctly on some corner case. Instead you should use a base image that doesn't contain anything that you don't really need. alpine makes a great minimalist base image, weighing at 5MB, it contains just the most useful stuffs and nothing more.
If you know that your application doesn't use any external binary or shell, or if you know exactly which external binary you will be needing, you may want to consider compiling into a statically linked binary and then basing your container of the scratch image.
",1,2018-03-22 10:53:34Z,310
25,3,"
The answer is: it depends. Removing bash and all other interpreters reduces the attack surface of the container. The same applies to e.g. compilers, nc, wget, ...
The real question is: is this the best way to spend the money?
E.g.

Testing and production images probably diverge 
creating a patched container gets (slightly) harder
Troubleshooting gets way more difficult 
....

A better way to spend the money might be to harden the application itself by investing in other security measures like regular penetration tests, security monitoring, automated patching, ...
",0,2018-03-22 11:15:18Z,311
25,4,"
It depends to what extent you are following the Container philosophy. If you don't need it, you should avoid using anything not strictly necessary. You should define and refer to everything you need in the Dockerfile, and the entrypoint of a web server should be the apache (or any other web server you are using) binary itself.
But more importantly, take into account that many binaries (as ls, find, etc.) are linked to the base image layer of the container, so you should leave them untouched, or make a more custom base image. Any ubuntu, debian, etc., base image will have this binaries, and tampering the base image is not recommended.
It would be a much more secure approach to define an isolated network for databases, or other backend entities, leaving just the port you are exposing to the world as attack surface.
Obviously, hardening the machine you into what you are containerizing your services is critical.
I would not worry much because of these binaries. I would worry much more about securing the access to the Docker commands to avoid invoking these binaries during the container runtime.
",0,2018-03-22 10:17:05Z,313
25,5,"
I wanted to add some more resources to the answers so far. In general, it's considered best practice for an OS container image to be as basic as it can possibly be. Removing unneeded shells is just a start. There are multiple hardened OS images out there specifically for a secure container deployment, but there are also many other things to consider, like lowest possible privileges, Read-Only file systems, and much more.
This page: https://access.redhat.com/articles/2958461 Has multiple links to many good articles. Dan Walsh's Red-Hat blog is also very good for deep level information. https://rhelblog.redhat.com/author/dan-walsh/
This page has lots more that is directly helpful to this question in the sense of practices even more useful than removing un-needed applications: https://access.redhat.com/blogs/766093/posts/2334141
And here is a hardened OS for containers. Full disclosure: I haven't used it though, our Systems Engineering worked with my security team to build a gold image for us on our cloud deployment. It's more of an example: https://coreos.com/os/docs/latest/
",0,2018-03-23 01:37:46Z,314
26,1,"
XSS is a form of code injection, i.e. the attacker manages to inject its own malicious code (usually JavaScript) into trusted code (HTML, CSS, JavaScript) provided by the site. It is similar to SQLi in that it is caused by dynamically constructed code, i.e. SQL statements, HTML pages etc.
But while there are established techniques to solve SQLi (i.e. use parameter binding) it is even harder with XSS. To defend against XSS every user controlled input must be properly escaped, which is much harder than it sounds:

Escaping rules depend on the context, i.e. HTML, HTML attribute, XHTML, URL, script, CSS contexts all have different escaping rules.
These contexts can be nested, i.e. something like <a href=javascript:XXX> is a JavaScript context inside a URL context inside a HTML attribute.
On top of that you have encoding rules for characters (UTF-8, UTF-7, latin1, ...) which again can be context specific.
The context and encoding need to be known when constructing the output: while most template engines support proper HTML escaping they are often not aware of the  surrounding context and thus cannot apply the context specific escaping rules.
On top of that browsers can behave slightly different regarding escaping or encoding.
Apart from that you'll often find in grown code with no proper separation of logic and presentation that HTML fragments gets stored in the database and it is not always clear to the developer if a specific value is HTML clean already or need to be escaped.

And while there are some simple ways to protect against XSS by design (especially Content-Security-Policy) these only work with the newest browsers, have to explicitly enabled by the developer and are often only effective after large (and expensive) changes to the code (like removing inline script). 
But still, there is a good chance to do it right if you have developers with the proper knowledge and are able to start from scratch using modern toolkits which already take care of XSS. Unfortunately in many cases there is legacy code which just needs to be kept working on. And development is usually done by developers which are not aware of XSS at all or don't know all the pitfalls. 
And as long as web development is done in environments very sensitive to cost and time the chances are low that this will change. The aim in this environment is to make the code working at all in a short time and with small costs. Companies usually compete by features and time to market and not by who as the most secure product. And XSS protection currently just means additional costs with no obvious benefit for many managers.
",32,2016-07-07 11:54:36Z,66
26,2,"
It seems easy, but is hard
First of all, the attack surface is huge. You need to deal with XSS if you want to display user input in HTML anywhere. This is something almost all sites do, unless they are built purely in static HTML.
Combine this with that fact that while XSS might seem easy to deal with, it is not. The OWASP XSS prevention cheat sheet is 4 000 words long. You need quite a large sheet to fit all that text in. 
The complexity arise because XSS works differently in different contexts. You need to do one thing if you are inserting untrusted data in JavaScript, one thing if you insert into attribute values, another thing if you insert between tags, etc. OWASP lists six different contexts that all need different rules, plus a number of contexts where you should never insert data.
Meanwhile developers are always searching for panacheas and silver bullets. They want to deletage all the hard work to a single sanitize() function that they can always call on untrusted data, call it a day, and go on with the real programming. But while many such functions have been coded, none of them work.
Let me repeat: XSS might seem easy to deal with, but it is not. The main danger lies in the first part of that sentence, not the second. The percieved simplicity encourage developers to make their own home brewed solutions - their own personalized sanitize() functions, filled with generalisations, incomplete black lists, and faulty assumptions. You filtered out javascript: in URL's, but did you think about vbscript:? Or javaSCripT   :? You encoded quotes, but did you remember to actualy enclose the attribute value in quotes? And what about character encodings?
The situation is quite similar to SQLi. Everybody kept looking for the one sanitation function to rule them all, be it named addslashes, mysql_escape_string or mysql_real_escape_string. It's cargo cult security, where people think it is enough to ritually call the right function to appease the Gods, instead of actually embracing the complexity of the problem. 
So the danger is not that developers are blissfully unaware of XSS. It is that they think that they got the situation under control, because hey, I programmed a function for it.
But why is it hard?
The web suffers from the problem that structure (HTML), presentation (CSS) and dynamics (JavaScript) are all mixed into long strings of text we put in .html files. That means that are many boundaries you can cross to move from one context to another.
Again, this is the same as with SQLi. There you mix instructions and parameter values in one string. To stop SQLi, we figured out that we have to separate the two completely and never mix them - i.e. use parametrised queries. 
The solution for XSS is similar:

Separate the scripts from the HTML.
Turn off inline scripts in the HTML.

We created an easy way to do the second part - Content Security Policy. All you need to do is to set a HTTP header. But the first part is hard. For an old web app a complete rewrite might be easier than to surgically disentangle the scripting from the HTML. So then it is easier to just rely on that magic sanitize() and hope for the best.
But the good news are that newer apps often are developed after stricter principles the use of CSP possible, and almost all browsers support it now. I'm not saying we are going to get the percentage of vulnerable sites down to 0%, but I am hopeful that it will fall a lot from 60% in the coming years.

TL;DR: This turned into a rather long rant. Not sure if there is much useful information in here - just read Steffen Ullrichs great answear if you want a clear analysis.
",22,2016-07-07 13:07:29Z,9
26,3,"
My set of opinion on security and XSS:

Rule of programming: You can't know everything. Sooner or later you are going to make a mistake.
Rule of programmer: A programmer works 12h a day: 3 is discussing with other programmers random things, 3 is thinking at other things, 3 is discussing on what it should code, 3 it's programming .... projects are made for 12h a day of almost non stop programing.
XSS is simple. And there are a lot of inputs waiting for XSS.
Security still comes last and most of the time is seen as a deficient.

",7,2016-07-07 11:31:39Z,322
26,4,"
As mentioned in the answer to a similar post of yours (SQL injection is 17 years old. Why is it still around?):

There is no general fix for SQLi because there is no fix for human stupidity

Developers sometimes get lazy or careless and that causes them to not check the application they are developing. 
Another popular reason is that the developers aren't aware that there is a security issue. They never went through any security training and so they don't know what is a potential security threat and whats not. And for company's that don't understand that a penetration test is important this lack of security knowledge is a potential threat by itself. 
Simple security issues such as SQLI and XSS will only be fixed when a company decided to spend time on code review and testing. Until then you have 65% of all websites globally suffering from the XSS vulnerability.
",6,2016-07-07 11:31:46Z,321
26,1,"
Consider this a very, very, very short description. The answer to your question can encompass an entire set of books.
Security is about risk management. Making something secure is about lowering risk. The lowering of risk goes together with putting controls into place, each control may have a certain price and can mitigate a certain risk, by lowering the likelihood a vulnerability can have a negative impact on your assets. Assets in this context can be anything a company considers valuable, this includes systems, applications, but also people, infrastructure etc...
A and B are both valid, except A would probably depend on cost. Security costs money and it doesn't make sense to spend loads of money to secure an asset if the security costs more than the asset is worth. If the value of an asset increases over time it may make sense to ""make it more secure"" over time by spending more on extra controls.
",1,2016-03-20 12:39:51Z,116
26,2,"
Risk management is the key to answering this deceptively simple question. To make data (or a device) perfectly secure, one would have to erase all knowledge of it. Destroying the data/device to make it secure makes it unavailable, violating the CIA triad cited by @SilverlightFox. By using the Enigma encryption device, the Nazi made it vulnerable. 
As long as the future (and life) is uncertain, ""as secure as possible"" is a never ending and fruitless task. So making something more secure is more to the issue of securing something. However, that raises the question: how much more secure does the data (device) need to be? How much more effort and ingenuity is appropriate to ""secure the asset?"" 
This reminds me of the following joke:

Two men are walking through a forest.  Suddenly, they see a bear
  running towards them.  They turn and start running away.  But then one
  of them stops, takes some running shoes from his bag, and starts
  putting the on.
          “What are you doing?” says the other man.  “Do you think you will run fast than the bear with those?”
          “I don’t have to run faster than the bear,” he says.  “I just have to run faster than you.”

So ""how much security is enough"" depends on the context. That's why most homes have key locks and banks go to the trouble of installing vaults. The impact of a breach on the former - though quite personal to the homeowner - is likely less costly in total that the impact to the bank. Thanks to the measures banks take to protect the contents of their vaults (steel, alarms, guards, etc), homeowners pay banks to hold their valuables in a lock box rather than replicate the bank's security measures for their homes. The exception that proves the point is the visibly wealthy homeowner who declines to put all her valuables into the bank vault. What is the point of valuable paintings, sculpture, and jewelry if you can't enjoy it?
So how does one decide how much effort or money spent on security is enough? One tool is Annualized loss expectancy. In short, ALE is the cost (tangible and intangible) of a loss times the probability that incident will occur in a year. If the bank has $10,000 in the vault and there is a 10 percent chance of getting successfully robbed, the bank should no more (or less) than $1,000 to protect the vault. Bruce Schneier points out that the calculation get complicated and messy very quickly. 
Another approach is return on security investment (ROSI). Mirroring the business principle of return on investment, The cost of a bad thing happening is called risk exposure. A security solution mitigates that risk by some percentage. Multiplying the exposure by the percentage mitigated gives the expected return. Thus ROSI is used to identify the appropriate expenditure on securing an asset. However ROSI is not without its detractors, notable among them Bruce Schneier. 
Andrew Jauquith's Security Metrics is another worthy read on the subject. Jaquith writes on p 33 ""...practitioners of ALE suffer from a near-complete inability to reliably estimate probabilities [of occurrence] or losses."" You can read his clear and cogent alternatives to ALE and ROI. 
",1,2016-03-20 16:45:58Z,326
26,3,"
Both are applicable.
Securing something means reducing the risks associated with running such a network, system or application.
In information security the risks are often in the CIA triad - Confidentiality, Integrity and Availability. For example, an application may have a security vulnerability that affects the confidentiality of data. Securing the application against this risk would involve remediating the security vulnerability or removing the threat. However, it is not usually possible to achieve the latter. It may not be practical to make this vulnerability ""as secure as possible"", however the risk can be mitigated to a level that is acceptable by the business, so often the action that is taken is to make it ""more secure"".
",0,2016-03-20 12:43:27Z,327
26,4,"
Security usually refers to secure sharing, meaning techniques of sharing certain information only with the people you want to share it with.  If you do not have anything to share, then it is easy to be secure, just do not give anyone any information about yourself.  However, that approach is quite impractical, as living and doing business require sharing certain information: financial information with business partners, medical information with doctors, identification information with business partners and government, personal information with friends and family, etc.
Usually when people talk about a 'lack of security' they are referring to a demonstrated vulnerability and usually a sever vulnerability.  In some sense, hackers, cons, fraudsters, and thieves are always testing your security, both electronic and physical.  Most of the time you, your computers, and your life are okay, you loose no information to the wrong person, and we call this 'secure'.  At other times, there is a problem, such as a break and entry in your home, or perhaps a data breach on your computer.  In this case we call this a security problem.
The process of recovering a secure situation after a breach involves risk analysis and vulnerability repair.  If you are interested in legal action, then forensics comes into play.  Fixing the security holes is fundamentally different than getting justice which involves law enforcement.  Most computer security professionals focus on fixing security holes first, and forensics second.  Although, during the vulnerability analysis phase, there is a certain amount of forensics used to discover the vulnerability, but usually it does not pass the standard used in a court of law.
",0,2016-03-20 15:07:32Z,328
26,5,"
In terms of cryptography, secure means not broken. And broken means, there is a method to decrypt better than brute force.
And as you know, brute force is the method of desperation where you need to try all possible keys. 
",0,2016-03-20 16:17:23Z,329
27,1,"
If you are attempting to be compliant and/or avoid the breach notification rule under HIPAA in the United States, then obfuscating data relationships within your database will not help you at all.
Under HIPAA, if data is encrypted you need not notify patients/insurance providers/etc about the breach.  However, encrypted means that it was encrypted using a FIPS 140-2 compliant cipher and only this encrypted data was obtained by the third party.
On a database, you can make use of tools such as transparent database encryption, or third-party encryption suites.  This would mean if someone got a copy of the database, it would be FIPS 140-2 compliantly encrypted, and thus not triggering breach notifications or other consequences under HIPAA. 
However, simply having a hard to understand database is covered nowhere under HIPAA, and security-by-obscurity doesn't win you any points on a CMS audit...even if you explain obscurity is a ""compensating control"" for a HIPAA requirement.
Also while security typically means keeping unauthorized individuals out, the next worst thing to be able to happen to an organization keeping PHI data is losing that data.  Having a DB with complex relationships makes it easy to lose that data, and integrity of your data should be considered a security concern when dealing with HIPAA.
Note that transparent database encryption or other on-line encryption methods will do little against the online compromise of the database, e.g. through SQL injection.  If someone gets a DB dump by running SELECT * statements, this data wouldn't be encrypted.  For this, youl'l need to secure/segment your database users and also look into best practices for securing the frontend web application. 
In short, encrypt the database somehow; but DB encryption should only be a small part of a larger HIPAA compliance strategy.  Security by obscurity, however, should not be any part of your security strategy. 
",4,2015-11-03 18:39:01Z,339
27,2,"
You asked for best, let me start by qualifying my answer as best in a sense that it is my best effort to allow you to make your own choice perhaps with more confidence. Since 'best' can be quite subjective, and I am not the all powerful knower of things, I'm quite sure another best answer can be found.
I also want to quote AviD's Rule of Usability:

Security at the expense of usability comes at the expense of security.

(While there is no direct connection to this, I do want to point to it if only to make mention that if you break usability, then all is for naught)
My first thought is ""Oh no, your entire DB is stolen!"". I would hope that your first priority is to prevent this (I don't know if you're actually worried about this or if it's just planning for a worst case scenario). Then make every necessary effort to hide all required data. Any additional data that you can hide without a noticeable performance hit is gravy.
I've done a very small amount of HIPAA in my time and have found some rules to be perhaps intentionally vague. I assume this is to allow new techniques to be used without having to change the rules or something to that effect.
In short, if obfuscating ids is required in your situation, then you must do that (requirements are requirements), if not, then you can do that if you feel the difficulty and performance hit is tolerable. Follow the requirements first. Go above and beyond where it makes sense.
In your case, it seems as though it doesn't make sense to go to all that extra trouble only to make future maintenance that much more difficult. It might be a better use of time to focus on securing the DB itself while maintaining encryption/obfuscation only on the required fields. You can obfuscate ids all you want, but if the DB is easily stolen, then what is the point?
PS: I'm not a lawyer, nor do I play one on TV. Make sure you check HIPAA requirements yourself if you are in fact dealing with HIPAA.
",3,2015-11-03 15:29:47Z,340
27,3,"
This is ultimately a terrible idea, and will cost you in terms of data integrity.  Data relationships that are obscured are data relationships that will become corrupted, and unreliable.  It doesn't even make you more secure.  Obscurity protects only from the mildest of curiosity.  Look at the long history of cracked obscured systems for evidence of this.
This approach will lead to lost appointments, possibly appointments made with the wrong doctor, and a generally unreliable system.
In general, these sorts of requirements (HIPPA, PCI, etc) are written by non-technologists, and are up for broad interpretation.  
Ask the person driving the requirement if he/she is willing to sacrifice the integrity and reliability of the system for security through obscurity.  That's essentially what's being asked for.  My guess is someone is interpreting HIPPA is a very narrow way if they're asking for crazy things like ""obscuring database table IDs"".
My general advice is find someone with expertise in interpretting HIPPA requirements of what you can actually do rather than someone trying to interpret it in the most restrictive sense possible. 
",3,2015-11-03 15:45:34Z,26
27,4,"
Just started looking into HIPPA compliance for an upcoming application I'm building, and from what I see you're taking the rules/regulations a little overboard. As far as obfuscating your data goes, one of the main requirements is protecting your Electronic Protected Health Information or (ePHI) This includes stuff like Names, SSI numbers, or private or patient sensitive data. One of your internal Id's in your database wouldn't fall under this category, and trying to obfuscate that would just create a whole world of problems for you while not accomplishing anything. For example, if some data breach were to happen and they see that Id 5 is tied to id 8, it would not mean much to anyone without some other personal data, which would be encrypted.
Again I may not be the best source since I am just learning about HIPPA compliance also, but I know a good amount about PCI compliance and they seem pretty similar. 
",2,2017-01-17 21:43:14Z,341
27,5,"
I agree with the others: this is bad practice, from every POV, including design, audit, recovery, security, performance.  The list goes on.
If a hacker gets into the database, the game is over.  It doesn't matter if you obfuscated data, it will be a matter of time before that can be de-obfuscated.
RDBMS these days allow column-based permissions, and so, using real keys to join to other tables will allow standard SQL queries to be used, without the overhead to de-obfuscate.  Moreover, your auditors will appreciate the attention to ability to scale, upgrade, troubleshoot, and still maintain integrity and security.
Having said that, some vendors, like Microsoft and Oracle, maybe others, have stepped up and allowed very focused encryption.  Keys are stored in a separate system, and access to columns are protected.  Microsoft SQL Server, for example, uses TDE (transparent data encryption) and extensible key management (EKM) to do just this.  Therefore, your table structures don't change, and you don't compromise the overhead of joins with obfuscation, et al.  So, while you might have a doctor and a patient, you won't necessarily know the names or other details of either, since TDE can allow encryption of these fields.
RDBMS systems also allow for encryption of the entire database, to handle instances where the database itself is stolen.
In short, don't obfuscate the keys to the data.  Rather, encrypt the data that is sensitive.  You can look at like this: Don't obfuscate the metadata; rather, only encrypt the data.  Think of the keys and foreign keys as metadata.
Be careful, too, not to fall victim to the practice of placing strong security on the front door, all the while leaving the back door unlocked: your transaction logs, error logs, application, application logs, backup systems, and people who are trusted to have access to this data and applications are all sources for leaks, so, shore up permissions, and be careful what you log, and what you do with those logs.
Once you apply these principles, it becomes less important to obfuscate keys.
",0,2017-01-18 15:30:59Z,342
28,1,"

Is there such a thing? 

Absolutely.  Feeding malicious input to a parser is one of the most common ways of creating an exploit (and, for a JPEG, ""decompression"" is ""parsing"").

Is this description based on some real exploit?

It might be based on the Microsoft Windows GDI+ buffer overflow vulnerability:

There is a buffer overflow vulnerability in the way the JPEG parsing
  component of GDI+ (Gdiplus.dll) handles malformed JPEG images. By
  introducing a specially crafted JPEG file to the vulnerable component,
  a remote attacker could trigger a buffer overflow condition.
...
A remote, unauthenticated attacker could potentially execute arbitrary
  code on a vulnerable system by introducing a specially crafted JPEG
  file. This malicious JPEG image may be introduced to the system via a
  malicious web page, HTML email, or an email attachment.

.

This was published in December 2006.

The GDI+ JPEG parsing vulnerability was published in September 2004.

Is it sensible to say ""the operating system"" was decompressing the image to render it?

Sure; in this case, it was a system library that required an OS vendor patch to correct it.  Often such libraries are used by multiple software packages, making them part of the operating system rather than application-specific.
In actuality, ""the email application invoked a system library to parse a JPEG,"" but ""the operating system"" is close enough for a novel.
",177,2015-08-26 19:10:55Z,35
28,2,"
Agreeing with others to say yes this is totally possible, but also to add an interesting anecdote:
Joshua Drake (@jduck), discovered a bug based on a very similar concept (images being interpreted by the OS) which ended up being named ""Stagefright"", and affected a ridiculous number of Android devices.
He also discovered a similar image based bug in libpng that would cause certain devices to crash. He tweeted an example of the exploit basically saying ""Hey, check out this cool malicious PNG I made, it'll probably crash your device"", without realising that twitter had added automatic rendering of inline images. Needless to say a lot of his followers started having their machines crash the instant the browser tried to load the image thumbnail in their feed.
",47,2015-08-27 03:18:34Z,347
28,3,"
Unrealistic? There was recent critical bug in font definition parsing: https://technet.microsoft.com/en-us/library/security/ms15-078.aspx and libjpeg changenotes are full of security advisories. Parsing files[1] is hard: overflows, underflows, out of bounds access. Recently there were many fuzzing tools developed for semi-automatic detection of input that can cause crash.
[1] or network packets, XML or even SQL queries.
",10,2015-08-27 15:48:14Z,351
28,4,"
As others have pointed out, such attacks usually exploit buffer overflows.
Regarding the nuts-and-bolts of how, it's called a stack-smashing attack. It involves corrupting the call stack, and overwriting an address to legitimate code to be executed with an address to attacker-supplied code, which gets executed instead.
You can find details at insecure.org/stf/smashstack.html.
",1,2015-08-30 19:42:25Z,352
28,1,"
You can't.
To securely send information over an unsecure channel, you need encryption. 
Symmetric encryption is out, because you would first need to transport the key, which you can't do securely over an unsecure channel[*].
That leaves you with public key cryptography. You could of course roll your own, but you don't want to be a Dave, so that's out, which leaves you with HTTPS.
[*] You can of course try to use a secure channel to exchange the key, for example physical exchange of a key. Then you can use secret key crypto, like Kerberos.
",109,2018-11-09 14:08:48Z,332
28,2,"
TLS is really the only way to do it.
But what if I encrypt it with JavaScript?
The attacker can change the JavaScript you send to the client, or simply inject their own JavaScript that logs all information entered.
Then I'll use CSP and SRI to prevent the addition of scripts and the modification of my own scripts.
Glad you're using modern tools to help protect your site, but SRI in a non-secure context really only protects against the compromise of an external resource. An attacker can simply modify the CSP headers and SRI tags.
There's really no way to do this without using encryption from the beginning, and the only standard way to do that is to use TLS.
",45,2018-11-09 14:09:52Z,220
28,3,"
While I agree that you should use HTTPS, you can make it even more secure by using the Salted Challenge Response Authentication Mechanism (SCRAM, see RFC 5802) for the actual authentication exchange.
It's not trivial to implement, but the gist of it is that, rather than send the password to the server, you send proof to the server that you know the password. Since deriving the proof uses a nonce from the client and server, it's not something that can be reused by an attacker (like sending the hash itself would be).
This has a few added benefits to using HTTPS with the basic authentication you describe:

The server never actually sees your password as you log in. If someone has managed to insert malicious code onto the server, they still won't know what your password is.
If there is a bug in the HTTPS implementation (think Heartbleed), attackers still won't be able to acquire your password. Because, even once TLS is bypassed, the password isn't available.
If, for some reason, you can't use HTTPS, this is better than sending the password in the clear. An attacker will not be able to guess your password based on the exchange. That said, you should still use HTTPS, because defense in depth is better.

Please note that this is only secure if the client is able to perform the SCRAM computations without downloading code from the server. You could provide a fat client, or users may be able to write their own code that they control. Downloading the SCRAM code over HTTP would give an attacker an opportunity to modify the SCRAM code to send the password in cleartext, or otherwise expose it.
",26,2018-11-09 18:01:00Z,358
28,4,"
You should definitely deploy TLS in this case.
If you decide to try to invent a transport security system over HTTP, ultimately you're going to end up implementing a susbset of the TLS functionality anyway. There are many pitfalls to be aware of when designing and implementing such a system, which are further amplified when you choose to try to implement the system with a language that doesn't offer many safety features.
Even if you did implement a correct implementation of a cryptographic protocol in JavaScript (which is already a big challenge) you can't rely upon that implementation being resistant to timing attacks, and the whole thing breaks if someone has scripts disabled (e.g. with NoScript).
As a general rule you should choose the implementation that consists of the most simple, well-understood, trusted components that require you to write the least amount of security-critical code. To quote Andrew Tannenbaum:

A substantial number of the problems (in application security) are caused by buggy software, which occurs because vendors keep adding more and more features to their programs, which inevitably means more code and thus more bugs.

",7,2018-11-09 18:31:24Z,21
28,5,"

Do I need to implement some encryption algorithm like RSA public key encryption?

If you're considering trying that, you may as well just go the full mile and use HTTPS. Since you don't seem to be an expert, ""rolling your own"" encryption will undoubtedly have misimplementations that render it insecure. 
",6,2018-11-09 16:03:34Z,361
28,6,"
There is actually a way to authenticate a user over an insecure connection: Secure Remote Password (SRP) protocol. SRP is specifically designed to allow a client to authenticate itself to a server without a Man-in-the-Middle attacker being able to capture the client's password, or even replay the authentication to later impersonate the client, whether the authentication succeeded or not. Furthermore, successful authentication with SRP creates a shared secret key that both the client and the server know, but a MitM does not. This secret key could be used for symmetric encryption and/or HMACs.
However, there are a number of limitations of SRP:

The user must have registered in some secure fashion, as the registration process does require transmitting password-equivalent material (though the server does not need to store it).
Although it is safe to attempt an SRP login with an untrusted server (that is, it won't expose your password to that server, and you'll be able to tell that the server didn't have your password in its database), It's not safe to load a login web page from an untrusted server (it could send down a page that captures your every keystroke and sends it off somewhere).
Although successful authentication via SRP generates a secure shared secret key, the code to actually use this key in a web app would need to be loaded from a server, and an attacker could tamper with that code to steal the symmetric key and make changes to the requests and responses.

In other words, while SRP can be useful in situations where you have a trusted client that doesn't need to download its code over the insecure connection and also you have some other secure way to register the user(s), SRP is not suitable for a web application. Web apps always download their code from the server, so if the connection to the server isn't secure, a MitM (or other network attacker, for example somebody spoofing DNS) can inject malicious code into the web app and there's nothing that you, the victim, can do about it. Once that code is there, it can capture any password or other data you enter, steal any key that is generated, and tamper with any requests you send or responses you receive without you even knowing.
",2,2018-11-10 10:42:20Z,362
28,7,"
It is actually quite possible.
Password has to be hashed on the client side, but not with a static function. The following method uses two steps, and is inspired by the Digest access authentication:
NOTE: The server keeps a HMAC of the Password and the corresponding Key (Digest);

The client starts by requesting a nonce to the server, the server returns the Key and the Nonce;
Client calculates: HMAC( HMAC( Password, Key ), Nonce) and sends it to the server;
Server calculates: HMAC( Digest, Nonce ) and compares it to the received value.

This protects you against replay.
The main interest I see is not a protection against eavesdropping, but to be completely sure that the plaintext password will not be store on the server, not even in a system log (see Twitter or GitHub).
Note: HMAC can be replaced by PBKDF2.
",2,2018-11-10 09:38:45Z,364
28,8,"
It is certainly possible to send a password securely using HTTP. There is even a standard for it. It's called HTTP-Digest and it's defined in RFC 7616. It's been a part of the HTTP spec for quite a while. It was originally designed to use only the MD5 message-digest (""hashing"") algorithm, but later revisions of the standard allow the client and server to negotiate which algorithm to use.
Unfortunately, there are many problems with HTTP-digest and the most glaring of them is that the server needs to have the password in cleartext. So while it's possible to communicate passwords securely using HTTP, it's not possible to securely-store passwords on the server while using it. So basically, it's not useful.
As others have said, it would be better to implement TLS because it solves multiple problems all at the same time, and it's fairly forward-compatible.
",1,2018-11-10 22:51:55Z,365
28,9,"
As others have said TLS! TLS! TLS! (and with decent key length too)
But if you have to to use HTTP and any server to implement, is this for programmatic use or human use?
If for human use, are you looking at basic authentication or form-based? And are your uses able to apply a little logic E.g. not for joe public to use.
Does anything else need to be protected, or just the password itself?
If so you may be able to look at:

Use a one-time password pad/generator (RSA fobs ain't cheap, and if you've got that sort of money to spend then you can afford TLS)
Some other 2-factor auth like SMS (not without it's own issues).
A simple challenge/response mechanism to obfuscate the password, for example the  page displays two numbers on it in the text (not blatantly labelled either), user has to start password with a simple calculation such as difference between the two numbers added to each digit of a PIN, maybe with random stuff before and/or and after, eg 98634572dkkgdld. Changing the challenge numbers should stop replay attacks that aren't tried instantly, but if an attacker can capture enough attempts they may be able to work it out. You can't put any obfuscation logic in user-facing script either 'cos that'll be seen and if the scheme is too complex your users will hate you even more.

I'd still so with TLS though, certificates are cheap (even free) these days so convoluted obfuscation schemes really have no reason to exist any more.
",1,2018-11-12 21:55:03Z,366
28,10,"
An alternative would be to set up a secure VPN to the destination server, and perform a HTTP login from there. A malicious man in the middle will therefore have to break the encryption on the VPN channel in order to attack the connection and retrieve the passwords. 
However, in terms of end user usability, it's still not as good as HTTPS, since it requires them to setup and use a different channel with a different set of credentials. 
",-3,2018-11-09 23:17:20Z,368
28,11,"
You need the recipient to send you his key. You can't randomly send someone encrypted info without them first providing you with their personal public key.
",-4,2018-11-11 01:58:52Z,369
29,1,"
Welcome to the internet! This is the normal situation, business as usual.
You don't have to do anything, but to harden your website. Probes like that occurs all the time, on every site, day and night. Some people call that ""voluntary pen testing.""
Depending on your site, there are some tools that you can use to help you keep those kinds of probes out of the site. Wordpress sites have a couple plugins (you can search for Security plugins on the plugins directory), and I believe the other popular platforms out there will have equivalent plugins.
Other tool I usually employ is fail2ban. It can parse your webserver log files, and react accordingly.
",67,2018-11-05 14:08:39Z,219
29,2,"
The first step outside of immediately looking to a solution is to conduct a pentest of your own site and be actually aware of what weaknesses there are in your site. If you don't know what you are protecting, then how will you know to protect it?
First, look at the infrastructure such as CMS. For example, if you are using Wordpress, then there are pentesting tools for Wordpress available both as apps and cmd tools. ie Wordfence , and I've used WPscan also.
Second option is to look at tools like OWASP zaproxy and do an attack scan of your network and gain a list of vulnerabilities. Just a note that some of these could be false positives.
Your findings may mirror what has already been found but I think knowing what the vulnerabilities are in your own site is useful.
The next step is how you are finding out about these probes. If it was a manual check, you can also consider setting up some log collection system like NXLog 
",5,2018-11-05 15:18:27Z,374
29,3,"
Work out what they are looking for, and ban their IP for a month or two if they try it on.  You might also dummy up some PHP to slow them down. 
Do not refer them to other sites for huge downloads, and do not leave malware for them to find. 

90% will be Wordpress, PHPMyAdmin, Telephony. If they are script kiddies the same old values pop up.

Look into Fail2Ban and DenyHosts for ideas.
If you are actually running WP, harden it up with a security solution.
Only allow access to admin tools and any database by exception, and this should almost never be from an Internet address, but something local with it's own Bastion-like protection.
",2,2018-11-06 02:09:45Z,12
29,4,"
If I had a cent for every scan my website gets...
Literally, if you check your logs, you will notice a constant stream of automated probes and attacks. When I consult clients (I work in information security), I call this ""background noise"". It is there and any attempt to do anything about it is more costly than just accepting that it's there. I would even go so far as to filter it out before you pipe the logfiles into your monitoring, alerting, SIEM, etc. systems.
What you must do is keep your systems up-to-date and patched. Almost all of these attacks are using well-known and often quite old exploits. They are fishing for easy targets.
What you should do is spend a little bit of time on hardening your system. Setting up permissions correctly, blocking unused ports, disabling unused software, running stuff under dedicated users, that kind of stuff.
What you can do, especially for a private website with a local audience, is to block out broad IP ranges belonging to China, Russia, Europe and/or the USA, depending on where your audience isn't. The vast majority of attacks originate from these origins, and if you don't have anyone in, say, the USA who reads your webpage because your webpage is about your local dog club in Spain, you can reduce the noise just by blocking them out at the firewall. I write ""can"" because it doesn't make much of a difference, really, but it will reduce the noise in your log (it will also affect your Google ranking, but that's a different subject).
",0,2018-11-06 13:06:14Z,25
29,5,"

Block the whole country
Check ASN and it’s allocated IP range, and block that IP range.
Fingerprint the attacker using a user agent or a JavaScript library and attach a strong captcha when the fingerprint is detected.

Last but not least, secure your site and monitor attacks regularly.
",-5,2018-11-05 14:07:36Z,0
30,1,"
The server time is of little use to an attacker, generally, as long as it is accurate. In fact, what is being revealed is not the current time (after all, barring relativistic physics, time is consistent everywhere) so much as the exact difference between your server's notion of the current time and the actual current time. Note that, even if you do not explicitly reveal the time, there are often numerous ways to get the local server time anyway. For example, TLS embeds the current time in the handshake, web pages may show the last modified dates of dynamic pages, HTTP responses themselves may include the current time and date in response headers, etc.
Knowing the exact clock skew of a server is only a problem in very specific threat models:

Clock skew can be used in attacks to deanonymize Tor hidden services.
Poorly-written applications may use server time to generate secret values.
Environmental conditions of the RTC may be revealed in contrived situations.

",84,2018-06-08 09:22:39Z,76
30,2,"
""Servlet"" sounds like you're already running a complex server there.
There's a lot of ways that protocols leak server time. For example, HTTP has a popular header field for creation/modification time, and dynamically generated data would always have the current system time, if used properly.
For TLS authentication reasons, you can even binary search to find an endpoint's date and time using expiring client certificates.
This is a necessary evil – if you're doing TLS, your server needs to have a notion of time to know what's a valid key / cert and what not. If that's the case, this will very likely be an NTP-coordinated time. So, you really can't tell anything about the server that an attacker wouldn't already know – there's really only one ""correct"" time.
If you're not doing TLS: leaking system time is probably not the first thing you should fix.
Regarding security: Not quite sure you should expose yet another servlet just for devices to figure out world time. 

To explain more, this end-point will be used by mobile apps to enhance their security (to avoid cheating by adjusting the device date).

Red flag. You're depending on your client's unmodifiedness for security. Never do that. You simply can't trust anything happening on your user's devices. These are not your devices. They can simply hook up a debugger and modify notions of time in-memory of your process, no matter whether kept by the phone's OS or by your application. 
",10,2018-06-08 09:24:02Z,385
30,3,"
The only thing that could really expose a security risk is the high-precision timers and performance counters. These could be used to precisely time how long some cryptographic process takes on the server, and from there infer secret data.
Timing data to the milisecond or normal ""tick"" rate is unlikely to expose this.
",2,2018-06-08 15:30:15Z,386
30,4,"
Knowing the server time is of little use for an attacker. So here you should find about possible side effects.

is the protection for that servlet weaker (in any way) that the one of other servlets? What are the possible implications of the no need for authentication in term of DOS/DDOS attacks. It could matters if authentication was processed in other more secured servers. Is there any difference for reverse proxying?
what are the risks for security flaws in that servlet? Has it followed the same quality insurance controls than the other servlets? What about its maintenance cycle?
what about its servlet container?

",1,2018-06-08 12:44:48Z,387
30,5,"
I would like to answer that by pointing out, what would follow from the assumption, that exposing the server time would in fact be a risk.
It would mean that system administrators would have to set random times on their systems, because any server set to the correct time would be vulnerable, since a correct or near correct server time is just to easy to guess.
It would also mean extreme development effort to translate a false setting to a correct time for use in every time related application. Every single file on the system would  have a wrong time stamp.
In my experience you invite much more trouble by setting wrong server times, than you can expect with the correct setting. Some examples are mentioned in other answers, but you also have to consider distributed applications or clusters, where correct time settings are usually essential. Basically any time related information you store would be flawed.
So if exposing your system time would be your biggest risk, you were in luck. But most likely there are many other concerns that you haven't paid enough attention to. See https://www.owasp.org/index.php/Main_Page for security related issues and best practices.
",1,2018-06-08 22:44:07Z,388
30,6,"
Server time is not a secret. On the contrary, you are strongly encouraged to sync your time to an outside objective time source (such as an atomic clock exposed through a time server).
Not being a secret has two consequences:

you don't need to protect or hide it. You can safely assume that the attacker is capable of figuring out what the current time is.
it should not have a function on anything that you do wish to protect or hide. For example, you should not derive any keys or secrets from the time. Any random functions you are using should be seeded not with the current time (still a lazy default in many), but with a better seed source.

",0,2018-07-06 10:06:46Z,25
31,1,"

Is there any standard defense technique against it?

As outlined in the other answers, bit errors when querying domain names may not be a realistic threat to your web application. But assuming they are, then Subresource Integrity (SRI) helps. 
With SRI you're specifying a hash of the resource you're loading in an integrity attribute, like so:
<script src=""http://www.example.org/script.js""
    integrity=""sha256-DEC+zvj7g7TQNHduXs2G7b0IyOcJCTTBhRRzjoGi4Y4=""
    crossorigin=""anonymous"">
</script>

From now on, it doesn't matter whether the script is fetched from a different domain due to a bit error (or modified by a MITM) because your browser will refuse to execute the script if the hash of its content doesn't match the integrity value. So when a bit error, or anything else, made the URL resolve to the attacker-controlled dxample.org instead, the only script they could successfully inject would be one matching the hash (that is, the script you intended to load anyway).
The main use case for SRI is fetching scripts and stylesheets from potentially untrusted CDNs, but it works on any scenario where you want to ensure that the  requested resource is unmodified.
Note that SRI is limited to script and link for now, but support for other  tags may come later:

Note: A future revision of this specification is likely to include integrity support for all possible subresources, i.e., a, audio, embed, iframe, img, link, object, script, source, track, and video elements.

(From the specification)
(Also see this bug ticket)
",131,2018-05-08 12:32:24Z,389
31,2,"
Your concern is very likely unfounded.
First of all, you need to realize just how unlikely these memory malfunctions are. The person who wrote the above article logged requests to 32 clones of some of the most visited domains on the Internet over the course of 7 months. Those 52,317 hits had to be among hundreds of billions of requests. Unless you operate a website on the scale of Facebook, an attacker would have to be extremely lucky to even get just one unlucky victim on their bitsquatting domain.
Then you have to note that memory errors cause several malfunctions. The author writes:

These requests [...] show signs of several bit errors. 

If the system of the victim is so broken that they can't even send a HTTP request without several bit errors, then any malware they download from it will likely not execute without errors either. It's a miracle it even managed to boot up in that condition.
And regarding those cases where bit errors were found in ""web application caches, DNS resolvers, and a proxy server"" and thus affecting multiple users (some of them maybe unlucky enough to get enough of the malware in an executable state): In these situations, the HTTP response would come from a different server than the client requested. So when you use HTTPS-only (which I assume you do, or you would have far more serious attacks to worry about), then their signature won't check out and the browser will not download that resource. 
And besides, HTTPS will also make it much less likely to get a successful connection when there is a system with broken RAM on the route. A single bit-flip in a TLS encrypted message will prevent the hash from checking out, so the receiver will reject it.
tl;dr: stop worrying and set up HTTPS-only.
",65,2018-05-08 13:56:54Z,71
31,3,"
I doubt about the article's dependability. While discussed at DEFCON and published as whitepaper, I have serious concerns about the experimental results.
According to comment by @mootmoot, the author failed to determine deterministic programming errors from random fluctuations of bits.
My concerning statement is

During the logging period [Sept. 2010 / May 2011, ndr] there were a total of 52,317 bitsquat requests from 12,949 unique IP addresses

No, the author only proved his squat domains were contacted, but likely failed to provide additional information

What percentage of original CDN network does that traffic represent (this is verifiable in theory, but I don't have those figures)
Occurrence of source referral domains

The second is very important because it helps isolate deterministic programming failures from random fluctuation of bits.
Consider the following example: if you find any entry to gbcdn.net (facebook squat) with referer https://facebook.com you likely have found a bitsquat.
If on the contrary you find multiple entries from a poorly known webiste which you can inspect to find with a broken like button, then the problem is probably due to a programmer not copying/pasting the code correctly, or even a bit flip occurred in the programmer's IDE. Who knows...
",20,2018-05-08 14:01:05Z,397
31,4,"
Disclaimer: I'm an employee of Emaze Networks S.p.A.
One way to defend against this kind of attacks is to not allow attackers to register similar domain names.
To achieve this you can register many bitsquatted/typosquatted domains together with your main domain name, but this is impossible to do with all bitquatting/typosquatting cases, since they can be billions (consider also unicode etc!).
An alternative is to periodically monitor the domains registered, and if you find a suspicious domain you can check what it does and, if it seems to be a malicious site, you can report it to the registrar and ask them to pass the ownership of said domain to you.
We have a small service, called Precog that does this, by aggregating registrar information from different sources and running various kind of queries to detect bitsquatting/typosquatting/punycode-squatting domains: you can register your brand, put some keywords and we will contact you if a suspicious domain is registered.
Our tool takes into consideration 2nd level domains, obviously, but is also able to detect registration of many 3rd (or more) level domains, so we may be able to detect someone is going to use app1e.account.com to try and steal your apple credentials.

I must add: I believe the biggest use case for attacks of this kind is not to ""get lucky"" to receive a request because somebody mistyped the domain, but to use the domain as a phishing domain. So people will register the site àpple.com and send tons of emails that look like Apple's emails and try to get some people to insert their credentials/credit card information on their page.
",3,2018-05-08 18:40:25Z,277
31,5,"
Among all the answers, I am surprised I haven't seen a reference to Content Security Policy.
It is basically a one-stop solution for white-listing allowed sources of content. A simple fix would be to only allow JavaScript from your current domain (www.example.com) and block everything else.
",1,2018-05-11 15:18:49Z,401
31,6,"
Arminius's answer is great. You can also use a strong Content Security Policy as another line of defense against bit-squatting. 
CSP is an HTTP header that allows you to craft a fine-grained whitelist of URIs that your website can communicate with. For example, you can tell the browser that you'll only allow JavaScript to come from example.org/js/file1.js, only allow images from example.com/imgs, and fonts from example.net. 
For a bit-squatting attack to be successful they'd need to flip a bit in the HTTP header AND in the request. If an attacker can flip only one bit then your website will throw a lot of errors - regardless of which bit is flipped.
",0,2018-05-11 15:16:16Z,402
32,1,"
Technically slightly, yes. But:

It would be security by obscurity, which is a bad idea
It does not boost confidence in your product
It would be very easy to figure out what does what, it would only take a bit of time
Google Translate, you can just use meaningless names, it would still not help much
It would make maintenance harder
It would make audits very hard, as the auditors may not understand the language

All things considered, it is probably never worth it.
",174,2018-04-30 14:03:35Z,406
32,2,"
It would not be appreciably more secure. Reverse engineers are often forced to work with systems that do not have any original names intact (production software often strips symbol names), so they get used to dealing with names that have been generated by a computer. An example, taken from Wikipedia, of a snippet of the kind of decompiled C code that is often seen:
struct T1 *ebx;
struct T1 {
    int v0004;
    int v0008;
    int v000C;
};
ebx->v000C -= ebx->v0004 + ebx->v0008;

People who are used to working with this kind of representation are not fooled by the usage of variables and such that are given irrelevant names. This is not specific to compiled code, and the use of C was just an example. Reverse engineers in general are used to understanding code that is not intuitive. It doesn't matter if you are using JavaScript, or Java, or C. It does not even matter if they are analyzing nothing but the communication with the API itself. Reverse engineers are not going to be fooled by the use of random or irrelevant variable or function names.
",62,2018-05-01 06:05:43Z,76
32,3,"
Not really - all of the built-in functions will still be in English, so it wouldn't take much extra effort to work out what your variables are going to represent. It might slow someone down slightly, but given that people still manage to reverse-engineer code with single character variables all over the place, or which has been run through obfuscators, swapping the language used for variables and functions just means doing a find-replace once you've worked out what one of your variables is used for, then repeating until you have enough understanding.
",35,2018-04-30 14:03:09Z,409
32,4,"
That is security through obscurity and will delay a dedicated attacker all of five minutes.
If you want to confuse an attacker, naming things their opposite or something unrelated would have the same effect. So your ""create user"" function could be named ""BakeCake"". Now you can answer yourself how much security that gives you. Actually, this would be more secure, as it can't be defeated by simply using a dictionary.
Yes, at first it would confuse, but one look at the system in operation and everything becomes crystal clear immediately.
",21,2018-04-30 19:11:16Z,25
32,5,"
Your system MUST be secure by itself. If it relies on user-side javascript passing a parameter with the value ""open sesame"", you are doing it wrong.
You should develop the program in the language that is more convenient for you (eg. based on your coder proficiency, consistency with your code base, or even with the terms that you are using).
Other answers already pointed out how it doesn't really provide security. If you want to make a secure program, rather than concerning about potential hackers easily reading your code and learning a secret hole, you should probably care more about making it readable to the people auditing it.
If there is a function parameter called nonce, and a comment saying how we are ensuring it is unique across the requests, yet it isn't sent on the request, you can be quite sure it is a slip-up. Actually, having that code easily readable will decrease the chances of that parameter being dropped/empty (after all, everything worked without it...), or if that really happens, make easier that another of your developers notices the problem.
(Third parties could hint you about it, too. Probably, there will be more people having a casual look at it than ones actually trying to break it. A random attacker will most likely start by launching an automated tool and hoping it finds anything.)
TL;DR: produce readable code. For the people that should deal with it. In case of doubt, you should prefer the one most people know about.
",9,2018-05-01 21:15:38Z,14
32,6,"
It could even make things worse by making the system harder to maintain. 
Take an extreme example inspired by history, and communicate between front and back ends in Navajo. When (not if) you need to patch the code you need to either:

work with what to you is nonsense (with the extra chance of bugs/typos plus it takes longer to work on non-intuitive code), or 
hire/keep programmers fluent in Navajo (a rare and potantially expensive skill combination, possibly impossible to find a contractor)

",7,2018-05-01 14:08:36Z,413
32,7,"
I would also note that in most instances for javascript on the client side, the script as developed (with meaningful variable names etc) would be 'minified' in order to increase performance on the client (smaller file size to download). 
As part of this, most variable names are reduced to single characters and as much whitespace as possible is stripped out.  This becomes just about as unreadable as anything else you might write.
I would also note that chrome (for example) has methods that take a 'less human readable' file like this and 'pretty print' it, making it a lot easier to figure out what is going on.
In short, the human language that you use to write your client side code really doesn't make a big difference.
",5,2018-05-01 12:56:41Z,414
32,8,"
If the mass media are to be believed, then the majority of hackers are Russian, Chinese, North Korean, so they are already operating under the “handicap” of having to hack Western systems in a non-native language. Therefore unless you choose something incredibly obscure, like in the movie Windtalkers it won’t make any difference to them. But the extra effort for you means less time for you to find and fix bugs, so if anything this strategy would make your security weaker.
",4,2018-05-03 21:36:22Z,415
32,9,"
Several responses have (rightly) pointed out that this is “security through obscurity”, which isn’t actually a form fo security. It is safest to assume that the attacker has fully annotated source code sitting in front of them while busily attacking away.
Software is “secure” when knowing everything which can be known in advance of a request / transaction / session is public knowledge AND this has no impact on the actual security of the product.
Source code must be assumed to be public knowledge, if only because disgruntled employees aren’t taken out back and shot when they are terminated. Or as is often said, “The only way for two people to keep a secret is if one of them is dead.” Thus any “special knowledge” which is being concealed by obfuscation of any sort must be assumed to have become “general knowledge” as soon as it is produced.
Security, at its core, is nothing more than “saying what you do, and doing what you say.” Security analysis — code auditing and formal evaluation schemes, such as are conducted under the Common Criteria — requires that the mechanisms for performing an action are well-documented and that all transitions from one secure state to another secure state are only possible when all the requirements are satisfied. One risk with obfuscation of all sorts is that these requirements are obscured by clever coding — see the example of “create_user()” failing because it is “secretly” the “delete user” or “rename user” or something else method. For a real-world example of how hard such renaming is on the brain, there are on-line tests in which you must read the name of a color which is printed on the screen in a different color. For example, the word “RED” written in a blue font will result in some number of people saying “BLUE” instead of “RED”.
",2,2018-05-03 21:47:01Z,416
32,10,"
It might be irrelevant. Consider the scenario where you're coding in (e.g.) Italian instead of English, but most of your customers are in Italy, naturally Italian-speaking hackers have a higher motivation/payoff from attacking you.
In this scenario, coding in another language has either no effect or it can even make it easier to hack.
",0,2018-05-01 05:15:32Z,418
33,1,"
/test and /Test are both hosted on example.com … so it's just a page redirect not a domain redirect … this is a non-issue.  
People redirect like this all the time, for instance redirecting from HTTP to HTTPS is pretty much industry standard at this point.
",30,2018-04-12 06:16:51Z,420
33,2,"
Implemented correctly, there are no issues with this.
There are two things you should look out for (I assume that test is not static here, but user supplied, so you eg want to upper-case every path):

Open Redirect: If your redirect is implemented incorrectly, it might be possible for an attacker to redirect outside of your domain, which could be used in phishing attacks
CSRF: If your CSRF protection is only a simple referer check (which isn't recommended), and if you have state-changing GET requests (which also is not recommended), those may be possible to exploit, depending on your implementation of the redirect mechanism

",18,2018-04-12 06:55:28Z,332
33,3,"
Redirecting users to different page or domain is a normal practice followed by many developers (even MNC's including FB, fb.com redirects to facebook.com). It's no harm if you try to redirect requests in a secure way.
You might want to check OWASP Cheat Sheet for Unvalidated Redirects and Forwards (Also called Open Redirection). This document provides to secure ways to redirect URL in multiple programming languages.
",8,2018-04-12 06:55:10Z,421
33,4,"
As far as hostname remains same and your user trust it. It should not be a problem.
This sort of redirection are common across internet and help to provide a better user experience.

For instance:
you have a resource at https://testwebsite.com/Test but due to some typo or 
   developer's mistake it is written as https://testwebsite.com/test. The redirection will help user to see an appropriate file instead of seeing a 404 file not found error or Internal server error. 

",1,2018-04-12 09:48:47Z,422
33,5,"

Redirect within the same domain - no risk. (because your domain is trusted)
Example: https://domain.com/login redirect to https://domain.com/dashboard
Redirect to third party website is medium severity risk if you are leaking sensitive data like access_token, secret keys to the third party website.
Example: https://domain.com/redirect_to_fb redirect user to https://fb.com
Attacker controlled redirect leads to:
a. Phishing https://domain.com/login?redirect_to=http://evil.com
b. Cross Site Scripting (XSS) issue: https://domain.com/login?redirect_to=javascript:alert(document.cookie)
c. Leaking tokens Example: https://domain.com/login?redirect_to=https://attackerdomain.com Example: https://r0rshark.github.io/2015/09/15/microsoft/
d. Content Security Policy bypass
e. Referrer check bypass
f. URL whitelist bypass
g. Angular ng-include bypass

If an attacker is able to control the redirect then it's a serious issue.
",-4,2018-04-12 10:07:09Z,424
