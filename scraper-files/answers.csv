Post ID,Answer,Vote Score,Date,User ID
1,"

Is there any way to break in HTTPs connection and see all content as plain-text?

Definitely yes. There is a way to achieve this but it's not that easy as it sounds. The technique you are searching for is called SSL inspection. It is used at many Firewalls or IDSs/IPSs to intercept a HTTPS connection. First you are establishing a TLS connection with your IoT device. If it wants to connect to a Server, the request is sent to your PC and decrypts it. Afterwards you encrypt it again and redirect it to the requested server. 
For your understanding I got this Image:

The firewall in this image shows your PC.
If you now want to try this in your network you can use SSL Strip. I think this will work for you.
",1,2018-11-05 11:31:17Z,1
2,"
I will assume the devices will transmit tuples (serial, counter, key, value) where:

serial: an unique id for the device (ideally set at the factory).
counter: a keeps track of the number of messages sent to the central server
key: name of the parameter you are sending
value: the number

Make the devices encrypt that big message with the public key of the center (if you can afford also having a private key per device, add a digital signature, too. Otherwise, use a fixed part on the serial id field that was generated at random and is not known to third parties).
The plant server only ever sees encrypted messages, so he can't modify them. It could choose to miss some messages, but you would notice the gap in the counter. Or that a given device stopped sending data two weeks ago.
Note that we go from ""securing data on three places"" to just ""secure between device and center"". Of course, there's no reason the plant server couldn't additionally use https when connecting with the central server, but that wouldn't be necessary for the confidentiality of the messages themselves.
You talk about ""SHA key"", which doesn't make much sense (SHA is used for message digests…). Simply use asymmetric cryptography. Your devices can probably use elliptic curve crypto.
You still need to get the other bits right, like using an authenticated encryption mode (eg. GCM) or else performing additionally checks, an attacker that compromised the whole device, could impersonate that device (but only that). The central server should check that things make sense... but otherwise this should provide a good basis to start.
",1,2018-10-16 16:06:07Z,3
3,"
If the issue is that you don't want any device to impersonate others, the problem is simple, and already solved. Just look at every single online service that have users and don't want one user to access data belonging to other user: authentication. 
You must use TLS, this is the first point. Without TLS you are building a castle on the sand.
Second: have a sign-up routine that your devices will follow when they connect for the first time and don't have a key pair yet: generate a new one, send to the server. Any time later that the device sends anything, it need to sign the data with its key, so the server knows it's legit. 
An attacker will only be able to impersonate devices he owns, or can physically access, and there's no way around this. But they cannot impersonate any other, even with access to your database. 
",1,2018-10-14 02:05:35Z,5
3,"

More specifically, how do I protect the data that is sent to and from the remote server?

I'm not sure if I properly understood your question. But I understand the above part that you just want to protect the data in transit between client and server. In this case HTTPS (or plain TLS) is enough. This will automatically use different encryption keys for different TLS sessions (where TLS session will have the same lifetime as the TCP connection unless session resume is used)  so the risk you imagine of having the same encryption key on all devices does not exist.
But HTTPS or TLS will not protect against somebody who has physical access to the device and intercepts the data there before encryption and after decryption. It only protects the data in transit. 
",1,2018-10-13 19:59:56Z,6
4,"
IDS is not about either penetration testing or malware analysis. An IDS inspects traffic (typically) to look for Indicators of Compromise (IoC). 
An IDS, then, is a consumer of whatever the IoCs are, and developing the IoCs is a function of malware and malicious traffic analysis. 
So, to develop IoCs, you might want or need to know quite a lot of about both penetration testing (can help with malicious traffic analysis) and malware analysis. 
What it sounds like is that you want to develop IoCs for IOT, and then deploy those IoCs in some kind of analysis platform (what you are calling an IDS). 
If you want to inspect traffic for previously unknown IoCs and determine IoCs on the fly without any previously defined IoCs, then that's a completely different prospect. I have done this, but you need quite extensive experience in a number of areas. Choosing one over another will restrict what you can detect.
",4,2018-10-07 11:01:24Z,8
5,"
If you only need this for testing and debugging, why do you need it to be secure? You should not be debugging live systems while they are in a potentially hostile environment where a secure protocol would be needed. Considering the fact that even the very lightest secure protocol would eat up a significant portion of system memory, it would be silly to allocate such a high amount of resources for just debugging and testing. What you should do instead is only test your devices when you have physical control over them, ideally over a physical network link.

32 kB of RAM is really pushing it. Even Dropbear cannot run on a system that limited. Even doing TLS will be difficult, as WolfSSL may barely fit. The WolfSSL SSH library requires a minimum of 33 kB. It's likely that you'll need to develop this yourself, which should not be too difficult if you are using FreeRTOS. There is likely nothing that you can simply drop in to a 32 kB system.
If you are using Contiki as an RTOS, you may be able to use this TLS/DTLS implementation. It requires slightly more memory than 32 kB, but you should be able to strip it down sufficiently if you are familiar with embedded programming and optimization and don't need all the features.
When you begin to use systems this constrained, each individual system will have different priorities. As a result, no one creates a one-size-fits-all library for something as complicated as a secure remote access protocol. You are generally expected to be able to implement it yourself to fit your exact needs, either from scratch or by forking an extremely lightweight library. If you are not likely able to do this, you may have to rethink either your hardware and switch to something other than FreeRTOS, or your threat model (and communicate with the system over a secure medium).
",2,2018-09-06 02:42:51Z,10
6,"
Yes, you still need a firewall on your end. In the case of a DoS/DDoS, your connection would potentially be flooded to the point where your devices would not be able to establish the IPSEC tunnel and would therefore not function [properly].  If the firewall only exists on the cloud side, that does nothing for your service at your house/office.
A DoS/DDoS attack will probably not be stoppable no matter what equipment you have since it's happening at the connection level.  Typically with a DoS/DDoS you deal with it further and further up the path as close to the source as can be.  For example, you call your ISP so they can block it from getting to you, then they call their provider to have them block it before it gets to them, and so on.  It can only be blocked so much before someone is left to take the hit but that's usually at the backbone level where they have more than enough bandwidth to deal with it until it is dealt with by other means (like finding the people causing it and stopping the attack).
",1,2018-09-01 12:19:33Z,12
6,"
Since you've mentioned it's a closed network (connected through IPSEC), instead of deploying firewall and other traditional VPN (IPSEC) networks for your IoT network, you may try to implement SDP (Software defined perimeter) solutions.
You may refer details through wiki: https://en.wikipedia.org/wiki/Software_Defined_Perimeter
Or refer working group : https://cloudsecurityalliance.org/group/software-defined-perimeter/#_overview
This could be an alternative to traditional approaches and there are commercial solutions available to try out : https://www.perimeter81.com/software-defined-perimeter-enterprise
",1,2018-09-03 02:35:16Z,13
7,"
The risk is the same as with any other exposed service. Misconfigurations or vulnerabilities may end up providing an entry point to your LAN.
However, why expose it at all? Why don't you setup a VPN, connect your mobile to the VPN and then access the device internally in the LAN? That would be a better approach than just expose this service, as usually these kind of devices use software that tend to be somewhat weak in terms of security.
",1,2018-08-06 08:52:48Z,15
8,"
It looks like you want to accept any certificate issued by Let's Encrypt for your specific domain. This is done implicitly without any pinning if your only root CA on the device is Let's Encrypt (provided of course that you do the usual proper certificate validation). If you have multiple root CA on the system you might pin at the specific public key of the root CA you want to use.
Another option would be to not care about the CA at all but instead ping against the public key of the leaf certificate. Usually this public key stays the same if one is just renewing a certificate, i.e. one will submit a CSR with the same public key as in the previous certificate. In case the private key was compromised you might also have some more spare keys you could use to create a new certificate and have these setup for pinning on the device too.
I'm not sure what the current capabilities of the ESP8266 platform are currently regarding certificate validation. But as far as I remember there was a time when all it could do was to check for a specific leaf certificate. In this case the second option of pinning against the public key of the leaf certificate and reusing the same key with certificate updates should be easier to implement.
",1,2018-07-26 10:24:57Z,6
9,"
IoT covers a broad range of devices and use cases, like at home the router, smart light bulbs, smart thermostat, maybe smart locks etc. And even more cases can be found in the industry where sensors or data collection systems are everywhere.
Given the broad range of devices they have some things in common: they have some firmware which might have bugs or missing features and thus need to get updates. And they need to communicate with other systems to receive and/or provide collected data.
And this is where certificates (or other trusted containers for a public key) help: 

It is essential in most use cases that the device only runs authorized firmware, i.e. no firmware which can be used to destroy the IoT device or even attack connected machines, sensors or other devices in the network. Digital signatures using certificates are typically used to provide such authorization. 
In order to protect the data in transit encryption is usually required. This requires that the client knows that it transmits the data to the correct server. Certificates are typically used for this server authentication, for example in protocols like TLS. Alternatively the client does not trust the server it is directly talking to at all, but instead encrypts the data so that only the ultimate recipient can decrypt these. In this case the client can use protocols like PGP, S/MIME or similar to encrypt the data for the final recipients by having the recipients certificate.
Often it is also necessary that the recipient of some data knows for sure which device is sending/creating the data to make sure that it does not get fake data from unauthorized devices.  Again certificates are used for this: either in the form of client certificates within mutual authentication in protocols like TLS. Or by digitally signing the data using a device specific certificate.


Or also for encrypting communication?

Certificates use public key (asymmetric) cryptography, i.e. RSA or ECC. This kind of cryptography is not suitable to encrypt large amounts of data. Instead symmetric cryptography is used for this (i.e. AES and similar). Certificates still play an important role in encryption in that they are used to protect the symmetric keys used for encryption, for example  in the key exchange in TLS or to encrypt the symmetric key with the recipients public key in protocols like PGP or S/MIME.

I am assuming that sensors etc are too small to get certificates and use them? 

Public key cryptography as the central part of certificate is not that resource intensive as one might think. It is for example already implemented in smart cards or TPM chips which means that it can probably be implemented also in most IoT devices which are capable enough to do some kind of network communication. The usual use cases for public key cryptography within IoT require only the occasional creation or verification of digital signatures or encryption of small data, which means that speed of the operation is usually not that critical.  
",2,2018-06-24 08:27:45Z,6
9,"
I'd say that certificates are an important part of the security of an IoT devices, not the most important. 
Think of an IoT device use case? Is it a connected Light Bulb? Then is the web site that the App connects to control the bulb more important to protect than the bulb itself (which uses Zigbee)? Or is it an open platform that has a web app running on it? then make sure the OWASP Top Ten is being used to harden the app. 
IoT Security falls in to a multitude of areas Device, Communication and Cloud. Putting one above the other is not a good strategy. 
",0,2018-06-21 21:41:55Z,18
9,"
The question is certainly very broad; it's a bit like asking ""what devices can use an internet connection?""
We should first establish why we need certificates.
Secure Communication
With a valid certificate, a device can verify the identity of a remote host. Combined with encryption of communications, you can be reasonably sure that nobody is tampering with or reading your messages.
Signature Validation
Firmware updates are usually signed by the manufacturer. Devices only accept an update if they can verify its signature. They check the signature against a certificate installed by the manufacturer.
So, broadly speaking, any device that communicates or updates can make use of certificates, because public-key cryptography is fundamental to keeping these things secure.
",0,2018-06-23 22:26:15Z,19
10,"
Azure Sphere is not just another IoT operating system, but a unique combination of secure hardware (with a separate security core), secure OS (based on Linux), and a cloud security service. The proprietary hardware for example ensures that an application cannot hack the OS and the application can only access specific authorized servers.
",1,2018-07-28 00:21:54Z,21
11,"
Popular IoT botnets and attacks:

Linux.Aidra 
Bashlite 
Mirai 
Linux/IRCTelnet
Linux.Wifatch
Hajime
Brickerbot

Reference: Radware Blog
",0,2018-04-23 09:42:32Z,23
11,"
The truth is that IoT is a new trend, a new surface with a lot of possibilities, and one of the most popular IoT botnets is Mirai, here is a brief description of it:

Mirai (Japanese for ""the future"", 未来) is a malware that turns
  networked devices running Linux into remotely controlled ""bots"" that
  can be used as part of a botnet in large-scale network attacks. It
  primarily targets online consumer devices such as IP cameras and home
  routers. The Mirai botnet was first found in August 2016 by
  MalwareMustDie, a whitehat malware research group, and has been
  used in some of the largest and most disruptive distributed denial of
  service (DDoS) attacks, including an attack on 20 September 2016 on
  computer security journalist Brian Krebs's web site, an attack on
  French web host OVH, and the October 2016 Dyn cyberattack.
  According to a leaked chat log between Anna-senpai and Robert Coelho,
  Mirai was named after the 2011 TV anime series Mirai Nikki.

If you want to check the source code, here it is. Also you can check this report from Arbor Networks about IoT DDoS Attacks for further information.
Hope it helps.
",0,2018-04-23 09:41:17Z,24
12,"
The most common vulnerability is no security at all. Many brands just:

put a completely insecure web server onto their devices. 
put plain-text hard-coded passwords into their code for ""maintenance access"".
standard issues such as buffer overflow.
the firmware update functionality is often vulnurable
signature of an update was in past verified to be a valid signature, but the certificate itself was not checked for validity. So any self signed certificate provided access.

For best practices, try to keep your code simple and authenticate everything. There will probably be no need to include something like a web server in a camera and it brings many vulnerabilities. Also remember, that users may not ever update your firmware, so keeping bugs to a minimum by making the device simplistic is a good idea. 
PS: Also authenticate using something like HMAC, as your connection will most likely not be secure. You should avoid sending passwords.
PPS: If you want some sort of developer access, use public key cryptography and always include an option to turn it off.
",2,2018-04-20 08:37:59Z,26
12,"
Peter's answer is good and covers most of the problems. The typical end-user must be taken into consideration, too. The problem with IoT devices is that many of them does something that is not perceived as data processing by the end-user. Therefore, they might be used just like the item before its new smart version, leaving default password as is.
As a developer using common default password for every device isn't a good practice. Better alternatives include:

using a random password shown to the user at initial setup
forcing user to pick own password during initial setup
using default passwords calculated from e.g. device S/N and/or MAC address using complex algorithm. Such password can be printed on a label outside the device and it would survive resets and firmware upgrades.

",1,2018-04-20 09:39:58Z,27
13,"
Let AppK the Application Key. The Application Session Key (AppSKey) and the Network Session Key (NwkSKey) are generated by using only the AES algorithm without adopt the Diffie Hellman protocol.
Let us suppose that we have a Device Node that generates a 16 bits pseudorandom nonce namely DevNonce. This nonce is sent to the application server. The application server generates a 22-bits pseudo random nonce namely AppNonce that is sent to the Device Node.
The Network ID, called NetID is pre-shared between the entities. Moreover a standard 32-bit code (for NwkSKey this is 0x01, for AppSKey this is 0x02) is used.
Finally the resulting message is padded to 128-bits.
NwkSKey = aes128_encrypt(AppKey, 0x01 | AppNonce | NetID | DevNonce | pad16)
AppSKey = aes128_encrypt(AppKey, 0x02 | AppNonce | NetID | DevNonce | pad16)

However I advise you this link to deepen the LoraWan Security.
",1,2018-03-25 16:11:59Z,28
14,"
You would want a large list of secrets on SPIFFS files (removing after use) or track used secrets (and search for usage using a set of nested 1st letter\2nd letter\etc folders) to prevent a replay attack. 
A dash button is simple; depending on your other libs and code, you probably have enough RAM left to use HTTPS (depending on the cert). That would make replay a lot harder since the prior guesses would be ""dark"" to observers. You can likely re-use keys or at least parts of keys if you use HTTPS.
Another option would be to use secure ESPNOW to talk to another ESP which has an Ethernet controller which talks over the wire to a PC that then uses HTTPS to talk to the outside world, and can further authenticate requests in a PC environment (lots of easy options) instead of an MCU environment (few hard options).
",1,2018-03-22 13:03:17Z,30
15,"
Only reason you need the additional JWT is if there are going to be different claims and identities being used with the devices that require seperate privileges.
Now as this is IoT do you have a way to revocate and update certificates? Are you able to handle all the required certificate operations?
",0,2018-02-07 21:04:59Z,32
15,"
Let me explain with example: suppose you are using device provisioning service from Azure, it requires 509 certificate. But to get signed certificate we need key/pair to be generated on the device. so KEY plays important role and loosing it is risk. So we need to design in such a way that key is possessed based on request/demand. so to request or demand, we can initiate the trust by using JWT :) so this helps to secure the design 
",0,2018-08-08 07:58:11Z,33
16,"
If your devices need Internet access, use a three router Y-configuration to separate your 'ordinary' (home/work) network from the one containing your IOT devices.
There are plenty of resources about that on the Internet.
This article goes in-depth describing the disadvantages of simpler configurations, leading up to the Y concept. The author even has a walkthrough showing how he set this up for himself.
Note: Many modern router devices let you mimic this configuration with one box only, but talking about three physical ones makes you understand it better. You may even have some old routers laying around that you now have a purpose for.
",1,2018-02-05 20:59:37Z,35
16,"
You start by not giving them access to the Internet.  If your Intranet of Things devices have no direct contact with the outside world (eg. your IoT sprinkler controller is firewalled off from the Internet and needs to go through a proxy when requesting a weather forecast), it becomes much harder for an attacker to do anything.  If there's no connection whatsoever, any attacker needs to be physically present.
",2,2018-02-02 22:42:21Z,36
16,"
You didn't mention the first rule of IoT security. Change the admin password. If possible make it complex. I have witnessed hundreds, if not thousands, of penetration attempts and most of them are attempting to compromise some form of administration account with guessable passwords. 
Another thing that you can do is register it with a directory. You may have AD available to you but there are also dozens of online cloud versions available too.
If you are going to have multiple devices this can simplify any administrative
activities as well.
",1,2018-02-05 17:36:33Z,37
16,"
IoT is a loaded term.  I'm going to assume the topic is about ""How to keep self-configured IoT devices secure"" and disregard cloud-based IoT devices.
The best way to learn about securing IoT devices is to learn how generic security works.  IoT devices are by definition, internet connected devices.  They are no different than computers, servers, network printers, routers, switches, etc.  They face all the same problems that those devices face when it comes to security.
You can find a ton of information on how to secure network infrastructure online.  There are many other resources as well, such as academic classes.
I personally, run a firewall on my router that segments my network and white-list locations I access it from.
To disregard my disregard, cloud-IoT devices are at the mercy of their cloud services.
",1,2018-02-05 20:10:00Z,38
16,"

Lastly as an importart part of the threat model is physical security, because if I put a sensor outside with a solar panel to power it, how do I stop mischievous kids or crooks from destroying or stealing the technology?

You, essentially, have to build a better mouse trap. Consider your physical security situation - is the street side of your house safe, but kids out of view of the streets will wreck things? Perhaps the opposite is true - position the sensors within view of the street (or vice versa). Next, consider replace-ability - is this a cheap sensor or a $150 weather station type deal? Consider pairing down the features of outdoor sensors to lower the cost of replacement. If it takes you five minutes and five dollars to replace the sensor, who cares if it's broken or stolen every three months. Thirdly, consider hardening - is your sensor on the top of a 30 ft roof? Is it resistant to a baseball or rock? Do you need to walk up to it and wail on it with a golf club for a few minutes to break it? Can you make it that resistant? 
All security is about making compromise ""not worth it"" for the attacker. It's hard to overcome the ""worth it"" factor for teenagers - the destruction itself is the reward, and making a harder target makes the destruction more fun. You'll have to think critically about what you need to do to make the sensors unlikely to be damaged, or make them easier to replace when damage occurs.
",1,2018-02-05 20:34:31Z,39
17,"
What does it mean by credentials
A lot of devices like this will have a main interface and a web interface, which you normally use to update the device or check status. This usually has a login, which uses the creds described above. Examples of devices like this I have seen left using default credentials:

Cameras
Printers
Routers / APs / Switches

The problem is that if someone can get to the admin interface they can configure the device to do whatever they like, or install backdoors, hence the advice to completely reset the device and start from a clean slate.
Part of the appeal of these devices is that they can be used in a plug and play way, you just connect to the WiFi and they work, the problem with this is that it is very easy to not change the default password.
",3,2018-01-31 23:21:14Z,41
18,"
Let's look at your fridge. Why is it connected to the Internet? It could send maintenance data to the manufacturer, connect to stores to order food for you, or tell you its contents (""fridge, how many eggs do I have?""). 
If you classify this data as having no impact if adversely affected, then yes, a massive infrastructure like PKI would have little value (the risk is low). 
But, it's not that low. Imagine a hacker accessing your fridge to raise the temperature and spoiling the entire contents? What about a hacker accessing the fridge and ordering tons of food on your account? What if the type of food you store indicates certain sensitive information about you (only kosher foods or only halal meats, suddenly no more alcohol (pregnant?), or a 1-off big order of cranberry juice that gets steadily consumed (urinary tract infection?)). What about stats on how often the door is opened (are the owners away?) Assessing the information this way indicates quite a different risk level. 
PKI (properly implemented) can protect the control data (who can order, who can access, who can change settings) and can protect the information at rest and the information in transit. 
Imagine a WhatsApp or Signal-like encryption process where only the manufacturer's certificates or your phone app can access the control functions, or even certain data. That's where PKI can be useful.
",48,2018-01-22 11:37:02Z,8
19,"
How tap to pay works
Assuming you are not using magstripe emulation (Samsung is one of the few companies that do this), you will be communicating with the payment terminal in a similar way to how a contactless card would communicate with it.
How secure it is
The UK Cards Association says the following:

Reassure the customer that contactless is secure. Contactless technology uses
  secure technology (the same as chip & PIN) so customers can feel confident
  when using it to pay for items.
There is a maximum amount for a contactless transaction of £30 (if it is a
  higher value payment the customer will have to verify themselves). The card
  or device has limits built into it when it is being used for a contactless payment.
  This means it can only be used for a certain number of consecutive contactless
  transactions before the customer is required to perform a chip & PIN
  transaction.
All contactless payments, as with other card payments, are covered by the
  issuing bank in the event of fraud, so the customer won’t be left out of pocket.
All contactless devices rely on the same underlying, secure transmission
  technology. On mobile devices the cardholder’s payment details are held
  securely and may also be protected using a process called tokenisation
  .
  This substitutes the cardholder’s account number for a ‘token’ value
  that is only valid for transactions from that device

Like all technologies it has advantages and disadvantages:
Advantages:

The card number is tokenised, so your physical card number is not stored on your mobile device
The transactions are online (in the UK), so any fraudulent activity can be detected quickly.
Unlike magstripe, the terminal cannot just replay the data it recorded talking to a card to make a payment with another terminal
As the magstripe is not present, a skimmer cannot record the details and then use them to make copies of the card.

Disadvantages:

Storing your payment details on your device is another copy of the details
Depending on the device you may be able to pay without noticing, (On Android your screen must be on, but you do not need to unlock the device).

References

Guide for retailers:
Accepting contactless and higher value contactless payments

",2,2017-12-28 01:30:02Z,41
20,"
The problem is less the open access to a server with some unimportant task. The problem is more that this server is probably located in the same network as more important systems (unless your home router provides network separation, most don't). Thus if an attacker manages to hack into this unimportant server he gets access to your whole important network.
There are several ways to make this harder:

If your home router provides network separation put this server into a network separate from the rest of your system. There are many information online about this.
Limit access to the server by requiring some kind of authentication. This could be plain text authentication with username and password, some secret authentication token or client certificates if HTTPS is used. What you use here depends a lot on how do you need to access the server (web browser from any system, web browser from specific system, mobile app...). In case of IFTTT see here for the possibilities you have.
Make sure that nobody can sniff the traffic and thus capture any plain text authentication or modify the traffic by using HTTPS.
Even if the attacker gets authenticated access to the API make sure he cannot exploit it further by using good programming practices and hardening the setup as described in the following.
No direct invocation of shell commands with arbitrary user controlled parameters, i.e. limit the parameters to specific values in the server (don't trust the client) or sanitize parameters if arbitrary values should be possible.
Don't run the server as root but run it as a user with low privileges. If you need to have root permissions to control the hardware write a separate local service with a small and secured API and let it communicate with your externally reachable web service (see privilege separation).
Ideally add some additional sandboxing, i.e. use containers, chroot, seccomp etc to make it hard to break out of the server. For example containers can also be used to limit network access, so that the attacker cannot access the internal network even when successfully breaking into the web service.
Harden you OS by having only minimal software installed and by keeping this software and kernel up-to-date. Privilege escalation attacks are unfortunately still common in Linux so once the attacker has low-privilege access by breaking into the web service he might be able to get root. Keeping the system up-to-date makes this much harder.

",4,2017-12-23 06:23:16Z,6
21,"
IoT security is currently so poor that there are endless opportunities for improving the state of the art using machine learning.
First, pick an area of IoT that you're interested in.  I'd recommend you pick one where you already have existing systems you can leverage, and where monitoring/sniffing tools are widely available.  Do you have a home automation system?  Do you have a car with a CANBUS network?  Do you wear a personal health tracker?  Do you have web cameras monitoring your property?  It's easiest to start when you already know something about the domain you're trying to protect.  
Next, develop a simple threat model.  Diagram the system's inputs and outputs.  Identify network protocols.  How is each trust boundary crossing step authenticated? How are the messages protected?  Make some assumptions and do some reading: if you think the devices are simple, and they talk to each other, they might be less secure than if they authenticate to a cloud server, therefore they might benefit from your tool's protection.
Now, see if you can insert some kind of sensor in between the devices and their controllers or clients.  For example, if they're connected via WiFi and use IP, you could attach a bro sensor to the network switch that connects them.  Commercial tools could help here; for example the Packet Squirrel might be a cheap way to sniff packets from the middle of an Ethernet cable.  If the devices talk via RF, can you use an SDR to monitor the airwaves?
The important thing to develop is the capability to collect traffic.  Also, take into account the ways you might analyze the traffic.  Is timing important?  Make sure your packet capture tool has precision timing capability.  Are electrical or RF characteristics important?  You might need  specialized equipment; or at least more sensitive than a $20 RTL-SDR dongle. 
Once you have traffic in hand, start applying your analytic skills.  What kinds of patterns are normal?  What kinds of attacks show patterns that are abnormal?  Are there regular time-based packets, where fast or slow responses stand out?  Are there particular sequences of packets that you can consider normal or abnormal?  Can you train your neural network to be silent in the presence of normal traffic?  Can you come up with a generic solution that doesn't generate false positives on several systems, not just yours?  Most importantly, when you attack the system, does your neural network detect it?
Again, you'll probably have the most success if you start with something you already know and enjoy.  Good luck!
",3,2017-12-21 18:49:00Z,46
22,"
Be very, very careful. It's not KRACK that is the problem, it is a lax attitude to security and privacy in general. So called ""smart"" consumer products can often be hijacked, accessed from the internet, or monitored. As a customer, it is hard to know if any specific product is safe or not.
The Norwegian Consumer Council has been on the case for a while, and produced a few horror stories. From a report, aptly titled #ToyFail, on three ""smart"" dolls:

When scrutinizing the terms of use and privacy policies of the connected toys, the NCC found a general disconcerting lack of regard to basic consumer and privacy rights. [...]
Furthermore, the terms are generally vague about data retention, and reserve the right to terminate the service at any time without sufficient reason. Additionally, two of the toys transfer personal information to a commercial third party, who reserves the right to use this information for practically any purpose, unrelated to the functionality of toys themselves. 


[I]t was discovered that two of the toys have practically no embedded
  security. This means that anyone may gain access to the microphone and speakers
  within the toys, without requiring physical access to the products. This is a
  serious security flaw, which should never have been present in the toys in the
  first place.

And from an other of their reports, again aptly named #WatchOut, on ""smart"" watches for kids:

[T]wo of the devices have flaws which could allow a potential attacker to take control of the apps, thus gaining access to children’s real-time and historical location and personal details, as well as even enabling them to contact the children directly, all without the parents’ knowledge.
Additionally, several of the devices transmit personal data to servers located in North America and East Asia, in some cases without any encryption in place. One of the watches also functions as a listening device, allowing the parent or a stranger with some technical knowledge to audio monitor the surroundings of the child without any clear indication on the physical watch that this is taking place.

And the FBI agrees:

Smart toys and entertainment devices for children are increasingly incorporating technologies that learn and tailor their behaviours based on user interactions. These features could put the privacy and safety of children at risk due to the large amount of personal information that may be unwittingly disclosed.

So unless you have a real need (other than ""this is cool"") for these kinds of products, I would say that your best approach is to simply stay away from them.
",92,2017-10-31 15:13:39Z,47
22,"
It really depends on your threat model.  I wouldn't be particularly worried about a particular sexual predator in your local area having the technical skills necessary to utilize Krack to inject voice into the toy. Unless it uses the vulnerable Linux driver, the key clearing won't work and the partial nature of the compromise for a general reset would make voice injection nearly impossible.  
Similarly, as a client device, it doesn't offer a whole lot of security risk other than possibly as a listening device, depending on if it is always on or activated by pushing a button.  Krack wouldn't make it usable as an entry point in to your network directly, so I don't see it as a particularly riskier device than any other IOT device.
As always in security, it comes down to your risk aversion though.  Personally, if I thought it would be valuable to my child (who is also 3) I don't think I would consider the local security implications as a reason not to get it for my home environment.  I'd be more concerned about the controls and security on the web side.  
My main concern for IOT devices isn't the local compromise so much as the web connected remote compromise.  The chances of a sufficiently skilled and motivated malicious individual in your direct proximity is pretty low.  The chances of a motivated and malicious user on the Internet trying to remotely access the IOT device is significantly higher and it's important to understand what holes the devices punch in your network protections.
Also, as Michael was kind enough to point out, the interests of such a broad hacker are much less likely to be concerned with your privacy and much more likely to either be interested in attacks on your other computers or on the computational capabilities of the device as an attack bot.
",15,2017-10-31 15:04:35Z,48
22,"
Welcome to the Internet of Things(IoT). This is a... thing. Therefore, it can be assimilated

Mirai is a type of malware that automatically finds Internet of Things devices to infect and conscripts them into a botnet—a group of computing devices that can be centrally controlled.

And

One reason Mirai is so difficult to contain is that it lurks on devices, and generally doesn't noticeably affect their performance. There's no reason the average user would ever think that their webcam—or more likely, a small business's—is potentially part of an active botnet. And even if it were, there's not much they could do about it, having no direct way to interface with the infected product.

The problem is that security is seldom a consideration when making toys like this. The technology to make all this work is fairly simple, but the companies aren't paid to think about this. It's a child's toy. It's meant to be cheap and easy. And you get what you pay for. 
Earlier this year, it was found that a similar child's toy had no security at all (emphasis mine)

A maker of Internet-connected stuffed animal toys has exposed more than 2 million voice recordings of children and parents, as well as e-mail addresses and password data for more than 800,000 accounts.
The account data was left in a publicly available database that wasn't protected by a password or placed behind a firewall, according to a blog post published Monday by Troy Hunt, maintainter of the Have I Been Pwned?, breach-notification website. He said searches using the Shodan computer search engine and other evidence indicated that, since December 25 and January 8, the customer data was accessed multiple times by multiple parties, including criminals who ultimately held the data for ransom. The recordings were available on an Amazon-hosted service that required no authorization to access.

I'm going to be honest. These things are scary powerful in what they can do. Even if it doesn't expose your messaging, it could still be used for something malicious like a DDOS attack. If I were you, I'd pass on anything like this unless there's something explicit about security.
",12,2017-10-31 18:09:50Z,49
22,"
This is pretty much the same kind of toy as CloudPets. Those were toys that allowed talking with the children (toy) by using a mobile app. The security was terrible. It turned out that both the user details and the pet recordings were available on databases with no password. And the company didn't even answer to the mails alerting them of the vulnerabilities.
You can view about this terrifying story on Troy Hunt blog: https://www.troyhunt.com/data-from-connected-cloudpets-teddy-bears-leaked-and-ransomed-exposing-kids-voice-messages/
Now, Talkies may actually have made the right choices (it's hard to do so many things wrong as CloudPets did!), but this showcases the level of security of this sector.
So no, I wouldn't trust this toy with my kids data. Not to mention how the toy itself could be compromised (eg. like Hello Barbie).
In fact, Germany went so far to ban the internet connected Cayla dolls over fears they could be exploited to target children: https://www.telegraph.co.uk/news/2017/02/17/germany-bans-internet-connected-dolls-fears-hackers-could-target/
",6,2017-11-01 17:29:50Z,3
22,"
Internet-enabled anything poses a risk.  As a rule, security is an expense and consumers as a whole really don't consider product security when making purchasing decisions.  For example, there was a thread on Reddit recently about a couple who got divorced and she didn't change the password on the Nest thermostat.  So while she was out, he would crank up the air conditioning or heat and cause massive utility bills.  We also know of baby monitors that have been used to listen in on neighbors without their consent.  I've attended IT security demos of internet-connected light switches, showing how easy it was to attack them.
krack is important, definitely, but when compared to a non-existent security posture is irrelevant.  Quite simply, if someone is concerned about security, I'd suggest not purchasing networkable anything unless they can identify a need for it to have a network connection and they have the skills to properly secure both it and their network.
WRT your trusted circle of phones: how often would you plan on managing that list?  What is the means to join that trusted circle?  Do you know when your friends sell their phones back so you can decommission them from your circle? (If your answer is not ""no"" to the last one, you're probably not being realistic with yourself.)
Encourage creativity.  Build skills.  Get the kid a bunch of building blocks or a train.  Get a Spirograph.  Play cards/games with them.  Find something that they will play with for hours that doesn't require your constant attention.  
",4,2017-10-31 17:22:04Z,50
22,"
This puts the ""IOT"" in ""IDIOT!"".
Most of the companies that make these have no clue how to prevent hackers from taking them over, sometimes programming comically stupid/obvious exploits into them.
The KRACK exploit might be irrelevant half the time since most of these manufacturers wouldn't figure out how to implement some form of encryption.
Any type of internet-enabled voice recording is potentially creepy and downright dangerous invasion of privacy. These devices likely use the cloud for sound processing and storage considering they are almost certainly based on low-grade ARM chips and minimal cheap flash storage at most.
Even if the device is properly made, there's no similar guarantee on the cloud app it uses. You'd be surprised how often researchers happen to stumble on valuable leftover data in the cloud that the previous user of a logical machine instance failed to clean up.
",2,2017-11-01 04:44:14Z,51
23,"
The design requirements of your system are not fully clear to me. But I will assume in the following that you have full control over the setup and that this setup has only the following requirements:

secure connection between devices and central management server
central management must be able to send commands to devices at any time
IP address for the devices are dynamic and not known to the management server up front, i.e. the device must somehow announce itself to the server initially.

There are several options to implement this, like

Create a VPN between the devices and the management system. This can be done for example with IPSec or OpenVPN and with some variations with SSH too. Within this VPN the devices can have a fixed private IP address thus addressing a device from the server is no problem. And TLS is not needed since the VPN already protects the connections against attackers outside the VPN.
Create a TLS connection from the device to the management system which requires only a certificate on the side of the management system. This TLS connection can then be used with a custom protocol to communicate between server and client. Depending on the setup and restrictions because of firewalls it might be useful to not create a simple TLS connection directly on top of TCP but use secure WebSockets for the bidirectional connection. Still, only a certificate on the server side is needed which is trusted by the client.

These setups are similar in that the secure connection (i.e. VPN tunnel or TLS) is initiated from the client, since the client IP address is dynamic and the management system has a static IP or at least a static hostname. It might be useful to add some authentication of the client to this because you don't want that some attacker connects to your management system.
One way would be to create your own PKI with your own CA and issue a unique certificate for each device. The subject of this certificate can be some device-id which you can use to identify the device. These client side certificates could be used for authentication in the VPN or as client certificates in the TLS connection. And, if a certificate got compromised (i.e. someone hacked the device) you can simply centrally block access with this certificate in your management system.
",2,2017-10-26 16:56:35Z,6
23,"

There are hundreds of such devices not having host-name and their IP
  is not static. So SSL certificates may not be possible here???.

SSL client authentication certificates can be issued without a DNS resolvable name or fixed IP addresses.  The server does not have to be configured to use DNS to verify them.
As Stephan suggested, standing up your own PKI for this system seems like the right approach.  Unless your organization already has their own PKI, this can be as simple* as using OpenSSL to generate your own self-signed certificate, then using that CA certificate to sign all your server and client certificates.  You'll have to install the custom CA certificate as a Trusted Root certificate in each of the servers and clients.  (If these are part of a closed ecosystem, consider removing any trusted root certificates that the clients don't need.)
You will probably have an inventory of the devices as part of running your organization.  Whatever ID is assigned to them, that would probably be appropriate to set in the DN of the certificates.  But you certainly don't have to do that either, especially if you don't want to put your internal name on your certificates.  Instead, you can pre-generate a large pile of certificates in advance, and hand them out to new clients when they register with you.  Then keep track of the certificate ID and the client ID it was issued to, and use that relationship for granting client permissions.  This keeps your issuing CA safely offline from the system that registers your clients.
If your organization has a PKI already, consult with them.  They should assign an Organizational Unit reserved strictly to your devices.  You can then have your server verify the client certificates have the OU valid only for your devices.  That way, other groups in your org won't be able to issue certificates to rogue/unknown devices.
Remember, once devices are out in the field they can possibly be stolen, tampered with, and/or disassembled.  Don't place any more trust in the client certificates than you need to.  Don't allow these same device certificates to grant administrative permissions on the server.  And if a device goes missing, invalidate its certificate quickly.
* I apologize for using the word ""simple"" in conjunction with PKI.
",1,2017-10-26 18:06:17Z,46
23,"

There are hundreds of such devices not having host-name and their IP is not static. So SSL certificates may not be possible here???

Remember: before trying to figure out any security solutions, start with determining your threat profile.  What attacks are you trying to prevent?
I would guess that one of the major ones is to prevent an attacker from pretending to be the control server and successfully sending commands to the controllers.  For this, then, you want the devices to be able to authenticate the server, and that requires a certificate on the server side, not on the devices - either a traditional HTTPS (or HTTPS-like, if you're using a different protocol) setup if the clients initiate a connection to the server, or a client-side certificate if the server is initiating a connection to the devices.
",0,2017-10-26 17:16:29Z,53
24,"

A wider exposure surface

That is correct. In case you did not open any ports to the internet so far and you open one now the direct consequence is a wider surface.
In my point of view the indirect consequences are much worse than that. Consider, that you put a device on the internet that is meant to be attacked. And it will be. So the question is rather when than if it falls. An even though you are using e.g. the DHSIELD software wich is designed to be hacked, someone might be breaking your ""sandbox"" at one point.
If someone compromises a host in the internal network a pivoting will be possible if he gains enough rights on the machine. So a secure approach using honeypots is to put them in a ""DMZ"" that has no connection itself to the inner circle of your network.
In that way its a lot less dangerous if it gets compromised.
",2,2017-10-23 08:01:50Z,55
25,"
UPnP can safely be disabled. If a service needs a port punched it’ll tell you. 
In regards to MITM and altered firmwares: it’s a Wild West our there. Some manufacturers have secure signed images severed on secure servers, some just flash whatever random binary they find over HTTP. You really won’t be MITM’d be your ISP hopefully so that’s not an issue however these unsigned and insecure firmware devices can be compromised from an adjacent compromised device. So if you’re already breached you may be in for an even bigger headache where your only real option is likely to bin the device. If the device is your fridge I’m sorry. 
This same principle also applies to all outgoing connections as well if there is some vulnerability in how it handles the responding data. If either the server is compromised or an adjacent device is compromised then an insecure and vulnerable device could be hijacked.
What’s your best bet? Probably keeping them from accessing the wider internet. When that’s not possible, use client isolation so that hijacked devices can’t infect your other devices. Really your best bet is to use as few IOT devices since you really have very little control or access to the configuration.
",0,2017-10-22 19:28:06Z,57
25,"

What (network) attack vectors need to be considered?

All of them. Well, most of them, unless you happen to control the launch codes for nuclear missiles.
While sitting behind a masquerading router with no port forwarding, external attackers can't connect directly into the IOT target, the IOT target may make frequent client connections out to the internet. Many devices are explicitly design to behave like a server but using polling or long lasting client connections to a broker service with the end result that they are no different than a server. There are even ways for it communicate with the outside world without having to make a connection externally (e.g. DNS smuggling). 

would need to make active connections

What is an ""active"" connection? I presume you mean a client connection.

never seem to mention what possible side-effects this might have

Because even when the role of the device is well defined (not the case in your question) the way it operates is not. The insecurity of IOT devices arises from multiple causes:

device not designed to operate securely
behaviour of device not documented
device deployed in a context other than that which it was designed for
users ignoring the documentation which is available when deploying the device


Do these [IOT device client] connections pose a threat?

Potentially yes. There are well known patterns for protecting against the third-party attacker model - but the reason people get excited about IOT devices is that so few of them implement these solutions, or fail to implement them correctly (e.g. requiring SSL, but not validating the certificate). 
",0,2017-10-23 12:16:23Z,58
26,"
Many companies (like fortinet, stormshield, ...) use selfsigned certificates. They have a internal PKI which produce these certificates but they are not trusted by web browsers.
Many of them permit to upload the owner certificate. There is no big deal with this operation except that permissions must be carrefully configured
What do you need is probably a PKI to manage your own certificates. Then, you can set a secured https channel if you control the gateway and the client.
If you do not control the client, you cannot force it to trust your Certification Authority (CA), so you cannot ensure that https between the gateway and the client is safe (man is the middle is possible).
So you can propose to clients to add your CA as a trusted one or make your CA be signed by a certification well known authority (comodo, verisign, etc.)
",1,2017-09-29 12:54:13Z,60
26,"
I suggest that the critical part would be the ability to add and remove certs from the device, and to set/change any passwords. 
If the device is acting as a server then the definition of certs is the server certificate(s) and private key(s). If the device acts as a client or validates client certificates then the definition includes ca certs. If you are not using client certs then you need some other way of authenticating management operations (e.g. password).
So that's at least 3 security related functions.
While CMP/rfc6712 springs to mind as a method for renewing a certificate, it strikes me that you probably should not initiate the certificate renewal on the IOT device.
",1,2017-09-29 18:05:47Z,58
27,"
Some people argue that code which is open source can be audited by many and therefor contains little bugs. On the other hand, attackers have the same easy access and also look for these same vulnerabilities. There is definitely a tradeoff here which is not correctly described in previous answers.
Others mention that code should be inherently secure and therefor requires no obfuscation/encryption/hiding. It is true that a system should be designed to be secure even if you know how it works. This doesn't mean that this is always the case AND the implementation is flawless. In practice, code is never 100% secure. (Take a look at web app security: Why do we need security headers to protect us against XSS and CSRF attacks if there are no vulnerabilities in the web application?) Additional security measures can be taken by trying to hide the code through encryption and obfuscation. In the mobile world, reverse engineering is even seen as a serious risk: OWASP Mobile Top 10 risks. 
As no system is 100% secure we can only try to increase the effort required to break it. 
So now, the tradeoff between open source/easily available code VS encrypted and obfuscated code. Allowing public review on you source code can help reduce the number of bugs. However, if you are a small company where the public has little incentive to freely audit your code, there is no benefit from publishing your code as nobody will look at it with good intentions. However, it becomes much easier for attackers to discover vulnerabilities. (We are not talking about the newest iOS version which every security researcher is trying to crack)..
In this case we aren't even talking about open sourcing the code for public review. We are talking about encrypting the firmware in transit. Security researchers are not likely going to buy your device to obtain the code to discover and publish vulnerabilities. Therefor the chance of having the good guys finding the vulnerabilities VS the bad guys finding them decreases.
",5,2017-09-05 07:50:19Z,62
27,"
No. You should not rely upon the obscurity of your firmware in order to hide potential security vulnerabilities that exist regardless of whether or not you encrypt/obfuscate your firmware.
I have a radical suggestion: do the exact opposite. Make your firmware binaries publicly available and downloadable, freely accessible to anyone who wants them. Add a page on your site with details on how to contact you about security issues. Engage with the security community to improve the security of your product.
",86,2017-09-01 13:38:54Z,63
27,"
Doubtful it would be beneficial. It is by far a better option to push it open-source than closed source. It might seem silly and even controversial at first, but opening up a project to the public has plenty of benefits.
While there are people with malicious intents, there are also people wanting to help and make the internet a better place. Open source allows more eyes to look over the project, not only to view about potential features, bugs, and issues but also increase security and stability of the ""thing""
And to agree with Polynomial's answer, engaging in a community and building a base of people that help you out with security, will increase the client base by a significant margin.
",11,2017-09-01 13:52:35Z,64
27,"
A well-designed firmware should rely on the strength of its access key rather than relying on the attacker's ignorance of the system design. This follows the foundational security engineering principle known as Kerckhoffs's axiom: 

An information system should be secure even if everything about the
  system, except the system's key, is public knowledge.

The American mathematician Claude Shannon recommended starting from the assumption that ""the enemy knows the system"", i.e., ""one ought to design systems under the assumption that the enemy will immediately gain full familiarity with them"".
You may be interested to know that prior to the late Nineteenth Century, security engineers often advocated obscurity and secrecy as valid means of securing information. However, these knowledge-antagonistic approaches are antithetical to several software engineering design principles — especially modularity.
",6,2017-09-01 18:33:32Z,65
27,"
Are you sure you are not confusing two cryptographic methods?
You should certainly sign your firmware updates for security reasons. This allows the device to verify they are from you.
To encrypt them adds a little bit of obscurity and that's it. Since the decrypting device is not under you control, someone sooner or later will hack it, extract the decryption key and publish it on the Internet.
",3,2017-09-02 05:49:21Z,66
27,"
You need to ask yourself this question.  Is someone clever enough and interested enough to download your firmware and start looking for vulnerabilities going to be deterred by an additional firmware encryption layer where the key must be revealed?
This is just another hoop to jump through, no different than figuring out what disk format your firmware image is in.  This isn't even a particularly difficult hoop to jump through.  Keep in mind that all FAR more sophisticated methods of what amounts to DRM have all been broken.
Odds are someone determined enough to hack your internet connected coffee maker/Dishwasher isn't going to be deterred by an additional encryption layer.
",0,2017-09-01 19:40:07Z,67
27,"
In the sense of ""will the encryption of firmware prevent the detection of vulnerabilities in my code?"" other answers have addressed the core of it: although it may discourage some attackers, security through obscurity leads to a false feeling of invulnerability that is counterproductive.
However, I'd like to add an extra bit on the basis of my experience. I have seen that the firmware packages are sometimes encrypted, but the motivation for that is only to preserve a company's intelectual property, rather than be a control against attackers.
Of course, hackers often find ways around this ""control"", but that's a different story.
",0,2017-09-08 06:53:14Z,68
27,"
Several people said you shouldn't rely on obfuscating the code by encrypting it, but to just make it secure. Quite recently the encryption of some rather critical software in Apple's iPhones was cracked (meaning that hackers can now see the actual code, no more). It stopped anyone from examining the code for three years, so the time from release to first crack has been increased by three years. That seems like some very successful obfuscation. 
And encryption goes very well together with code signing. So when your device is given new firmware, it can reject any fake firmware. Now that part isn't just recommended, that is absolutely essential. 
",-3,2017-09-01 21:03:46Z,69
28,"

""All the examples I have looked shows only how to do a brute force for the extension .php?. The login page of the IP camera I am testing however is in .asp""

extension of .php vs .asp should not matter.  

I do not see any <form> in the source code of the login.asp page. 

how did you come up with the URL /PSIA/Custom/SelfExt.... etc?

hydra -l admin -P wordlist.txt 192.168.1.3 -s 85 http-form-get
  '/PSIA/Custom/SelfExt/userCheck:lausername=^USER^&lapassword=^PASS^:User
  name or password is incorrect:H=Authorization: Basic
  YWRtaW46QkM1MUZFOTUyRkI3' -V

You appear to be using a mix of Forms based authentication and Basic Authentication with this command.  Also, you have expectations for two variables to be filled in with ^USER^ and ^PASS^ and are only supplying one variable as the file wordlist.txt just has one field.  How did you determine that this login is using Basic AUTH?  Does the basic auth pop up when you try to log in from login.asp?  
",1,2017-08-28 17:32:34Z,71
29,"
NAT does not explicitly add security. It only implicitly adds security because it needs to keep a state for all outgoing connections to map incoming packets back to the originating connection - and thus implicitly denies any incoming packets which don't match an outgoing connection. But, this security can already be done with a stateful packet filter (like iptables) which is usually the base for a NAT solution anyway. 
But, this kind of security is actually only needed to protect services on the phone which accept packets from outside even though they don't expect such packets. This means services which bind to any address instead of only the localhost address. Usually there are no such services, i.e. either they bind to localhost only or they actually expect traffic from outside. 
That does not mean that such a packet filter is not useful on mobile devices. It actually is very useful because for example on a rooted Android phone you can use the builtin iptables with a nice interface (such as Android Firewall) to limit which applications can access the local network, use WLAN, use mobile data or even can be used with roaming.
",2,2017-07-19 18:06:19Z,6
30,"

...take control of a connected thermostat and raise the temperature high enough to potentially kill someone in their sleep?

Not really. The scenario you are talking about is highly theoretical if at all possible. Thermostats do not go up as high as to kill someone in their sleep. Attackers might cause minor discomfort for a brief period if they are able to manipulate temperature settings. But that's about it.

....are there any safeguards built into a Nest thermostat ...?

You would have to look at Nest documentation for that. But my guess would be that they do not allow temperatures to be raised above certain, reasonable, thresholds.

Are there any known examples of this, and is there any practical way to mitigate the risk aside from not using a connected thermostat or using a strong, unique password to protect the account tied to the device?

There are plenty of examples of attackers exploiting weaknesses in IoT devices such as thermostats to use them as part of their botarmy in a DDoS attack (Google search should reveal some of these). Similarly, they might hold your IoT device for ransom until money is paid to a certain bitcoin address, but there have been no known cases of ""assassination"" attempts using thermostats. 
To mitigate the risks of someone gaining access to your thermostat, you could put it behind a firewall that filters out connection attempts from unknown IP addresses or put it entirely on an internal network behind a NAT. Also, you should constantly update the firmware since most OEM manufacturers fix known security weaknesses regularly in the latest firmware.
",1,2017-07-07 19:30:06Z,44
31,"
My answer would always be ""in comparison to what else?""
In this moment, SHA-1 is weaker than SHA-2:
https://en.wikipedia.org/wiki/Secure_Hash_Algorithms
It's got known weaknesses, a smaller number of security bits, increased risk of collisions, and SHA-2 is fairly prolific, has a good performance, and is fairly widely accepted.  So... why not use something better?
There is a reason - compatibility.  There will always be some suitably old technology out there that simply cannot (or has not) been upgraded to use the current recommended best practice.  So the big question is - does your business model benefit more from interoperability, or cryptographic strength?  
My not-spending-lots-of-time-doing-math-research answer on SHA-1 vs. SHA-2 would be that it's a no brainer - SHA-2 is better.  But if you told me that your biggest customer has a substantial investment in SHA-1 only technology and can't improve the system... well... I'd probably advise finding a way to limit the scope of where and who you do SHA-1 with, and then make sure you have that customer sign a nice sounding waiver that they understand that you can't be entirely responsible for the risks of using a not-best-practices algorithm.
",2,2017-06-26 19:50:58Z,75
31,"
The risk of using any algorithm is down to your risk appetite. Only you know if the risk of a SHA1 attack is something you are willing to accept. 
I would use it if that was the only option for none mission critical or sensitive data. But that's only a personal opinion.
",0,2017-06-26 17:45:16Z,76
32,"
Your question is similar to asking if there are any indicators that you can trust a person you've never seen before. The answer in this case is that there are no direct indicators and that you have to rely on the experiences of others with this person. This means in case of these controllers

Check the reputation of the company.
Check how much these products are used.
Check what experts (real experts, not claimed experts) say, i.e. if they recommend these products or not. Note that there might not only be a problem of explicit backdoors but also of bugs which might be used as a backdoor.
And, in times where the NSA and probably other government agencies are able to compromise everything you should also check the distributor and the parcel service, since you might get a version of the controller which was compromised on the way to you.

Of course, how much efforts you take in checking all these depends on your security needs. And this is again the same with the unknown person: if you just risk of loosing $10 when trusting this person you will not do much checks but if you risk of loosing $10.000.0000 you'll probably invest a larger sum in doing deeper background checks.
",3,2017-06-15 19:25:25Z,6
33,"
UPNP opens ports on the router without user intervention. This means in your example the FTP server would (could) be exposed on the internet by the IoT itself.
This means if there was a vulnerability within the FTP server or poor programming/setup by the IoT company, then an attacker could attempt access just as if you had manually port forwarded the port to the device. 
My advise in this example with UPNP is if you do not need it at all, turn it off at the router. If you need UPNP for a device then

make sure the device is patched with latest firmware
make sure the device doesn't have default username and password 
make sure (if possible) any services that the device has running are disabled if not being used
ideally run the IOT devices on a different network to your main home internet (such as phones and laptops) - that way if the device is compromised the hop to other devices on your network is mitigated

",3,2017-05-31 19:03:08Z,76
34,"

Is it safe to assume that in local networks attackers will not be able to intercept a request for login page from a browser to a device and send a hijacked login page instead with malicious JavaScript?

No, you cannot assume that unless the connection itself is reliably protected (e.g. an inaccessible UTP cable or already authenticated WiFi connection).
",5,2017-05-08 17:27:50Z,80
35,"
Lateral movement.
When you gain administrator/root access to a device with poor security you have a feet in a network, the first thing you need to do is to backdoor the device in order to get a persistent access to the device.
Then, you can easily create a SSH tunnel between your attacking box and your target, forward multiple or all ports through the tunnel. At this point you are able to work (play) with the rest of the network.
In few words, you create a pivoting point.
",0,2017-04-25 15:12:48Z,82
36,"
Unsure whether this is really a security question, but it is indeed a problem that security admins have to face.
Good practices recommend to considere that using some one's else credential without their explicit agreement is never allowed. IMHO, it is the same as entering their house, because you have found their keys.
Of course there are exceptions:

officers conducting legal actions can provided a legal court mandated it
in a corporate environment where a staff member has mission critical data protected by their credentials - in that case, the 4 eyes rules should be observed: 2 persons (generally one member of the SI department and one manager), each controlling that the other only looks at the necessary files
probably others...

You must consider that you enter their house when they are not there, so take the same precautions: only do that in exceptional conditions, ensure that it is really necessary, and only do the absolutely required actions.
IMHO, the present use case, a private investigator coducting an investigation for a university, is not a valid one, and you really should consult a lawyer about it.
",0,2018-11-06 15:38:21Z,83
37,"
The point of encrypting the key store is that theoretically you should be able to publish the encrypted file to the public and have nothing to worry about. 
With that said, all programs have bugs, and to counter that threat it's a good idea to store your key file in an encrypted/protected area such as Dropbox. 
While there's a chance Dropbox gets hacked or an employee steals your key store and attempts to hack it, the likelihood is extremely low.
Make sure you're using best practices and you'll be fine. 

don't reuse the password for encrypting the key store
the keystore pass should be difficult to guess
use the latest version of the KeypassX program
Sign up to haveibeenpwned to be notified about any breaches

",2,2018-10-06 17:18:14Z,85
38,"
Generally, there's a long list of servers and protocols vulnerable for UDP amplification. A few command line calls to determine the presence of some of the most prominent amplification vectors might be found here. All the commands should better be invoked remotely. E.g. if your server is 192.0.2.1, then, to check for an NTP amplifier, you do
ntpdc -nc monlist 192.0.2.1

from your laptop or an adjacent datacenter machine.
I'd recommend to check NTP and DNS first, and if there's no match, then to dive into details.
tcpdump -ni any udp on the server itself will definitely help you to dive. If you're able to spot any traffic you don't expect to see in the dump, you can then track down the vulnerable application listening on the port seen in the dump by looking (or greping) at the output of ss -lpn.

EDIT. So now, as you've provided the IP address of your server, I've been able to check it myself, and it turns out you have a PORTMAP amplifier active:
$ rpcinfo -T udp $ip | wc -l
      17
$

So you basically needed to go through the rest of the checks past DNS and NTP ones.
The fix is obviously to disable port mapper (here's how you do it on Debian/Ubuntu) or at least block the access to the port 111/UDP via firewall.
",1,2018-10-05 22:58:25Z,87
39,"
I'm not aware of any AWS hypervisor bypasses to date, then again I don't use AWS. Here's an article (admittedly from 2014) talking about the concerns you raise.
https://www.zdnet.com/article/hypervisors-the-clouds-potential-security-achilles-heel/
While the hypervisor doesn't seem like a likely attack vector, there's lots of ways AWS setuos can come under attack here's a pretty good list as well as a real world scenario of AWS vulnerabilities
https://rhinosecuritylabs.com/cloud-security/aws-security-vulnerabilities-perspective/
If an attacker has physical access to the system, it's safe to assume in all but the rarest if cases they can compromise the system in a potentially undetectable way. 
",0,2018-10-06 06:17:57Z,85
40,"
TL;DR: Not secure.
Exposing your VM to a single IP in practice mean that you are blocking connections from other IP addresses. This distinction is important, because IP addresses are not secured in any way and can be spoofed. Therefore this does not prevent the attacker from sending data to your VM with a spoofed IP. 
It does help to a certain extent as it is hard/tricky to intercept the responses from the VM (as they are sent to the IP of the office, not the attacker). This hinders the attacker as he has to hack the VM blind or somehow intercept the responses (MITM for example). However this does NOT prevent the VM from being hacked.
VPN of course on the other hand uses proper authentication and is therefore much more secure.
",1,2018-08-11 20:13:03Z,26
40,"

Given that my VM's and office's external IP addresses are not publicly known to a potential attacker and I'm not directly targeted, is that likely to happen? I'm trying to figure out the balance between more security and less complexity

Unfortunately this makes absolutely no difference, as an attacker is likely not targeting your company, but just running a scanner like nmap, which stumbles on your IP and then tries to attack using known vulnerabilities. 
",0,2018-08-12 03:50:52Z,18
40,"
If you lock down the cloud VM so that it only accepts connections from your office, then it's as secure as your office. No one on the internet is realistically going to try blind attacks, and to make it more feasible they would at least need to be able to MitM or intercept the traffic to/from your office. Unless your threat model includes major ISPs and nation states I would say you are absolutely fine with this approach.
",0,2018-08-25 16:22:40Z,91
41,"
The rules are:

you cannot protect a machine from its administrator
you cannot protect a machine from someone that has physical access to it

Depending of the kind of cloud service (PaaS or IaaS), the cloud admins may be administrators of the client VM. If they are, no more questions: they can read everything from the client machine. But even it they are not, they can always take a snapshot of the running machine and put their hands on it. As on a cloud, the VM is supposed to be able to restart unattendedly, any secret must be present in invertible form in the machine itself, so if the attacker is the hosting service, you can only use obfuscation technics, but no strong encryption.
TL/DR: The answers for question 1 are no data is not private and yes the cloud can read it. For question 2 the answer is still no you cannot securely hide anything on a cloud machine.

Not necessarily relevant here, but added for completeness:
Above assumes that the data has to be used in the VM. But there is a use case where data in the cloud can be seen as securely encrypted: when the VM contains encrypted data and not the key. That means that the data has to be encrypted on a remote system and is sent in encrypted form. It can later be retrieved (still encrypted) from the same or another system knowing a decryption key. Examples are webapps using client side encryption, or even more simply, raw storage of an encrypted file or container.
",3,2018-07-20 11:30:53Z,83
41,"
You should encrypt the VM disk, but the Cloud provider can still access your encryption key on the RAM. You can read more about it here
If the attacker is the Cloud Provider, they can always be able to access your VM information, one way or another. You can make it harder, but it is possible. Choose your Cloud Provider very well.
",-1,2018-07-20 10:13:06Z,93
42,"
The solution I use in the case of secrets (not only for your case, but also to make sure that I do not commit them with the code) is to put them in an environment variable when starting my code
$ API=kjsdhtgsdsdskdjsldjsldjklskjd /usr/bin/python3 mycode.py

It is not absolutely secure as the key is still in memory. It is also a pain to start the command with the secrets like this.
I usually end up adding the keys to the systemd service file I start my code with. It is in a file, but at least not together with the code.
Finally you can prompt for the keys at startup, which is not practical is the code is supposed to run headless.
The enterprise solution is a HSM, which is a glorified service to retrieve your keys from, based on some criteria.
",1,2018-06-26 11:12:27Z,95
43,"
How are they limited
Using OAuth, you ask for scopes, which are similar to permissions, this means they can only use the credentials for what they ask for, and cannot do things that are out of scope.
Some of the scopes do cover multiple things though, for example being able to copy an email means they can also read the email, which is not something that OAuth could defend against unless Gmail provided an encrypted access scope, but then if Gmail disappeared so would the decryption keys.
The benefits of using OAuth are:

You can revoke one app without revoking all of them
You can change your password without losing your apps
You can limit what apps do
Apps need to explicitly ask for scopes, so they can't escalate permissions

What do they ask for
Visually

Behind the scenes
Using data from the docs and a test login I found the following scopes are requested

email 
View your email address
profile
View your basic profile info
https://mail.google.com/
Read, send, delete, and manage your email
https://www.googleapis.com/auth/drive
View and manage the files in your Google Drive
https://picasaweb.google.com/data/
This appears to be an old scope: Info
https://www.googleapis.com/auth/calendar
Manage your calendars
https://www.google.com/m8/feeds/
Manage your contacts
https://www.googleapis.com/auth/tasks
Manage your tasks

Conclusion
As you can see above, they ask for a lot of permissions, in theory, they could get away with the read-only variants of the above, but they may allow you to push data back to your account, which requires being able to write.
",5,2018-05-25 07:47:25Z,41
44,"
TL;DR - You should receive a report from the penetration-tester which will evaluate the security of the systems you're running, usually they will use a CVSS score to judge how vulnerable something is.

If you've had a professional pen-test they ideally will produce a report on the issues they found, how they exploited the issues and of course how critical those issues are. This is typically done with a CVSS score as it's a good way of measuring how bad the vulnerabilities are.

The Common Vulnerability Scoring System (CVSS) is a free and open industry standard for assessing the severity of computer system security vulnerabilities. CVSS attempts to assign severity scores to vulnerabilities, allowing responders to prioritize responses and resources according to the threat. - Wikipedia

So in short, yes ideally they should be giving you CVSS scores, you should also receive their report which will detail their findings from that you will be able to work out how to fix it. Obviously some common-sense is required, the pen-tester can only evaluate what they know; like every human they can be wrong, so when reading through their report just make sure it all makes sense & you can evaluate the security yourself (to some extent) obviously the pen-tester is more qualified than yourself in identifying vulnerabilities so don't second guess everything they've told you, just make sure it makes sense.
Like everything, a lot of this depends on your threat-model if you're a small company and assume that you won't be getting hit by nation-states and he points out a flaw in a crypto suite you're running which only a nation-state could attack (and you know it will cost too much to fix) then maybe it's best you carry out an internal risk assessment on that. Just because they've produced a report doesn't mean you cannot produce your own and identify risks yourself - in-fact I'd encourage it. It makes no sense for a company to patch everything (if they cannot afford it) just be aware of the pen-testers level of expertise.
In essence, it all comes down to Risk Analysis
",1,2018-06-01 05:32:23Z,98
45,"
It is true you need to trust your Paas provider. They have full access to your stuff. What I would argue is not completely true is that a VPS is safe. It is maybe a bit safer, as it would be slightly harder for the provider to set up surveillance of your setup, but they still have full access to the underlying hypervisor, and can therefore access contents of your memory, and from there everything. If you want any real amount of security, you need a dedicated server and for full security, this dedicated server needs to be physically protected.
",1,2018-04-28 12:10:17Z,26
46,"
In at least some AV products (Kaspersky is one I'm familiar with), the local application has signatures and heuristic/behaviour controls. What it's doing is (I think) three things:

If there's doubt (heuristics and behaviour detection are statistical systems so they might not be 100% sure how to classify something), or it looks wrong but doesn't match a signature, uploading the file can allow human examination, and updating of signatures so that other systems are more exactly protected.
If its not clear or the user has specified they will choose all actions, then it can be useful to check how other users have handled the same file. If most block it, the user can be told so. (I've used this facility quite a lot for a sense of comfort on ""odd"" system processes that I can't tell are needed or wasting resources.)
Anti malware companies also track malware quite a lot. If they can't get feedback what has been detected (and what variants or novel behaviours seen), they can't do their own work as accurately. To take a simple example, Stuxnet apparently tried to attack only if it detected certain hardware and geographical regions. An antivirus company in (say) Canada just might never see many files to analyse them, unless their software in other locations can autoforward suspect files to them.

Against that, a lot of the time a hash (used as a numeric value that uniquely identifies a file) is enough, and much quicker to send, so I would expect a lot of the time, that is what's sent and checked. You can see this on the website virustotal.com which let's you enter a hash of a file instead of uploading the entire file itself.
Yes there are security concerns. But given the power within the system granted to an antimalware package, if you don't trust them, you'll have far bigger concerns than their cloud facility.....
",0,2018-04-12 16:42:55Z,101
46,"
This works differently depending on the AV that you are using. Some engines send the file to the cloud and generate a signature. In this case, the response is yes, they send the files to the cloud and this is a privacy issue (in some countries). On the other hand, some engines take metrics from the file and send these metrics to the cloud for post analysis or whatever. I would suggest you check the terms and conditions of the AV and you will probably find the answer. Or just ask them, specially if the information that they scan is protected and can not be transfered and things like that.
",0,2018-04-12 14:32:45Z,102
46,"
Antiviruses often send suspicious files to their editors to enable further analysis by humans, to determine if they're actually harmful or not. If yours does that, this can definitely be a privacy concern, and you should think about it before deciding whether to leave it on.
Some antiviruses (e.g Avira) prompt you by default before sending each file. If yours does, there's no harm in keeping the option active. If it doesn't and you're not comfortable with a few of your files being potentially seen by humans, you should probably turn it off.
",0,2018-08-10 20:42:54Z,103
47,"
I'll answer for Nessus and AWS.
Nessus has several options for performing port scans (e.g. netstat (WMI), SYN scan, TCP scan, and UDP scan). Some scans have specific requirements. For example:

TCP scanning requires Nessus to be installed on Linux (otherwise this scan falls back onto a SYN scan)
netstat (WMI) requires the target to be Windows
Netstat 'scanner' requires the targets credentials and the target has netstat available

Assuming that you're meeting these requirements for proper scanning, then I believe you likely have a networking issue and would recommend some low-level testing of a specific example. For example, if you're performing a TCP scan and, using nmap, see port 22 open on your target, then attempt to telnet to that port at the target from your Nessus host to verify that it's accessible.
Best practice is to run port scans on both the internal and external networks to identify what's open both internally and externally. Running the scans on internal addresses within your VPC should be done from within the internal network. To protect against an attacker who has access to resources within your VPC, run the scan from within the VPC. To protect against an attacker positioned within your internal corporate network (e.g. with a direct connect to AWS), run the scan from within the corporate network (assuming this is outside of the VPC). Defense in depth would dictate performing each of these approaches and ensuring results meet your expectations.
One technique I've used to perform periodic scans: configure an AWS Lambda function to run periodically and perform the scan. It will take some scripting but is easy to configure and you have flexibility in where to send results.
",1,2018-04-05 02:35:01Z,105
48,"
Hyper V has shielded VMs, which addresses some of your concerns. https://docs.microsoft.com/en-us/windows-server/virtualization/guarded-fabric-shielded-vm/guarded-fabric-and-shielded-vms
However, the ultimate answer is “no there is nothing you can do”. One of the fundamental laws of computer security is that if an attacker (in this case the cloud provider) has unlimited physical access to a computer, they can compromise the security of that computer. 
",1,2018-03-23 16:51:06Z,107
49,"
When you have a larger organization with a diverse IT infrastructure, then you have to get used to the fact that Identity Management becomes a full-time job. You need someone (or even a whole department) who is responsible for managing user accounts on all the IT resources you have. They (and they alone) are responsible for:

Creating accounts for IT systems
Assigning permissions
Revoking permissions when the functions of people change
Checking regularly if accounts are still actively used and lock them when they are not
Checking regularly for accounts with critical permissions and evaluate if they are still required
Maintaining records of all these activities so you can always tell who had which permissions during which timeframe (critical in case of a data breach)
Slapping people who put company data on cloud services not managed by them

When you want to minimize the work overhead for this, then you might want to consider to centralize and consolidate your IT infrastructure. Instead of distributing your IT infrastructure all over the net, you might want to consider to host more of it yourself. For every product mentioned in the question there is a solution you can install on-premise. This gives you far more control over your company data (have you actually read what you allow Google to do with your company secrets you upload to Google Drive?). It also gives you the ability to centralize the account management in your company. Many products support protocols like Kerberos which allow them to use user accounts from a centralized system. When every employee and business partner has just one account for everything in your company it becomes a lot easier to manage (for you and for them too). 
""going through all our files, and determining who needs access and who doesn't"" is something you shouldn't have to do. Proper enterprise-scale permission systems should be role-based. You don't get access to /project_x/business_plan_V14.67.docx, you get the ""projectx participant"" role which gives you access to /project_x/*.
",3,2018-02-26 17:28:24Z,109
49,"
I haven't seen any elegant solutions to your problem. I've seen the following:

Identify the responsible Point of Contact for each entity with access and provide them a list of that entity's users every quarter or six months and ask them to validate the users' ongoing requirement to have access.
Automatically revoke access to users who have not accessed any resource in 'value X' amount of time and re-enable upon request.
Request HR or Supplier Management contact you to revoke permissions for contractors or suppliers once their contracts are terminated or allow HR some level of management access to the platform.

Would it be possible for your business to issue a two-factor authentication soft token (e.g. Google Authenticator) which must be used to access any resource and you could centrally reactivate tokens for redundant users?
",1,2018-02-13 15:13:02Z,110
50,"
Spectre is far harder to use than Meltdown.  In a cloud hosting situation, an attacker needs to know:

What software the target is using
Where in memory that software is
Where in memory the target data is
The behavior of the host CPU's branch predictor
The behavior of the host CPU's speculative execution system
and possibly some other things I'm forgetting

It's also much slower than Meltdown.  (Meltdown can read protected memory at a rate of around 500 kbyte/sec under good conditions.  The Spectre paper didn't give a speed, but the need to train the branch predictor for essentially each byte read means I'd expect a rate of single-digit bytes per second.)
Because of the difficulty in using it, Spectre can't usually be mass-deployed.  I expect three-letter agencies will be making heavy use of it, but it's the sort of thing you'd use in a highly-targeted attack, where Meltdown is better suited for untargeted sweeps for interesting data.
In the short term, probably the best thing that hosting providers can do is make it easy for users to run their own software.  A Spectre attack tuned for Apache 2.2.34 for Debian Wheezy on x86-64 is totally useless against Nginx, or against Apache 2.2.33, or against Apache 2.2.34 built with different compiler flags, or...
A fully virtualized system that emulates all aspects of the CPU should be immune to Spectre, but full virtualization is slow.  Hardware-assisted virtualization may be vulnerable if branch-predictor training is shared between the VM and the host, while paravirtualization, sandboxing, and containers are completely vulnerable.
",8,2018-01-04 19:15:14Z,36
51,"
I don't think that there is any method supported directly by AWS to prevent users from logging into any account (that they have rights to).
AWS promotes the use of multiple accounts. With the recent announcement of AWS Single Sign-On it is even easier to use multiple accounts.
Announcing AWS Single Sign-On (SSO)
",1,2017-12-10 02:47:16Z,112
52,"
Is HSM an option?

CloudHSM: AWS will try to sell you ""CloudHSM"" for this: https://aws.amazon.com/cloudhsm/ -- I think you pretty much have to take their word (legally enforceable contract or something) for it when they say that it's an actually HSM on real hardware with no back door.
Keyless SSL: CloudFlare offers something called ""Keyless SSL"" where you host the HSM in house. https://www.cloudflare.com/ssl/keyless-ssl/


I know that I still have to believe that the authority chain is not compromised (vs PGP/distributed processes).

If your server box is taken over (either by physical access or otherwise) then the hackers also have your SSL private keys. Unless they are in a Hardware Security Module (HSM). From which they are hard/impossible to extract. (Even with physical access, a large budget and lots of time.)
",1,2017-11-15 17:42:34Z,114
53,"
This is a fairly horrible entry in the combined audit log and Microsoft certainly don't make it easy to decipher.
It appears to happen when a users device with an on-prem AD account sync'd to Azure AD tries to authenticate.
From what I can tell, the IP address should be related to the client device - which may itself not be on-prem at the time of authentication.
There are a couple of support articles related. As often the case with MS web support, they are fairly inpenetrable. 1, 2
",2,2017-11-12 16:02:28Z,116
54,"
Depending on what you are thinking of, it could be titled in one of the following ways:
Intellectual Property Policy
As the work done by employees is likely to be classified as a ""trade secret"", or ""intellectual property"" of the company, employees should be able to:

Recognize what IP belongs to the company.
Understand that IP of the company shouldn't be shared unless it's required (ex. manufacturing).
Respect the IP of other companies and holders.

See: https://wustl.edu/about/compliance-policies/intellectual-property-research-policies/intellectual-property/
See: https://www.uspto.gov/intellectual-property-ip-policy
Copyright and Intellectual Property Policy
Very much of the same thing, but extending further into the realm of copyrighted works, such as books, papers, magazines, video, and so on.
See: https://policies.yahoo.com/us/en/yahoo/ip/index.htm
Operational Data Policy
This might be what you are looking for, as data used for day to day operations could be classified as a ""code base"" or repository, as well as data gathered from clients.
See: https://web.archive.org/web/20160705201428/http://www.rcmvs.org/documentos/IOM_EMM/v3/V3S10_CM.pdf
Data Retention Policy
This focuses on data gathered from clients, and what is done with it. Many implement this as a ""Privacy Policy"". This can be found on any reputable website, or those required to have one by law.
Acceptable Usage Policy
I figured I should add this one in, as it is the over arching name for such a policy. It should lay out how corporate computing and information resources should be used by employees, and who is authorized, and who is not. There is an extensive list and examples online for this type of policy.
",1,2017-10-08 22:16:34Z,117
55,"
I'm going to have to disagree with the other responders. There are a variety of differences that could theoretically matter. Note that none of these are guaranteed, just that they COULD.

Your adversary is probably also able to use the cloud and scale up the attack in a much more cost-effective manner.
Some advanced adversaries could attempt something along the lines of a side channel attack. For example, if you are known to operate in a specific region or datacenter, they could attempt to choke out your available resources.
As a DDOS victim in the cloud, you could end up experiencing the pain of the attack in two different ways: one, unavailability, but the second, increased cost of resources to handle the attack.
As a DDOS victim in the cloud, you may be able to better cope with an attempted DDOS, because you may have easier access to resources that can scale with the attack. For example, you could more easily scale up your infrastructure, you are likely to have greater bandwidth, and you can more easily add new security ""devices"" into your traffic flows.

On the other hand, detection varies little. There's no silver bullet, no automatic answer. It's an active area for academic research. However, because you don't typically don't operate the full stack of your network infrastructure in the cloud, the cloud service provider has an interest in helping you deal with these threats. They may provide tools, documentation, or support to a bit to help with detection. Consult your provider, but as one example, AWS has some suggestions on how to use its tools for DDOS mitigation here: https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/
",1,2017-09-11 18:03:02Z,119
55,"
As @SmokeDispenser commented, there is no difference between your own infrastructure and a cloud service; the only difference is who owns it.
Many cloud service providers will attempt to prevent DDoS attacks, or at least let you know when your site is experiencing one.
",0,2017-09-11 15:51:38Z,120
56,"
With my security hat on, I think your optimal solution is to use #1. The inconvenience factor is certainly there, but 2 and 3 create a single point of failure - all environments would be compromised if a single AWS authorized user's credentials were compromised.
",1,2017-08-23 20:07:51Z,119
57,"
The question initially asks about security benefits, but then opens out to ask for ""the better option"" generally, and why.
In this answer I'm looking a the broader question, ""which is best and why"", for 2 reasons: first, being realistic, you won't decide on security alone (cost? knowhow? time? strength of personal feelings for/against privacy? enjoyment of challenge? other non-security pros/cons/motivations), and second, because security can be considered within the broader picture anyway. 
Framing the question
Giving a definitive answer to the question as posed isn't always easy, because the decision almost always involves personal preferences (you might not be at much risk of intrusive data mining but still have very strong feelings about avoiding it if practical), and incomplete information (the actual risks which an infosec specialist could assess better than you, including your own ability to manage data if you keep it locally and the data safety of the proposed commercial host). 
Fortunately I suspect that's largely a non-issue for your needs and this question, because the question suggests that you aren't already a data security specialist. So any answer has to fit an ""informed enthusiast looking for their best options"" or similar.  That means any answers where you magically make it all secure from scratch probably aren't suitable, which means we can look at the question from another angle that makes it much simpler.
Answer
From your question, the first definite point to discuss is the possible use of prebuilt systems (such as open source distributions that include cloud storage as part of an ""all-in-one""/""out-of-the-box"" general storage server). In these systems you don't just install a cloud storage package onto your OS. You install a complete customised and dedicated file server, which also provides cloud storage as a  feature.  You install from CD/USB, use the GUI/web to configure, and it's done. 
This can be a very helpful source of assurance, because if its a well known project and has a reasonable security focus, then you can count on their own work (and their community of contributors) to cover a huge amount of the security unknown. 
What I mean is, I wouldn't trust myself to set up FreeBSD, install NextCloud/OwnCloud/whatever, and trust it for security, but I probably would be comfortable downloading the FreeBSD-based installer for FreeNAS,  configuring it in the GUI for those tasks, enabling the NextCloud/OwnCloud plugin, and doing it that way, because the scope for my own ignorance and knowhow to wreak havoc is quite constrained, purely because the options I configure are preset in the GUI, they are functional, the important settings are already done, and it's mostly very clear to understand (or ask about if unsure). I'd still need to know some things, but much less and much more obvious. In fact that's exactly what I have done, minus cloud storage which I don't use.
So I'm going to assume that this is how you'd approach it if you did it yourself.
That means the real comparison isn't Google Cloud/Dropbox vs. do-it-yourself NextCloud/OwnCloud. It's Google Cloud/Dropbox vs. FreeNAS/NAS4free/Openfiler/Nextenta/OpenMediaVault etc that also supports NextCloud/OwnCloud - which has been properly configured and secured within a jail - and that's a very different proposition. It's much easier to be sure of the situation and security you'll end up with (the 'downside risk'). 
If the underlying system is good, the team building it is security aware, and you are comfortable with the basic config likely to be needed, then you will probably get a good level of security by any usual standard and can choose the final decision based on personal antipathy to commercial cloud storage and cost, which sounds like what you're after.
As an example, if you look at a similar decision for routers and firewalls, I switched many years ago from a commercial router to pfSense. Factors relevant to the decision included:

Cost (couldn't afford a router with the connections handling capacity I would need), 
Physical access security (short of Secret Service/NSA breaking into my home, nobody is in a position to access or interfere with my computers through physical access except me. If you have hacky kids, housemates or visitors, consider the security of your data disks, backup disks (including where you keep them), physical keyloggers, FireWire/other accessible DMA devices and ports, or whatever)
Non-physical security (based on FreeBSD which is probably a lot more secure than most Linux or Windows setups, compared to most domestic routers which stop getting updates after a while or are just insecure by design or (very often) have huge security holes due to bad OS config, very popular/large community/serious use, speedy responses to security issues, team  appears to have a firm eye on security, has published security audits of the software by third party assessors which showed few issues, suggesting the team knows its job, enterprise security features if I ever want to get heavier on security), 
Future-proofing (likely to have ongoing updates and active development for many years, or be forked if not, large user base, extensible and adaptable for almost anything I might need/want from it in future, even on my old Pentium 4 boards from 2003 which I have no other use for, I can get performance of hundreds of thousands of connections, caching, filtering, etc.), 
Certainty of code/privacy (well-known open source, backdoors unlikely and guaranteed killed if ever found (including backdoors claimed to be ""for support, honestly, bad guys won't find them!"" as some products secretly have) runs on own hardware), 
Easy to manage (very straightforward GUI after a bit of learning curve, any issues likely to be able to be addressed without buying another router), 
and for the file server I later added, guaranteed data storing (some cloud providers suddenly failed or lost data when hacked, while not common you can't know what is happening ""behind the scenes"" and how dependable their setup and disaster recovery will be in reality, as a security specialist would assess it, because you just don't get access to that sort of knowledge). 

I've used this platform for my router for many years now, and rarely need to do anything to maintain it apart from click when there's an update, and runs for months without issues. I've been using FreeNAS for data storage for a few months so far and it's been rock solid, any issues have been down to properly configuring the shares, so I expect much the same benefits from it.  It just keeps going, and once I get to a config I like, then I suspect I will barely need to know its there after that point.
The main downside in that equation is cost. Say you have 4TB of data (or whatever)... you might be committing yourself to a surprising hardware cost if you went that route:

Up to 4-6 x the amount of storage capacity compared to the actual data you expect to store, meaning for 4TB data you'd want up to 16-24TB of disks (file systems and especially the very reliable ones like ZFS used in many storage setups, might not want to be over about 60-80% full, disks fail so you need redundancy, and you do plan to keep a backup as well don't you?), 
A good quality motherboard probably with ECC/RDIMM memory (platform stability, probably means using a server not consumer board for the ECC), 
Lots of RAM (ZFS is hungry that way!), 
At least one, maybe more decent SSDs (mainly for write cache but if affordable and needed then also for read cache),
Good quality power supply and probably a good brand secondhand UPS (so that power glitches don't trash the data), 
Probably at minimum a second smaller, less well specified box for nightly backups/clones (replication, in case the entire primary box goes bang, so you don't lose it all). 
Periodic replacement costs as equipment fails (each disk might need replacing every few years, so 4-6 disks might average 2+ a year if they're cheapest commodity type or about 1 a year for 5 year enterprise types)
Power (in some countries and locations, electricity is expensive and running a server with multiple drives will definitely add to the bill)

So cost is definitely an issue to consider when it comes to storage locally or in the cloud, because with a commercial provider you don't have to worry so much (usually!) whether their hardware and backup approaches are data-safe. Less so when it comes to say, a router where you can just restore config onto new hardware and carry on as normal if it all goes bang.
There may be workarounds if decent hardware cost is an issue. For example you might decide to run it on commodity or consumer hardware, with no backup or redundancy other than using two disks configured as a mirror. That would slash the cost a lot. You'd then set a task to copy your modified or important data into an encrypted disk system locally (truecrypt/veracrypt etc) and upload that to Google/Dropbox instead, or keep a copy on a third disk which you put safely on a shelf and update weekly. That might be a possible solution to the cost issue, but as with all compromises you'll have to consider if it works and the pros and cons for you. 
As a guide and if it helps, I'm doing it all locally and its a lot of hardware - the primary server and UPS probably contains 8 x £250 ($300+?) of enterprise data HDDs, another £500 SSDs, and about 96GB of ECC ram for another £500 - and that's before the CPU, motherboard, PSU, UPS, and backup server. I've saved a bit with two 'tricks':  using a decent commodity brand for the backup (less worried about it once there's multiple copies of the data), and by having it do ""double duty"" (it acts as a router and service host when I'm awake during daytime, and when  asleep and the network is quiet at night it grabs a copy of the current file server contents acting as a backup server)  But that's my situation, yours may differ.
",5,2017-08-16 11:18:47Z,101
57,"
That is clearly a trust question. So the answer will be as usual another question: what are the threats you considere, and what are the risks?.
If you just want to protect your personal data and are not a publicly world known person, commercial solutions are an appropriate option.
If you want to protect commercial sensitive, or industrial R&D data, the attackers could be external companies or foreign intelligence services. In that case, you cannot really trust any external company and should use a private cloud on your own servers, or on a partner's servers. For example, you may trust a cloud controlled by your own government more than a foreigh one.
For what I have understood from your question, I would not make much difference between a Google Drive or DropBox solution and an OwnCloud or NextCloud hosted by a external company: in both case, the local admin can get full control on the data if he really wants to. The difference will just be on the following of security advisories: if you can read them everyday and pass the patches  immediately, you will be better than commercial integrated solutions, if you only do that once a month, you will be worse... But here you only considere attackers that cannot get direct access to the servers.
",3,2017-08-16 08:48:14Z,83
57,"
The public cloud like OneDrive or Google drive seems to be reasonable option when all data are stored in encrypted form. the duplicity/duplicati/deja-dup tool ( same storage format, just different implementations for different OS-es ) offer gpg encryption before sending anything to remote storage.
So you hold all your data on local computer and in addition, a daily backup is performed to remote cloud.
the drawbacks are 

cost for cloud provider if you need to store large amount of data
stable internet connectivity required for daily backup session
network bandwidth, especially upload capacity ( you have to transfer full backup once a month, and incremental backups every day )
remote storage excess ( depending how much full backups you want to keep in cloud and how often/how much % of your data set is modified ) it may be even few times more space required than original data takes

for local storage, a RAID-5 array may provide reasonable balance between protection level and cost of extra disks for mirroring.
together with remote cloud, you're already well protected! For especially critical data you could have third copy on removable USB-attached encrypted disk stored in different building/town.
",-1,2018-01-30 15:29:51Z,123
58,"

... or am I arguing an authentication technology that was never prepared for Cloud driven services?

SPF is not really an authentication technology. It does not authenticate who is sending the mail but only limits from which IP addresses the mail can be sent from and thus tries to limit misuse of your domain as the claimed sender of spam and phishing.

If they gave me companyx.amazonses.com I would be a tad more interested in including this.

There is no reason to believe that this would be more safe just because the company name is included in the domain name. The real question is which IP addresses are covered by the SPF (i.e. allowed to send mail for this domain) and one might simply set this ""company record"" to be the same as the global amazonaws record.
If you use the cloud to send mail you are usually doing this to profit from its advantages, i.e. flexibility, scalability, fault tolerance, low latency due to geographic routing etc. But, to implement these advantages it might be necessary to have lots of IP addresses and be flexible in which tasks are assigned to which IP address.
This means that the IP address might be an overly broad trust anchor when using cloud infrastructure. Instead or additionally you should rely on system specific instead IP specific restrictions. This can be done using DKIM where the outgoing mail system signs the mail and the recipient can check that it was actually signed by the mail system belonging to the sender. Both of DKIM and SPF can be combined with DMARC for even better protection against misusing your domain name for spamming and phishing.
In short: don't complain that SPF is insufficient and not designed for the cloud.  Instead use DKIM and DMARC in addition to SPF.
",1,2017-08-16 05:29:35Z,6
58,"
I agree with you. An overly broad SPF record is a much bigger risk than, as you say, companyx.amazonses.com
This email service company appears to be putting all their clients at risk by recommending this. I would look for another service provider.
",0,2017-08-16 02:38:40Z,125
59,"
About password complexity:
According to my experience and security standards such as SANS and OWASP, limiting password length to 12 characters is unusual. These standards say that it is better to support enough length, even allow them to set pass-phrases to make their Accounts more secure.
good lock.  
",2,2017-05-02 09:47:33Z,127
60,"

... but they used their own infrastructure

It's not really their own infrastructure what they use. They use instead botnets consisting of  hijacked systems. These are systems which they p0wn but definitely not own. And thus it is very cheap for them.
Apart from that any VPS provider who would rent their VPS for DDoS attacks would quickly lose reputation and thus proper customers. And if a VPS provider then specializes on providing VPS for DDoS attacks to make up for the loss of normal customers it would be more easy to block such DDoS because they all origin from the same networks, i.e. simply cut off this provider from having access to major networks.
",40,2017-04-25 04:54:20Z,6
60,"
Cloud based DoS attacks are possible, and they do happen from time to time. But it's not a very popular option for a couple of reasons:

Initial setup - Deploying hundreds of VMs is not an easy feat, and paying for them isn't simple either. However if you're using someone else's VM, then this makes things a lot easier. 
Detection - Many providers including Azure monitor their services to check for any malicious activity. In fact, launching attacks from their systems violates their ToS, and will have you shut down very quickly.

However, the ability to have thousands of machines spread over the world, each generating some traffic can be very powerful. If you want to take it a step further, tunnel your traffic through the Tor network, to make it nearly impossible for a defender to stop. 
It's been done before though:

In 2012 group of cyber-criminals exploite  the CVE-2014-3120 Elasticsearch 1.1.x vulnerability, followed by the use of Linux DDoS Trojan Mayday and with that, they compromised several Amazon EC2 Virtual Machines. Although this vulnerability was not unique to cloud-based systems and could have been used against any server, including non-cloud based systems, it did open up some interesting opportunities to the attackers. They were able to launch a UDP based DDoS attack from the compromised cloud instances. They utilized the outbound bandwidth of the Cloud Service Provider, Amazon in this case. 
  Source: Infosec Institute

Getting caught - this is more difficult. Creating and deploying VMs these days is as easy as signing up for an anonymous email ID, registering and deploying machines. However, providers will notice large amounts of traffic from a system. Since you're​ violating their ToS, they will nearly always shut you down immediately. However since you're not ever revealing your actual identity (assuming you're accessing their services through an anonymizer and using stolen credit cards (no morals ;] )), they mostly will not be able to discover your real identity. But this does mean that you're flushing your money down the drain - which is why it's just simpler to set up your own infrastructure and offer it as a service. 
",9,2017-04-25 04:48:58Z,129
60,"
The funny thing is that what you describe is available right now. It's called Mirai, it's more or less open source, and chances are you've already been affected by it

Mirai is a type of malware that automatically finds Internet of Things devices to infect and conscripts them into a botnet—a group of computing devices that can be centrally controlled. From there this IoT army can be used to mount distributed denial of service (DDoS) attacks in which a firehose of junk traffic floods a target’s servers with malicious traffic. In just the past few weeks, Mirai disrupted internet service for more than 900,000 Deutsche Telekom customers in Germany, and infected almost 2,400 TalkTalk routers in the UK. This week, researchers published evidence that 80 models of Sony cameras are vulnerable to a Mirai takeover.
These attacks have been enabled both by the massive army of modems and webcams under Mirai’s control, and the fact that a hacker known as “Anna-senpai” elected to open-source its code in September. While there’s nothing particularly novel about Mirai’s software, it has proven itself to be remarkably flexible and adaptable. As a result, hackers can develop different strains of Mirai that can take over new vulnerable IoT devices and increase the population (and compute power) Mirai botnets can draw on.

There's lots and lots of IoT devices flooding the market. Everyone wants ""smart"" technology. But, as is usually the case, security is an afterthought. So we put that device out there and it's on the Internet for anyone to contact and use as they see fit. And most users (and indeed ISPs) probably won't notice 

Fast forward another 45 minutes. The router was reset, and the network was set up again. By the time I was done messing around, Peakhour had my traffic clocked at 470GB. But I'd gotten rid of the problem (or so I thought). The next morning, before I left for the weekend, I checked: the total traffic was at around 500GB. Maybe I'd defeated the hackers.
That night, I heard from Donna. She'd been monitoring traffic, which was now over 3TB. And, just to make sure we had no doubt, devices were dropped off the network again.

The tipoff that something was amiss? His phone used all of it's 4G allotment despite his being at home. His ISP never batted an eyelash at 3TB of bandwidth consumed

Ultimately, you can count on people who are technically illiterate and companies that don't care to provide all the botnet devices you'll ever need.
",2,2017-04-25 16:38:18Z,49
60,"
Cloud providers generally require their customer's identity. If an enterprising young hacker wished to rent Amazon Web Services or the like, they would have to provide a credit card number (or more) to the service, which can be traced back to the owner. Cloud services don't want to engage in DDOS because their networks would be blocked, and it would cost them money in bandwidth. 
There are services where you can rent a VPS anonymously in Bitcoin, but they are generally smaller and they also don't want to be blocked by their uplink or peers. 
So that is why it isn't common. DDOS generally considered anti-social behavior, and sociopaths only make up 4% of the population.
",2,2017-04-25 17:11:10Z,130
60,"
The key to your question is in the first 'D'. 
What makes a DDoS attack so effective is the distributed nature of that
attack. With an old style DoS attack, the victim would usually experience a
large number of requests or connections to a specific server or resource
originating from a single or small number of sources. To mitigate the attack,
you could simply block the traffic from the attacking systems. Often, this could
be done by the local firewall or similar. 
Under the DDoS attack, the victim is flooded with requests from a large number
of different sources. The number is too high to block individually and the
volume of attackers will typically overload all local infrastructure such as
firewalls and network switches. In this scenario, you typically need to work
with your ISP to have traffic to the target system sent to a 'black hole', which
will reduce the volume and allow local infrastructure to recover, but typically
does mean that the DDoS on the specific system is successful (because traffic to
that system is being sent to the black hole). The key point is that because the
attack is distributed, it is very difficult to block the attacking systems. 
So, with respect to your question regarding cloud based DDoS services - this to
some extent depends on your definition of cloud. One definition would be any
service that is not on your own infrastructure and is delivered from 'the
cloud'. In this sense, DDoS attacks are already cloud based. They don't use the
attackers own infrastructure, but instead, use hosts the attacker has either
compromised or hosts the attacker has identified which have either poorly
configured services or lack sufficient controls to prevent them from being used
as part of a DDoS attack. For example, one of the reasons there is so much
concern surrounding IoT is that many of these IoT devices include services which
can be exploited as part of a DDoS attack and lack sufficient controls to
prevent this exploitation by unknown remote uses. 
If you define cloud to be just IaaS, PaaS and SaaS providers, the situation is
slightly different. You are unlikely to see these services being used to perform
the actual attack simply because the DDoS attack relies on high numbers of
attackers and being able to use that number of cloud providers is prohibitive -
remember that the cloud providers are not going to welcome this sort of use of
their infrastructure, so you will have to do it in a 'stealthy' manner, which is
becoming increasingly difficult as cloud providers lock down what is considered
appropriate use of their infrastructure (remember, they have a reputation to
maintain - if they become known as a host for 'bad actors', ISPs and others will
just block traffic from their IPs). 
This doesn't mean attackers don't use cloud services. What  you will often find
is that DDoS service providers will use cloud services as the command and
control centre for their DDoS agents/bots. They still need to do this in a
stealthy manner as most reputable cloud services will deactivate any users they
detect doing such things, but this is much harder to detect and they only need a
few cloud providers. The agents/bots they use to actually perform the attacks
are usually compromised desktops and servers, often in home systems which have
poorer security controls and increasingly IoT devices, many of which are also in
home or small office environments which lack enterprise security measures or
skilled system administrators etc. 
",0,2017-04-28 01:22:35Z,131
61,"
You are a data custodian of your customer's data. The fact that you are using Azure means they are also a partner with you in data custodianship. For you to accurately describe what that means you either need to 

Include all the terms of the Azure privacy agreement into your agreement, understanding that these can change rapidly without notice. 
or 
You need to clarify that your policy is X and that your service runs on Azure who's policy is Y. 

If it's an option the 2nd option will be the easiest way to resolve the problem with Azure's ever-changing privacy terms.
Do keep in mind that Azure's policy may also vary by jurisdiction and that you need to keep track of that and also which jurisdiction's laws apply to each of your customers. 
Useful Reference: https://azure.microsoft.com/en-us/support/legal/
",1,2017-04-19 02:33:41Z,133
62,"
Sounds like a job for Azure AD Connect, then your app can authenticate against Azure AD regardless of where the user is (interal/external to the company's lan).  As long as they can reach your app they'd be able to log in with their credentials.
https://docs.microsoft.com/en-us/azure/active-directory/connect/active-directory-aadconnect
",0,2017-04-13 18:23:37Z,135
62,"
If your users are using company supplied (domain joined) laptops when they are on the road, one option to consider besides Azure AD is Direct Access. This solution will provide exactly the same user convenience as if they were inside your network while maintaining minimal impact on your architecture (you will not be publishing the app to the Internet directly). A thing to consider with this solution is it would only work for domain joined computers. Any other device including phones and tablets will not be able to access the application. 
",0,2017-04-14 07:46:10Z,136
63,"
It is impossible to guarantee security in the cloud from the cloud provider itself, the cloud provider can (be forced to) take a memory dump of any running VM instance and extract any encryption key from a running instance. It is even possible to continuously and automatically monitor a running VM and extract AES keys as soon as they are used, especially when the VM is using AES-NI.
A truly secure solution would avoid decrypting, let alone using, the keys on the server.
So do the encryption client-side and use the server purely as blob storage.
Fortunately you can do quite a bit of crypto in the browser these days, that includes KDF and AES.
For KDF you can use e.g. scrypt-js, and for AES aes-js or aes-es.
",1,2018-06-09 08:40:13Z,138
63,"
Don't think this is possible with symmetric keys if you expect the server to accept clear-text data and then encrypt it;  only keeping the encryption key in memory during the session isn't really a protection of the data, as (for a relatively busy service) that key will be in memory, thus the data vulnerable to decryption, all the working day.
It could be a valid defense against someone intercepting your backups, or trying to steal data direct from disk; but not against hacks of an active service.
It might work if the encryption happens at the client, and all you ever receive in the SaaS cloud is ciphertext - but then the cloud wouldn't be able to do any processing of that data.
",0,2018-01-09 23:00:00Z,139
64,"
You are not off base on your assessment. Putting PHI in the cloud means that that provider is now responsible for compliance. They now play an important part in the chain of custody of the PHI.
BAAs are required in as much as if you use a cloud provider and don't have an air-tight BAA, you're on the hook for the risk. Think of the chain this way:
Hospital (Covered Entity) <-> You serving the hospital (BA) <-> AWS
You likely have a BAA with the hospital where you took on all the technical risk of handling the hospital's PHI. But AWS is a critical part in the chain that is handling the PHI. If you don't have a BAA with AWS, you are not handing off the compliance risk to the party who is actually responsible. For example, if for some ungodly reason a major data center was broken into and hard drives are stolen, YOU are on the hook if you don't have a BAA.
However, note that all public clouds only cover a very small fraction of compliance in totality. The translation is ""chain of custody"" to ""Shared Responsibility Model"". Here is AWS's: https://aws.amazon.com/compliance/shared-responsibility-model/
So, for example with AWS, they only cover about 1/10th (physical safegaurds, firewall). Their BAA doesn't take on other controls like encryption or logging or disaster recovery.
If you want use the cloud, you should investigate a company like Datica, which I work for. Companies like that who sit on top of public clouds will maintain and take on the risk of the other 9/10ths so to speak. This resource you might find valuable to explain the concept.
",1,2017-08-29 14:46:51Z,141
65,"
If you have ever uploaded data to someone else's servers, then you have lost control over it. You have no guarantee that if you ask them to delete it that they actually have (or have cleared it from all replicas and backups).
If you do not trust your storage provider, the standard solution is to only ever upload encrypted data (for which they don't possess the decryption key). You can do this yourself by running files through your file encryption program of choice, or use a provider that naturally provides this.
",3,2017-02-26 05:05:25Z,53
66,"
It uses SSL: https://github.com/GoogleCloudPlatform/cloudsql-proxy/blob/master/proxy/proxy/client.go
It uses Google Cloud SQL APIs for managing SSL (https://cloud.google.com/sql/docs/mysql/configure-ssl-instance, https://github.com/GoogleCloudPlatform/cloudsql-proxy/blob/master/proxy/proxy/dial.go).
",4,2017-07-08 22:22:55Z,144
67,"
Disclosure: I'm a developer of CryptPad.
The answer to your question is based on how much assurance you need. The idea of CryptPad is to be verifiably ethical, we can prove that we are not doing mass data collection because if we were, somebody would catch us eventually. Therefore we are verifiably better than almost every other service available because with CryptPad, you are not blindly uploading your data and trusting us never to go in the database and read it.
We cannot (at this point) prove that we are not going to leak your data, if for some reason we were very interested in you, we could conceivably take the risk of being caught serving evil javascript in order to get access to your data.
tl;dr it is better than virtually any cloud server available but not better than hosting on your own infrastructure. However if you do choose to host something on your own infrastructure, you may still want to use a Zero Knowledge solution such as CryptPad unless you really trust your system administrators.
",2,2017-03-07 11:13:42Z,146
67,"
If you install CryptPad for yourself it is probably safe. If you use an existing public service you have no control over the Javascript which is used to handle the unencrypted data in the local browser which means that you have to fully trust the providers of this service that they have no malicious intent and that nobody is able to hack their server and change the Javascript.
",1,2017-01-14 18:53:18Z,6
68,"
Specifically for the German one, Microsoft is an American company, therefore subject to American laws. Among many other things, such laws include the possibility to request information from an American company stored on their systems, whether or not those systems are on American soil.  
I'll not go into all the details, suffice it to say that the way such information is requested and retrieved (e.g. without judicial oversight or informing the person the data concerns) often clashes with European privacy laws. Within the EU, Germany has some of the toughest privacy laws. 
So the idea is that although it's still full Azure with everything you might expect, in the background it is not Microsoft operating the datacenter but the local German partner which means the data in the data center is not subject American laws. 
I imagine the same applies to other regions as well, China has data localization laws for example. 
For a US person, it might be useful to use one of these data center if you are processing personal data of EU residents, to ensure you comply with EU privacy laws or simply inspire trust towards your EU clients. 
From a privacy law perspective, you should indeed assume the lowest common denominator.
Opting in or out of a given data center: business or legal requirements I would say. 
",2,2017-06-07 04:49:48Z,148
69,"
It is unlikely, but not impossible.
As previously mentioned, cloud hosting services use hypervisors to isolate VMs from each other (if there is more than one) and to isolate the VMs from the underlying OS and hardware.
Hypervisors, however, are not immune to vulnerabilities. Here is an example of vulnerabilities affecting Xen (a popular hypervisor). Not all vulnerabilities allows an attacker to break out from the hypervisor, but it happens.
Let us consider an attacker that successfully escalated its privileges to hypervisor level. In addition, the attacker needs to find vulnerabilities to insert a backdoor/implant in the firmware itself. Again, this is unlikely, but not impossible. It is possible that the firmware integrity is not verified at boot time or during an update. It is also possible that the firmware vendor did not respect the specification or that there is a vulnerability. 
The combination of successfully finding and exploiting vulnerabilities in a hypervisor and firmware is low, but not impossible.
Knowing if such risk is worth considering depends on your threat model.
For example, it is worth considering in the case of a highly motivated and resourceful attacker.
One would still argue that it would be simpler for the attacker to use another vector. For example, a phishing campaign targeting the cloud provider employees?
",2,2017-04-13 11:29:17Z,149
69,"
Pretty unlikely in reality since the majority if not all of these services won't actually give you access to the firmware.
",1,2017-01-03 17:13:45Z,116
70,"
The cardholder data environment (CDE) is limited to the network segments storing, processing, transmitting cardholder data. Your in-scope system components are beyond this CDE as systems providing security or other services are in scope for validation - i.e. authentication, update/patch management, orchestration, anti-virus, logging, FIM.
So this means your SIEM solution needs to be validated. The system should do at least the following:

Capture the appropriate logs 
Logs should be reviewed daily  
The system should generate alerts 
File Integrity Monitoring should be in place to protect the integrity of the logs 
Log rotation/retention policies should be in place and enforced 
Users of the system should be managed and monitored appropriately 
The system should be maintained up to date

Either you or the cloud service provider must evidence all the above or your cloud service provider can provide evidence to show the above has already been validated - i.e. by providing an Attestation of Compliance for the scope of the service provided.
",1,2016-12-08 14:36:20Z,110
71,"
Yes, this secures your data from Google, sync.com and from anyone trying to grab your data while it is in transit to Google.
Three things to think about:

Your lastpass password is far shorter and contains (the way it sounds) far less entropy than the one that protects your gpg tar file. Someone attacking this scheme would therefore most likely attack your lastpass account (since now it's public knowledge that your password there is weaker).
It sounds like you're using GPG in symmetric encryption mode (more on an alternative at the end of my answer). So: What's to stop a disgrunted lastpass employee with a bit of technical knowledge to steal your gpg password and sell it to an interested party at Google? Or if you find that problem too theoretical, what about an attack on the lastpass service which steals millions of user passwords and publishes them? Since you've conveniently added a note about where the encrypted files are located, you're basically handing your encrypted files over to anyone who can access your Lastpass account (well, in theory, anyway - he or she still has to get your Google Drive password...)
You're splitting your file into multiple files. Why exactly are you doing this? If you lose just one, you're running the risk of losing all your data. Say you lose file 20 (out of 200): You've lost all the content that was stored in files 20-200 forever. Keeping everything in one big file might make up- and downloading a bit more difficult, but it reduces the risk that you lose one of the files.

A few suggestions
Point 3 is also a problem with a huge file that accidentally gets a single bit flipped in transit. I'd think about redundancy a bit. There are ways to add error correction without having to double the size of the data. (I think some compression programs support it natively, even though you can't expect them to actually compress your .tar.gpg file...). Confidentiality is all well and good, but your scheme is a backup scheme, so you should also think about availability.
I would also suggest not storing your GPG password on an external service (Lastpass). If you're afraid of losing the 100 character password, write it down on a physical piece of paper and put it in a safety deposit box of a bank you trust.
Also, this might seem paranoid, but I wouldn't use the same passphrase over and over to encrypt your gpg files. Instead, I'd genereate a file containing a number of truly random bytes whenever I needed to upload a new backup and use that as a key for my gpg'd tar file. I'd then encrypt this random file with my 100 character passphrase and keep it on my computer, send it along to be stored at google / sync and maybe back it up to an additional location. (Basically, I'd create a session key with which to encrypt my data and use the 100 character string as a master key to decrypt the session keys). You can automate this fairly painlessly, and it gives you forward secrecy in case one of your backup session keys is compromised and the ability to decrypt any specific backup without losing control over your master key.
Finally, if you're really just using Google Drive as a backup and don't really need to decrypt the data except in emergencies, you could think about using gpg in it's public key cryptography mode. Make a backup key pair, and then keep your private key safely offline (in a bank deposit box, on a CD ROM, printed out on a piece of paper somewhere for safekeeping) while using the public key to encrypt your symmetric session keys (using public key cryptography to encrypt with gpg would make the manual session key step seem redundant - I'd still do it for better forward secrecy). So you'd never have to worry about your master key (private key) getting compromised by cyberattacks, even if you stored your public key at LastPass. Even if they stole it, all they could do with it is encrypt more data; they couldn't use it to decrypt your session keys.
",1,2016-11-27 09:56:06Z,152
71,"
Your system is fine (a 30 char password is adequate for a wallet). Noone is going to bruteforce your 100 char pw gpg archives and sell your data to google. I tried selling my own backups to google once :):):) Anyway, you need a physical copy of your backup as well, for safekeeping, on a safe medium, in a safe place, offline. I would use a RAID box for this - perhaps with LUKS/softraid if you have a spare PC, so you don't need to gpg everything all the time. SSDs don't need magnetic refresh every few years, but if they're too expensive use magnetic disks. Always keep several backups of important data. If one backup set is wiped out, use the other set.
",0,2016-11-27 10:58:39Z,153
72,"
IMSI catchers essentially simulate cell towers. Nearby phones will try to use these fake towers in order to provide connectivity. But while normal cell towers usually act in the interest of their users fake cell towers might in theory misuse protocol features or send malformed protocol data in order to trigger bugs in the phones modem. Given that the modem driver usually runs with kernel permissions in current mobile OS a bug in the modem firmware and driver might give the attacker kernel level privileges which effectively would allow to control the whole phone. 
With 4G hotspots the situation is not much different, only that the initial attack target would be the hotspot and not the phone. Once the attacker has taken over the hotspot it can intercept any connections and unless these connections are specifically protected with HTTPS or by using a VPN the attacker could also modify the traffic. But even if the phone protects itself well against such traffic MITM with a VPN or HTTPS it still leaves communication at the lower level, for example attacking the phone with malformed DHCP or attacking it through malformed Wifi. 
Note that all of these attacks require bugs in the phones and hotspots software and firmware. But specifically at the lower network layers (i.e. 4G modem, Wifi firmware, Bluetooth firmware, DHCP) attacks are less expected and thus the software/firmware is usually not designed to be robust against attacks. And 4G hotspots usually have a similar shoddy security as known from cheap (and sometimes expensive) WiFi routers.
To summarize: using a 4G hotspot will make it harder for an attacker since there are more protections to bypass. But, if there are vulnerabilities at the various stages then attacking the phone connected to the hotspot can be done too. Note that most of the attacks would need a very knowledgeable attacker though which makes it less likely to get attacked unless specifically targeted. And if one is specifically targeted then the attacker might also choose a completely different path for attacks since this might be easier, so don't just try to protect the phone communication.
",1,2018-11-11 06:33:46Z,6
73,"
The YateBTS Wiki states the MBTS software supports Ciphering for A5/1 and A5/3 in the air interface, though not the A5/2 you have mentioned, perhaps this implimentaion would suit your needs?
MBTS features
",0,2018-11-04 14:51:44Z,156
74,"
I would say no.
To quote StackOverflow: 

Authentication is the process of ascertaining that somebody really is who he claims to be.

Only by posessing the phone I cannot be sure that you are who you claim to be.
",1,2018-10-11 12:53:45Z,158
75,"
Do you trust all of your client code? If yes, you can store session IDs in local storage, and make sure to submit them with every HTTP request. On your backend, simply have a table with Session ID hash as the primary key, and User ID as the foreign key, and an expiry time. Don't keep a list of deleted sessions, just delete the row (or mark it as deleted).
The risk with cookies is that they're sent with every HTTP request to your domain, even if it originated from a fake client. This is called CSRF. Unless you have some sort of CSRF protection, avoid cookies and use local storage.
Of course, if you can't trust all your client code, then you'll need another solution.
Also, if you're using HTTPS, then session IDs, either cookies or form submissions, will be encrypted.
",0,2018-10-10 18:51:42Z,160
75,"
Before going into the details I will say that both session cookies and JWTs work for your case and both are secure if implemented correctly. Personally I would go with JWTs if only because it's easier to get up-to-date information or ready-built solutions.
JWT
JWTs were really designed for stateless authorization in mind but you can still use them for sessions. You'll want to look in particularly at using an access/refresh token model where you keep track of active refresh tokens in your database.
As for encryption, there are two main implementation of the JWT standard, namely JWS (a signed token) and JWE (signed then encrypted token). What you want to keep in mind is that a signed token's signature already ensures the integrity of the token. You would only implement JWE if you are also passing sensitive information in the token that you want obscured from the client. However JWT by itself does not solve problems with man-in-the-middle attacks so you should remember to use SSL whenever transmitting the token.
Storage of the token will differ between your web app and mobile native app. For mobile apps you should store them in the OS's Keychain/Keystore (most likely through a wrapper) which is designed for such a purpose. Where to store JWTs on a browser on the other hand is still a rather controversial topic as storing in webstorage (sessionStorage/localStorage) is vulnerable to XSS-Attacks while storing inside a cookie is vulnerable to CSRF.
From what I can gather the general trend is to avoid webstorage due its larger attack surface, but to be honest I've seen examples of both methods. For single page applications you can also consider keeping the token in memory without persistent storage.
Session Cookies
Without knowing details of your mobile app it is hard to say but from my experience if you are using the CookieManager/NSHTTPCookieStorage mechanisms provided by Google/iOS there shouldn't be a problem of deleted cookies that you describe.
Storing cookies on the browser will require you to secure it against CSRF. For what protection you should use it will depend greatly on your specific server implementation and restrictions and I think you should check out the resources on OWASP or ask a more specific question.
",0,2018-10-12 07:39:40Z,161
76,"
It depends on the app, of course, but most (citation needed) major apps use HTTPS.
Facebook talks about this specifically:

... virtually all traffic to www.facebook.com and 80% of traffic to
  m.facebook.com uses a secure connection. Our native apps for Android
  and iOS have long used https as well.

It is still possible that the TLS connection could be broken by MITM, but there is a process called Certificate Pinning that helps to prevent that. Fewer apps have this turned on. In the quote from Facebook above, which was in 2013, they said:

We're actively testing pinning in Facebook mobile apps and plan to use
  it in browsers as well.

So, is it possible that your app credentials could be sniffed from the network? Yes. But many apps use TLS and Certificate Pinning to make this unlikely. You need to investigate each app to determine what they use.
",1,2018-10-08 16:01:39Z,8
77,"
(I assume that the service provides scanning of HTTP and HTTPS traffic)
If you are using a device via someone else's network you have very little privacy.
HTTPS is intended to address that (up to a point). However since you seem to have discovered this after the fact, it strongly suggests that Vodafone have access to configure your device / deploy certificates. 
You have no privacy from someone who controls the device you use.
Security? That's way too broad to answer here :)
",1,2018-09-27 12:37:22Z,58
78,"
Replying to an SMS message will not have infected your phone.
(I am assuming your phone is secure enough not to be infected by trivial means like displaying an SMS)
",1,2018-09-24 21:50:05Z,3
79,"
This may not be the answer you will be happy with but how about abstaining from having any undesirable data inside your phone in the first place and instead using the right tool for the job? 
According to Wikipedia:

The app records information about the device it is installed on, including its [...] IMEI, the phone's model and manufacturer, and the phone number. The app searches the phone for images, videos, audio recordings, and files [...]

So, instead of trying to tamper with this spyware in any way (which can get you in a much bigger trouble), simply don't do anything suspicious on this phone and let this app do its job. Prepare against it by not having any photos, videos, audios, file, etc., and instead use the right tool for the job. Use some other secure software/hardware to connect to internet, use encrypted email provider and do all of your communication through the computer where you can do communication safely, and store all of your files somehow in a safe place (encrypted, somewhere on computer or USB, etc). Pretend to be an obedient citizen and use the right tool for the job to do whatever it is you don't want your government to find out. 
Some people may wonder why bother having a phone in the first place (and FYI, I asked the same question under OP's question, for clarification). My answer is: 

to make phone calls (and have conversations which are not going to be considered by Chinese government suspicious, in case they are tracking that too)
to use it as a ""red herring"" - if police asks you to give them your phone you won't have to lie to them that you have no phone, or worry that they will find out that you tampered with app, or get in trouble if you don't have app, etc. You'll just confidently give them phone, with no ""illegal"" information on it, they will check it, and walk away. You may, actually, even have some ""red herring"" files: pictures of nature, shopping list (milk, eggs, etc.), etc., just so that they wouldn't suspect that you deliberately not using your phone for such purposes, and harass you farther. 

I mean, not long ago mobile phones didn't even have the ability to store pictures, videos, files, etc. 
Are you willing to put your life in danger simply because you want to have some files on your phone?
Tough times require tough decisions.
",335,2018-09-24 18:47:56Z,166
79,"
Get a phone which doesn't support Android apps.
Why are so many of the answers complex? And not just complex, fragile and suspicious and downright dangerous to the questioner? 
You want to use your phone to send messages and make calls, right? You don't want this app installed, right?
Say hello to your new phone:

Good luck getting an Android app running on this. 
It's probably not illegal to have an old phone.
",177,2018-09-26 11:47:30Z,167
79,"
This is a tricky one.  It goes without saying, but it's also a dangerous one.  Attempting to circumvent these restrictions and getting caught doing so will potentially cause a lot of legal trouble.  If they throw people in jail for refusing to install the app, I wouldn't want to figure out what they do to people circumventing the app restrictions.  It is especially relevant because even experts in tech security have gotten caught by their governments despite extensive safeguards (the founder of Silk Road is a great example and is now serving a life sentence).  Granted, evading this app is most likely a much less serious ""crime"", but the Chinese government isn't exactly known for lenience here.  So while I would like to answer your question, please don't take this as me suggesting that you actually do any of this.  I consider myself a tech-expert, but I still wouldn't do it.
Still, to answer your question, you have a few options.  I won't bother mentioning the ""Get a second phone"" option because you've already ruled that out.
1. Virtual Machine/Dual Boot
There are some options for ""dual booting"" android phones.  I don't have any examples to immediately link to (software suggestions are off topic here anyway) but there are options.  If you can get your phone to dual boot then you can install the tracking software on one ROM and then do all your personal stuff on the other.  You may need to put some basic information on the ROM with the tracking app installed just so you don't raise too many flags.
Of course there are still risks here: risks that they might reboot your phone and notice, risks that they might realize you have a completely different system installed next to the tracked one, and the simple risk that you would go out and about and forget to reboot into the ""tracked"" system, allowing a police officer to find and install the tracking app on your actual system.
2. App modification/interceptors
If this app creates enough bad press it is possible that anti-tracking apps or hacked versions of this app may start floating around that try to automatically protect you from it.  I would not expect there to be any general tools already available that would protect you from this, so this is something that would simply take lots of googling or (perhaps) requests to the right people.  This has a major downside that unless you are an expert at reverse engineering, there isn't much to do to make this happen.  It's also hard to estimate what the risks of detection are.  That will obviously vary wildly depending on the skill level of the person who put it together.
3. Server Spoofing
Depending on your level of technological know-how you might be able to put something together yourself (note: this is not for novices).  Based on what I know and my experience in this area, I'm going to try to summarize some details about what a server-spoofing measure might look like.  Again, I'm not summarizing this because I think you should do it, but because understanding how things like this operate can be generally informative and also help understand the risks there-in.
Built-in security
First, we need to understand how this spying app might secure itself.  From all information available so-far, the answer is ""it doesn't"".  This is a pretty simple conclusion to come to because the app communicates exclusively through http.  It is very easy to intercept http requests, either from the device itself (if your phone is rooted) or with network sniffing tools on a computer attached to the same network as the device.  Most likely it is also very possible to easily figure out how the app authenticates itself with the end-server and how the end-server authenticates itself with the app.  In all likelihood there is no authentication in either direction, which means that spoofing requests in either direction is trivially easy.  This might be hard to believe (given that a country like China sets aside lots of resources to invasive technology like this), but the reality is that if the people who developed this app wanted to secure it from outside tampering, using HTTPS for transit would be the very first step to perform.  It is cheap, easy, and very effective.  The lack of HTTPS means that it is very likely that there is no actual security in this ecosystem, which is a plus for anyone trying to evade it.
Sniff all traffic coming out of this app to determine what requests/responses it makes
This is the first step.  By watching the traffic leaving this app (which can be easily intercepted in the network itself since there is no SSL encryption) you can figure out what requests it sends to the destination server and what responses it expects back.  Understanding the underlying API is critical, but easy due to the lack of encryption.  This will also let you know if there is any authentication happening in either direction.  If there is, you can at least see the full request and responses, so you can most likely figure out how to spoof it.  It is possible that there is some hard-to-reverse-engineer authentication going back and forth, but again, given the lack of basic encryption, I doubt there is any such thing built in.
Figure out if the app is talking to a domain name or IP address
The destination server the app is talking to is either found via a DNS lookup or has its IP address hard-coded in the app.  In the event of the former you can edit the DNS for your android phone to repoint it to a different server, including one running on your phone.  In the event of a hard-coded IP address you will similarly have to redirect all traffic to that IP address to your local android phone (presumably you can do this with Android - you can with other operating systems, but you would definitely have to root your phone).
Setup a replacement server
You then setup a local server that responds to all requests just like the server did in your initial spoofing.  You would have to get this server to run on your phone itself, that way it is always available.  This doesn't necessarily have to be complicated (although that depends on how detailed the actual server interaction is), as you don't actually care about keeping any data on hand.  You just need to make sure that you provide valid responses to all requests.
Risks:

The app may auto-update itself (although your mock-server may make this impossible) and point to new domains/ip addresses, suddenly removing your protections
If there is an auto-update functionality and your end up unintentionally killing it (which would be good per point #1 above), a police officer may notice that it is not properly updated, flag you for ""extra"" checking, and discover what you are doing.
They may do server-side tracking and discover what you are doing because they don't find any data on their end for your particular IMEI (because your mock-server acts like a black-hole and sucks up everything).  Even if you send spoofed requests there will be easy ways for them to determine that (imagine the police copy a blacklisted image to your phone and discover that the app doesn't block/report it)
They may have root-checking in the app itself, which will cause you problems

Actually, that's it
I was trying for a longer list but that is really what it all boils down to.  Short of not carrying around a phone or purchasing a separate one, these are about your only options.  For reference, I haven't gone into details about the server spoofing because I think you're necessarily going to go out and do it.  If anything, I've gone through it because it gives opportunity to talk through the risks in more detail, and those should make it clear that there are a lot of risks.  Even if you find a solution from someone, they have to deal with all of these same risks (or ones like it).  Right now this app sounds like it is poorly executed and easily fooled, but depending on how much the Chinese government decides it cares, that could change very quickly.  At that point in time not getting caught basically turns into a cat-and-mouse game with the Chinese government, and that isn't realistically something that someone can continue to win for an extended period of time.  There are a lot of risks, so tread lightly.
",79,2018-09-24 18:55:17Z,168
79,"
They can execute code on your device while they have physical access to it. And you can't refuse it. I'm sorry to say that but you are basically doomed. There's no way to trust this device anymore. That's part of the 10 immutable laws of security. In your case the rules #1, #2, #3, #6 and #10 are applicable.
But when you act like you don't trust the device you could raise their suspicion. Maybe. Because nobody knows what they are actually doing with the collected data. Maybe nothing at all. In the ""best"" case it's primary for spreading FUD.
But when they are actually using the data it's easy for them to spot burner phones and all kind of tampering. As far as I know you only get a SIM card by identifying with your ID. Since the spyware reads identifying information like IMEI and IMSI they can simple compare the collected data from the phones with the purchase records. They can combine this with behavior tracking based on metadata collected on the phone (Which apps are used and how often, how long the screen is on etc.) and the mobile network (usage of data, location based on cell tower etc). Since they can do that on a large scale, they can spot strange usage patterns by your usage history or how a ""average"" user is behaving. Of course there's a vast amount of ambiguity and such in this data but they have the ultimate interpretational sovereignty.
You must also keep in mind that you need to keep your measures working all of the time because it could always happen that you get stopped on the street again.
I'd like to emphasize on rule #10. You are basically trying to solve a social problem with technology - just as your government does.
",54,2018-09-24 21:51:43Z,169
79,"
Use a custom ROM (two, to be correct).
Android phones can have more than one ROM installed, and you choose one or the other. So install two copies.
On the clean ROM you install the spyware, anything not dangerous, games, whatever you feel clean. On the secure ROM you install things you don't want anyone to know about.
Keep the clean ROM running almost all the time, specially when you are out of home. Boot on the secure ROM only when you really need.
You will need to keep a secure mindset too, to have awareness of what ROM you are using and what content you can create or access. That is the main point of failure. Using a different keyboard on each ROM, or different OS languages can help: Chinese on the clean, English on the secure, for example.
But first you must weight the risk/reward of doing so. If the risk of getting caught plus the mental effort to keep activities and files containerized is worth the benefits of bypassing the spyware, do it. Don't do otherwise.
",28,2018-09-24 20:42:44Z,5
79,"
I'd recommend you just go with it. The Chinese police doesn't just stop any random person in the street and asks for their phone. They stop Uyghur.
This happens for reasons which are somewhere in between ""mitigate a real threat"" and ""Woah, no go, dude"", but whatever it is, it's what the government does, so it's legal and ""right"". No benefit of doubt, and no assumption of innocence, no Sir. By Western standards, it's kind of unthinkable, but you cannot draw to the same standards there.
So the situation is that you are easily identified as Uyghur, both from looking at your face, and from the fact that police knows. They know who you are and where you live. And sure they know whether you've been stopped before. Again, you're not being stopped at random. You're stopped because you are already a well-identified target, on their screen.
It isn't even unreasonable to expect that your Internet traffic is monitored (targetted) and even asking about how to circumvent the measures may move your name onto a different, more high priority list.
You can bet that police keeps a list of people where the spyware has been installed (with device IDs), too. If no data comes in from your device, well, guess what. You'll be stopped again by police, and they will look very carefully why this isn't working.
Insofar, it is kind of unwise to try and circumvent (and risking being caught) what police wants. From their point of view, you are a possible criminal, and a possible terrorist. By trying to circumvent the measures you prove that you are a criminal.
The surveillance happens on the base that if you have nothing to hide, then you need not bother if they're watching you. Again, by Western standards, this stance would in no way be acceptable. But whatever, in China it's perfectly acceptable.
I wouldn't want to risk disappearing in a detention camp if I was you. Rather, let them have their spyware, and simply don't do anything that isn't opportune to the system.
",15,2018-09-25 16:46:31Z,170
79,"
First of all, I think you should search for solutions that are already implemented by other people. For instance, what do other people in your case do to prevent the spying activities?
One possible solution would be to have a man-in-the-middle implementation analyzing the information that is being sent, altering it, and sending it to the same server and port the spyware is trying to connect to.
I read a bit about the functionality of the app, and the information it gathers is, and I quote from the Wikipedia source you provided:

sent in plaintext

Hence, after doing some tests with a packet sniffer tool and clearly understanding how the spyware and server exchanges made using the HTTP protocol work, you could, if you have root access to your Android phone, redirect the traffic of the spyware app to a process that is running on the background of your Android OS. This process would change the data that is going to be sent to the server the spyware is trying to connect to. That way, you can send data that matches another cellphone (maybe, literally faking the data is a bad idea, because that can trigger alarms).
You should also take into consideration any kind of validation processes that the spyware has implemented so you do not alter them. More specifically, the data that is in the HTTP packets’ headers and that is sent from the spyware app to the server to gracefully initiate an upload.
Of course this is theoretical, but it is a realistic thing to do. Also, you probably will require knowledge of Android programming (mostly in C or Java) and IT.
This approach is stealthy and will not require an uninstall of the spyware app. There is always a risk, but in this case, depending on the data that is actually spoofed, the risk is minimal.
",11,2018-09-24 15:43:41Z,171
79,"
Due to the nature of the spyware, they will be able to detect any mitigation techniques which will make you a person of interest to them.
I know you said you can't afford two phones but it really is the best advice - why not clean and refurb an older phone if you have one around?
A burner phone doesn't need to be anything special and even better if it isn't a smartphone.
",9,2018-09-24 13:41:15Z,172
79,"
One idea is to think of your phone as a networking device and nothing more.  If you carry a secondary ""tablet"", that is NOT a phone, you can tether through your phone, and use a VPN on the tablet to protect your data.  Now you can hand out your phone to be inspected as all of your actual important data and work is on your tablet which is clearly not a cellphone.  If traditional hotspotting is too dangerous, you may want to consider alternative methods such as using a USB2Go cable or bluetooth pairing. If hotspotting is detected or blocked, you might also be able to use an app like PDANet to bypass those restrictions.
",8,2018-09-25 13:52:59Z,173
79,"
Disclaimer: I live in North America.

Knowing that I may be forced to install it sooner or later, what are my options to prepare against it?

Remove illegal items and anything else that you think that the government wouldn't approve of from your property - your clothing, phone, desk at work, home, etc.
If the government can't find you doing anything wrong then you only have to worry about someone planting false evidence on you - you need to search your own stuff from time to time.
When I go through Customs I've made absolutely certain that I have checked everything for anything I shouldn't have, because you know that they will likely check when you go through the port of entry.
As a result of my efforts they've not found anything to object to and have even simply waved me through a few times. Nothing to see here, move along.

Ideally:

Make it appear like the app is installed and working as intended, without having it actually spy on me.


We have different ideas of what's ideal.
Ideally I'd prefer to be paid millions of dollars per hour.


I don't know whether it includes sophisticated anti-tampering features or not. I can't afford two phones nor two contracts, so using a second phone is not a viable option for me.



It wouldn't make any sense for it not to detect tampering.
The APP is probably an excuse.
If they obtain the information by another means (like monitoring the cell phone towers and WiFi, along with all Internet traffic, and then there's your neighbors whom earn a healthy living turning people in) they can say in Court that they obtained the information from the spyware - that way you don't know how the information was actually obtained.
This happens in more places than just where you are, it's different where you are in that roving gangs force you to install the APP. In other places (including North America) they get by without using an APO and rely on other techniques.
Proof: In the last few months people whom have a lot of contact with children (Coaches, High School Principals, etc.) have had their work, home and computers searched for possession of inappropriate images. This appears in the news monthly.

Moral of the story: If you are poor and unsophisticated don't fight the rich, powerful, intelligent army trying to do something that they can find someone to back their actions to prevent you from doing it.
If you can't afford a second phone and contract that's a hint that you couldn't afford trouble in the first place, and the fine.
In your country they are upfront about it and demanding because objections fall on deaf ears. It's not much different in North America, just that they are sneaky to avoid complaints and only focus on major infractions so people don't suspect that they can see the lessor consequential things just as easily.
",4,2018-09-30 13:09:39Z,174
79,"
While other answers do provide useful insights, my alternative would be to use the phone as you normally would but isolate the app access to data
One method has been suggested by rooting the device, installing Xposed and corresponding modules of Xprivacy / XPrivacyLua. While I personally use this, rooting the device, managing additional risks due to unlocked bootloader and malicious apps gaining root access is yet another challenge - IMO beyond the scope of average user and hence easy alternative approach below
There is an open source app called Shelter that does two things

Use the ""Work Profile"" available in Android from version 5.0 (Lollipop) onwards
It allows you to install the app of your choice, in this case, your spy app in the work profile. Once it is installed on the work profile (by cloning) , it essentially is on an island and can't access any data that is outside your work profile (your photos, emails, SMS or any other app information). You can safely uninstall the original app and if police check, show them your app, which has a padlock icon indicating it is on your work profile. You can keep the app always in your recents or Overview and pull from there so that they don't even see the icon

In simple terms, the spy app has nothing to spy on!
Download Shelter - an open source app from F-droid or GitHub. Quoting the developer, relevant use case

Run “Big Brother” apps inside the isolated profile so they cannot access your data outside the profile

(Emphasis supplied) 
I am currently using this, in addition to XprivacyLua. For more details see WhatsApp: How to isolate contacts and photos - Privacy concerns 
",3,2018-10-01 09:03:49Z,175
79,"
For a rooted phone (without you will have a hard time to stop the app from doing what it is doing), you can use AFWall+ to prevent the app from phoning home and XPrivacy.
XPrivacy on Android up to 6 is better than XPrivacyLua (Android 6 and newer), because the old version can block more different things.
This setup works best, if you can install the app voluntary at a time nobody is watching you configuring the security solutions. If you can only get the app when a policeman stops you, it is a bit harder to hide the security apps.
",2,2018-09-26 09:49:13Z,176
79,"
I wouldn't be shocked if there isn't a fake version somewhere--looks exactly real but doesn't actually spy.  Unless the police are actually checking functionality in some fashion this would stop them.
As others have said, though, getting caught fooling the cops would not be a good thing!
",2,2018-09-29 02:59:38Z,177
79,"
There are a lot of great answers, but most of them give solutions that are beyond the ability of the average Android user.
I would like to offer a compromise between absolute privacy and getting spied upon.
I assume that your government has other means of acquiring the IMEI and phone metadata. You can stop the app from acquiring other information by denying it permission for:

Body sensors
Calendar
Camera
Contacts
Location
Microphone
Phone
SMS
Storage

Here are instructions on how to do it.
(I am not sure of your Android OS version, but the steps should be similar.)
This way the spyware would not be able to collect information about you, and therefore I assume that it will either not try to send information to servers, or it will send empty information.
However, this means that you install the app before police stops you and makes you install it. You can let the app download, kill the network connection while it's installing, and then remove permissions before it starts.
Risks
You should be aware that if the police does a deeper investigation, turned off permissions would be evidence that you have willfully tampered with the spyware. You should calculate if this risk is worthy.
This will also be a useful method of stopping other spyware (such as your flashlight app that requires contact information) from breaching your privacy.
",1,2018-10-02 04:29:12Z,178
79,"
You can simply resolve the servers it attempts to connect to back to localhost. 
This can be accomplished by modifying the /etc/hosts file on your device if you have root. 
Alternatively, you can use a local VPN which doesn't require root, such as this. 
",0,2018-09-25 21:42:49Z,179
79,"
Based on your question I cannot be sure whether you are a Chinese resident, or someon who is planning to visit (for instance a tourist).
It was already mentioned earlier that so far it seems that only certain minority citizens people are targeted (and if they were to start targeting foreigners that news is sure to spread very quickly).
Regardless of what I think about that, the following bit may actually help most tourists:
The google play store is currently blocked in China, so though it is easy to overcome, this likely means that you will not be able to get the app through the standard procedure.
",0,2018-09-27 14:53:24Z,180
79,"
(radically edited) 
I had proposed that one idea to consider was that

Playing dumb can be a good strategy.  
For example, ask the phone vendor, :""Hi.  My phone is working really slow and keeps crashing.  Can you show me how to do a factory reset?""  

However, I've been informed (see comments below) that even if this led to it being documented that someone else had removed the spyware, the person would likely still get in deep trouble when it was noticed that the spyware was no longer installed.   It sounds like the answer to the question, ""Is there any excuse for not having the spyware installed that would be considered valid?"" is 'NO!'  So it wouldn't matter whether the factory reset is more effective at wiping the system than the spyware is at staying installed.  I would say don't risk it, especially if you think you'd make a high-value target.  
Sadly, the software is effective at repressing dissidents and dissent even if technically it is 100% defective.  
So my answer is that secretly helping to evangelize, build or crowdfund efforts to make it not work or prove it doesn't work well or to break it or make the phone to appear broken when spies try to install spyware would be better than a more direct approach, which would paint a target on your back.  Good luck, and may the force be with you.
",-5,2018-09-25 00:03:02Z,181
79,"
I agree with Doomgoose. Get a burner phone for this. Alternatively, get an app called Orbot (it's Tor for Android) which can get you a bit of the privacy you so desire by encrypting your online activities. Another alternative is adding a VPN on top of the equation.
",-15,2018-09-24 15:12:23Z,182
80,"
I have one main concern with your setup: You should hash the UUID tokens! Otherwise, if your database leaks all accounts will be exposed. Given the amount of entropy in a securely generated UUID, a simple hash function as SHA-256 will do the job.
A few minor points.
Using a UUID as an authentication is not wrong from a security perspective, as long as you are 100% sure that it uses a secure random source (and the Javadoc seems to agree with you on that). To make the code more explicit, personally I would generate the token directly from the random source without mixing in any UUID. But that is mostly to make the code cleaner.
Make sure that you are using a version 4 UUID. The other versions are not random!
Also, I would keep the tokens in a separate table and not in the users table. But that is more about database normalisation than it is about security.
You should think about when you expire tokens. If you have some kind of log out feature, the token should expire when the user log out. In the same way, if the user is logged out when the app is closed, the token should expire when the app is closed.
",2,2018-09-18 13:53:51Z,47
81,"
All normal (GSM,CDMA,UMTS,LTE) mobile network communications are encrypted, except when it is not. For example, very old 2G GSM networks may only offer A5/0 or A5/1 ""encryption"", which is in effect none or broken. 
Many other protocols are also already broken. See: Cellular encryption algorithms currently in use globally.
Some attacks are done by downgrading the communication from a safe/encrypted protocol to a broken one.
",0,2018-09-19 09:21:18Z,185
82,"
Speaking as a former security consultant who regularly performed penetration tests of third-party mobile apps, it absolutely makes sense. In practice, mobile apps are a bit less amenable to black-box pentesting than some other types of app (such as web apps or kernel-mode drivers), but there's a lot you can do even black-box (and more with white-box, though reverse-engineering most Android apps is so easy they are practically always white-box).
A few common security issues we found:

Apps not doing TLS validation correctly (for example, allowing self-signed certs or even arbitrary invalid certs).
Apps that had vulnerable entry points (file associations, URI schemes, share contracts, or other ways to programmatically invoke them and then make them do something they shouldn't).
Apps that used out-of-date libraries or SDKs with known vulnerabilities.
Apps that used web views with insecure JavaScript bridges.
Apps that used HTTP instead of HTTPS sometimes.
Apps that used web views but didn't lock down their paths or JS execution, and were vulnerable to web app vulns and/or to phishing.
Apps that stored secrets (credentials, tokens, private data, etc.) in plain text and/or in readable locations.
Apps that stored keys or other secrets (for example, an API key for a web service) in their binaries, as though an attacker can't reverse engineer the app to find it.
Apps that used native code insecurely, and had memory corruption bugs.
Apps that used cryptography incorrectly and therefore insecurely.

There are plenty more that can crop up, though those are some of the most common and/or severe. Mobile apps are definitely capable of having security bugs, and should be subjected to security reviews including penetration testing.
",4,2018-09-16 22:32:58Z,189
82,"
As well as the penetration testing mentioned above you could have a third party perform a secure code review or have both performed by the same organization. If you have any static analysis tools you could supply the results as well as a list of vulnerabilities the tool checks for.
In my role I routinely take applications apart and find code that can be taken advantage of in given circumstances due to design decisions.
",0,2018-09-17 02:34:46Z,32
83,"
You could salt and hash the identifier, and store the result on your database. When the user wants his data, you hash and salt the ID, do a lookup on your tables and recovery the data. Guessing the ID from a properly salted hash is near impossible.
On Android phones, you can change the Android ID on rooted phones. I don't know if this is possible on Apple phones.
",0,2018-09-13 18:24:51Z,5
83,"
If you can get the device ID, so can any malicious app
Think about it this way: would you want every other app on the player's phone to have the ability to send data to a centralized hub where they could then make fake web requests to your game on behalf of this user?
Now, sure, most of the apps on the user's phone aren't doing this, but do you really want to open that door?
If you want passwordless authentication and good security, you are probably better off going with a standard solution (Authy, maybe Authentiq, etc.).
",0,2018-10-18 23:44:28Z,191
84,"
The answer you linked, from 2013, is out of date. You can have secure enclave-backed Keychain private keys now. See Apple's article on how to store private keys in Keychain. 
You will have to re-architect your API to use a public key scheme, but there is no way around this, since any fixed API key will require the key to be in memory at some point anyway.
",1,2018-09-13 16:28:00Z,193
85,"
A QR code is just text, encoded in a format that is easily readable to a computer. Can you make the preceding sentence uncopyable?
You cannot make text incopyable. Forget it. It is utterly impossible. What stops me from whipping up paint.exe, and copying the QR code pixel by pixel? Or place it on a scanner and print thousand copies?
QR codes relies on that the smart phone is able to make a copy of the content. When you scan it with your phone, the picture is a copy of the content. The picture is decoded, and a new copy of the information is made, this time as a text string.
So what you ask is not only impossible, it strikingly impossible. You must look elsewhere for how to solve your problem - for instance by invalidating the data in some online database.
",5,2018-09-05 05:27:04Z,195
86,"
So you have a setup where the user needs to do full username / password authentication, but as a short-cut you allow them to use a PIN until the session expires.

I would follow the model used by the Keepass2Android client:

When you open the app (equivalent to your new session), you need to do the full authentication process.



While the app remains open, but in the background, you can re-open it using a short-cut -- last 3 characters of your password, fingerprint scanner, etc (equivalent to your PIN, I think).




When you fully close the app, you will need to do the full login next time.


So following that model, I would have them enter one PIN at install, when opening a new session they must re-enter their full credentials, and while the session is open they may use the PIN.
I don't see any difference here between whether the session expired naturally or because they clicked Log Out. I also don't see any reason to ever force them to choose a new PIN; that seems like it's asking for usability problems (""What's my PIN again??"") for minor-at-best security benefits. 
",0,2018-08-27 17:09:45Z,90
87,"
You should only control what can matter. The localisation is always off on my smartphone, unless I am actually using it. Not really that I want to hide it, but I can hardly understand why a banking service could be interested in it. And I know that a legal enquiry would immediately get it from my operator. The same if I use a (public or not) Wifi access. I assume that my bank also knows it, so they do not know it but know that they could get it if an attack came from there.
Things are different when you use Tor or some other well known(*) proxies that are used for privacy, because now even legal enquiries cannot easily find where the connection came from, it they even can. So IMHO I would neither be disappointed nor even surprized if my bank rejected all Tor accesses or required at least a strong second authentication factor (nor a secret question...) for security reason. After all I have never entered a bank office wearing a mask, and I would expect that they do not let me in.
So reject Tor accesses if you want, but please do not concentrate too much on geolocalisation for a banking service. What matters here is to authenticate the clients not where they are. Anything that can prevent access point identification is suspect, but simply not giving localisation is not.

(*) Not all proxies are the same. Do not reject corporate proxies for example...
",1,2018-08-12 07:10:31Z,83
87,"
Are you talking about faking location services in a browser or on a mobile device, or connecting through a VPN to appear to spoof IP-based geolocation?
Personally... if my bank asked for access to geolocation data from my phone for anything other than finding a branch or ATM, I probably wouldn't be using that bank. As for IP geolocation checks... I'd use that data to add weight to fraud detection.
If there are enough factors to suggest the request is not by the customer, then block as potential fraud. But current location alone probably shouldn't be enough, because people travel. Did they suddenly move from one country to another faster than one could travel by plane? Are they connected from two countries at once? Is the user-agent of the connection different than what the normally use?
Add up all these little things and set a threshold at which a block action will take place. If you want an example of how this would be structured, email anti-spam systems are a great example to look at. There's not one single factor to decide if email is spam... it's a system of different checks that work in concert with one-another.
If VPN services are a concern... add them as a factor in the system. But many people will be connecting to a VPN for banking if they are on public WiFi. I don't think that alone would be a legitimate reason to block a request. I'd probably set the system to adjust the weight of the factor based on whether the user normally uses a VPN or not.
",1,2018-08-11 17:48:15Z,198
87,"
Malicious individuals attempting to break your application will probably fake their location and hide their public IP through VPNs, Tor and/or proxies. Not blocking users faking their location could expose you to their attempts
On the other hand, faking location is usually something a user worried about his privacy would do.
Also note, that this kind of controls are made client side of the application, so a malicious individual with enough knowledge in reverse engeneering may bypass that kind of filters, while a legitimate user probably won't do it
You should balance what do you prefer, block possible attackers and possibly legitimate users faking their location or expose your application to those attackers
",0,2018-08-11 15:43:50Z,199
88,"
Let us review the basic access control corning stones with their pros and cons relating to your case of device theft:
Something you know
Referring to passwords, passphrases, PIN codes, secret questions.  
Pros:
It can be a hard time for the thief to ""crack"" this factor as is, especially if you enforce a strong policy.
You can mitigate the concern of resetting the password through an email using a challenge (e.g strong secret question, perhaps recent activity based) as a must step for the password reset. 
e.g:

Where did you make your recent payment? to whom?

Cons:
This is considered to be the weakest factor among the others I will mention, and indeed in case of theft, it will be easily changed without the abovementioned mechanism.
Something you have
A physical token that your application can scan using RFID, NFC, etc.  
Pros:
Provides a higher level of security as long as the token is not near the device itself   
Cons:
Pricey, perhaps overkill. User experience might be bad as well.
Something you are
Biometrics; fingerprint, iris scan, voice analysis, behavioral analysis (continuous authentication)  
Pros:
Considered to be the strongest factor, industry standard, most of today's devices have this hardware in place for you to utilize.
Cons:
Biometric authentication has its errors; false rejection of the user and acceptance error of an unauthorized entity (e.g: printed picture, looked alike person, etc)
TLDR:
you can enforce a strong secret questionnaire upon sensitive operations (transactions, password reset requests, etc), you can add a physical token or verify the client upon sensitive operations based on biometrics
",1,2018-08-02 15:46:44Z,201
88,"
Only if you haven't read this already, I would recommend you take a look at the PCI DSS Mobile Payment Security Guideline standard: 
https://www.pcisecuritystandards.org/documents/Mobile_Payment_Security_Guidelines_Developers_v1.pdf
Making your payment app PCI Compliant should be your number one goal in security terms. After that, if you think you can strengthen your app's security, go ahead! 
",0,2018-08-02 18:31:21Z,202
89,"
It perhaps makes more sense if I paraphrase your question:
Can a router be infected by devices connected to it?
Yes, although in such a scenario the router and infected devices will typically have very different configurations and operating systems - implying that an exploit targeting the client device is unlikely to affect the router device. And it would be very unusual (but not completely impossible) for the router to be exploitable via the traffic it routes - more likely by services exposed on the router device for control, configuration or other purposes (such as storage). 
If the router is a phone then it will provide additional functionality (such as storage). But it is unlikely to provide the remote configuration functionality one would expect on a dedicated router (having its own user interface and limited number of interfaces).
Using USB tethering rather than Wifi for connectivity changes Some things - by exposing different services on the phone to a (possibly) different set of clients - e.g. it might be impossible to invoke the telephony operations over wifi, but accessible via USB. Hence (except for the case of running an open hotspot) its impossible to say if USB offers any security benefit over wifi or vice versa.

at more risk if usb debugging on the phone was turned on

Yes, you would be at more risk - there's a bigger attack surface. 
There will always be a lot more infected devices on the internet side of the ""router"" than on the local network interface. 
",3,2018-07-23 11:22:27Z,58
89,"
I don't know any vulnerability that an infected computer can attack an android device acting as an hotspot.
Related to the second question, it's possible that it could be fine, but can always exists a vulnerability or 0day to bypass that protection. One example in Android 4.4.2. The best solution for this is to use a ""condom"", read more in here.
If you don't need USB debugging enabled, it should be disabled. If you don't want to turn it on and off every day, you should use the solution above (usb condom) when connecting to untrusted devices. Be aware that some devices have ADB enabled over the network, that it's a common vulnerability that is being exploited on the wild.
",3,2018-07-23 11:38:02Z,93
90,"
An easy way to check if they are somehow getting access to the OTPs (and therefore to your accounts) is to log into your account and theck the activity log. Most reputable services (GMail, Amazon, etc) should have some way to view a list of date/time/location of recent successful logins.
If any of those were not you, then you have a problem.

Are you sure they are triggering your account's OTPs (which would sound like they are trying to get in to your account and / or may have already gotten past the password page)? Rather than signing up your phone number on new accounts (which could be the beginning of harassment / extortion)? OTP SMS messages don't typically tell you which account / username triggered them, but are you getting any OTPs for sites on which you don't have an account?

It's hard to say for certain that your sensitive information isn't at risk (apart from whatever was accessed via your Outlook before you changed the password), but this smells more like a social engineering attack of some sort rather than a technical attack.

My condolences that you've been caught up in this.
",2,2018-07-13 22:44:02Z,90
91,"
Yes, it is possible. There's in general two main ways to achieve this:

Social engineering. Fool the user into installing a piece of software. Installing software not in the app stores are doable on Android, and quite a lot more difficult on iOS.
Find a vulnerability, that either lets you install arbitrary apps without user interaction, or let's the browser leak the desired data to you. Sandboxing attempts to avoid browser data leaks.

It's not an easy task, but it's not entirely unlikely either.
The easiest way to mitigate the risk is not to access unknown or unexpected links, or in case of messages pertaining to be from a service, e.g. your bank, access them by manually entering the address in the browser. This will mitigate for instance Punycode attacks. 
TL;DR: Software may have security holes.
",2,2018-07-07 17:26:06Z,195
92,"
Is this technique used?  I can't speak specifically to mobile phones, but in general, it's a regularly used technique usually referred to as de-anonymization or re-identification.
One common legal application is in web advertising.  Advertisers aren't allowed to have access to your personally identifiable information, but they may have access to your anonymized information and some unique identifiers (usually from cookies or browser fingerprinting).  Those unique identifiers can become lost or replaced as cookies expire or are deleted, but by cross-referencing patterns and other anonymized information, it's sometimes possible to draw a conclusion that two different unique identifiers are actually the same individual.  In the advertising world, this can be used to identify that two seemingly different people are actually the same person using different devices (e.g. mobile phone, desktop, tablet) without ever actually knowing who the individual is.
That said, it's not always an easy or foolproof process, but it is possible.
",1,2018-07-06 17:27:07Z,206
93,"
SMS messages are always sent to a phone number. If you don't have access to the phone number registered for 2 factor authentication at some provider, let them know immediately so they can remove the number.
",4,2018-06-19 13:40:09Z,208
93,"
They are specific to the phone number. Before you change your phone number (SIM) you should first disable 2FA and re-enable it with the new number.
",0,2018-06-19 14:39:02Z,209
94,"
Does the package name need to be kept secret
No.
Here is a link to one of my apps:
https://play.google.com/store/apps/details?id=uk.co.jrtapsell.appinfo&hl=en

(Link purely for example, it was found on the first page of google when the app name was searched for)
You can see that google provide the package name in the URL, so even if you hid it, opening the store page for an app would give it out anyway.
Do bundle IDs need to be kept secret
No.
Following the steps here, you can get the bundle ID from the app link, so it would appear that it does not need to be kept secret.


Find the app online (Google for the iTunes link). For this example we use Apple Pages: https://itunes.apple.com/app/pages/id361309726?mt=8.
Copy the number after the id in the URL. (Here: 361309726).
Open https://itunes.apple.com/lookup?id=361309726 where you replace the ID with the one you looked up.
Search the output for ""bundleID"". In this example it looks like this (next to a bunch of other data): ""bundleId"":""com.apple.Pages"". 

So for Apple, the bundle ID is com.apple.Pages.

",1,2018-06-13 09:19:14Z,41
95,"
TrueVault recommends doing data de-identification and re-identification client side (within your web or mobile app logic). This makes two interesting flows:
De-Identification

Data is entered into your application by the user. 
Your application (e.g. JavaScript code in a browser or Swift code on an iOS device) will split that data into what is identifying (names, phone numbers, etc) and what is not identifying (other health data). 
The identifying information is sent directly to TrueVault from the client application through the TrueVault API. Ideally the end-user is authenticated as a TrueVault user, so the API permissions can be granularly defined. The API returns an opaque identifier that can be associated with the de-identified data and stored on your server.
The de-identified health data (along with the TrueVault id) is sent to your server. Your server is not handling identifying data & health data, so it doesn't fall under the purview of HIPAA regulations.

Re-identification
Let's say your end-users want to search for data based on criteria stored on your server (e.g. blood pressure) but they want to see identifying data in the result set. This re-identification should happen on the client. Something like:

Client device makes a request to your server to retrieve information matching the query.
Client device pulls TrueVault ids out of the server results. These ids are opaque, they are not identifying, but they can be used to retrieve identifying data from the TrueVault API (if properly authenticated).
Client device requests identifying data from TrueVault API corresponding to the TrueVault ids returned by your server. Ideally the end-user is authenticated as a TrueVault user, so the API permissions can be granularly defined. 
Client device merges the datasets so the UI shows a re-identified view of the data.

Compliance
Following these steps, you can create a seamless user experience. There's no reason the end-user should know that you're de-identifying and re-identifying data. 
This process is designed to keep your server infrastructure free from HIPAA Regulations, so you can host this de-identified data in a non-compliant manner if you chose. This is all above-board with HHS, they even have specific guidance on it: https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html. 
By avoiding storing Identified PHI on your servers, you can skip the physical and technical safeguards that apply to server-side infrastructure. Please understand that you don't escape the administrative requirements of HIPAA entirely. For example, in the system described above there are still human beings looking at Identified PHI. Those humans must be trained and their access must be limited to the minimal amount needed for them to perform their jobs. All of this work is a hard requirement if you're making an application that must be HIPAA Compliant, whether you use TrueVault or not. What TrueVault saves you is all of the work to have secure and compliant infrastructure for storing data, maintaining audit logs, secure backups, high availability, controlling access, managing users, and lots more.
",1,2018-06-11 21:15:01Z,212
96,"

What are some attacks that a hacker could do to take advantage of a registration setup such as this?

Although this will most likely not help you in your case to change the registration flow, the first thing that comes to mind is to abuse the SMS function. Since only a username and a phone number are required this can be used to:

Get the company out of SMS credits (depending on the setup)
Get the company a bad reputation for sending random text messages (in case the company name is used in the SMS message).
In a replay attack this could lead to unnecessary costs.
The company registration flow can be used as a ""prank platform"".

A better solution would be to have a user enter a username, an email address and get the phone number programmatically (https://stackoverflow.com/questions/2480288/programmatically-obtain-the-phone-number-of-the-android-phone) from within the mobile application. 
An email is sent to the given email address with a verification link. When the user clicks the verification link from the email, a password should be set. Make sure that once the password has been set, the registration link is invalidated.
Additionally, the registration link should have a specific life time, e.g. it should expire after X amount of time.  
",3,2018-06-02 06:18:09Z,214
97,"

That way the hacker won't know if they have got the login details correct or not.

If the information presented after login has no relationship to the person who the login should be for, then most hackers will quickly recognize that the login is probably not the real one. 
But, in order to show information which looks like it fits the user, considerable effort could be needed. It also needs to be created specifically for each user and show some true information about the user so it does not look fake but not too much so no important information is leaked. 
You cannot expect your provider to do this for you but you might try to do this yourself in many cases, i.e. add another email account, another facebook account etc. 
",66,2018-06-01 12:58:38Z,6
97,"
The concept you're describing is called Plausible Deniability and methods to provide it have indeed been implemented in some software, VeraCrypt being one example.
One problem with implementing it in websites, as you suggest, is that it's very hard for the website developer to come up with fake data that is realistic enough to fool an attacker while not giving away any sensitive data about the user. In encryption software like VeraCrypt, that task is shifted to the user, who is obviously in a much better position to do that.
",50,2018-06-01 12:58:54Z,216
97,"
Because hackers don't attack login forms
The flaw is that you assume hackers get into accounts by brute-forcing credentials against remote services. But that's futile anyway.
Any website with decent security (the ones without decent security wouldn't care about your idea either) will have a limit imposed on how many failed login attempts can be made in a certain timeframe per IP address, usually something like 5 failed attempts every 6 hours. If security is a bit stronger, accounts might also need action from the owner after a number of failed attempts, and/or the owner might be notified of failed login attempts or even all logins from new devices.
So while brute-force attacks may well be feasible against plain data (such as password hashes exposed in a breach), they are nowhere near feasible against any service with even a bit of security.
For attackers, it is thus much easier to go phishing, or better yet set up a genuine free service themselves and work on the assumption of password reuse:

",31,2018-06-02 04:07:43Z,217
97,"
I have never heard of any service or device implementing this either.
The case where an attacker is present and forcing you to login is pretty unlikely. They are more likely to just take your $1000 iPhone and run.
However, it is very plausible for this to happen if the ""attacker"" is a security guard/TSA officer at an airport security checkpoint. Especially if you are in a foreign country. (There was a PHENOMENAL Defcon talk on this subject a few years back.)
Websites
It probably wouldn't make much sense to implement this on a website. If you (the admin) are certain that someone who is attempting to access an account is a hacker, just block them/lock the account. Problem solved.
If the attacker is trying to access multiple accounts, they will probably know something is fishy if they are able to ""successfully"" login to multiple accounts on the first or second try.
Phones
While phones don't allow fake logins (?), but you can set them to lock after the password isn't entered correctly n times.

Attacker/TSA agent tells you to unlock phone. You intentionally enter wrong password on 1st try.
""Oh, oops, wrong password...""
You enter the wrong password again on the 2nd try.
""Sorry, my hands get sweaty when I am nervous...""
You enter wrong password on 3rd try. Phone is now locked for 30 minutes!

This of course will not work if you are reciting the password to the attacker, and they are entering it in the phone. And I think most phone lockouts only last for 30 minutes (?), during which time the attacker/TSA agent will do their best to ""convince"" you to remember the password in a back room.
Laptops
Your suggestion would be relatively easy to implement on a laptop...
Create 2 or more user profiles.
The first profile you name after yourself (first and last name). You set a picture of yourself as the profile picture. This will be your ""fake"" account. Set the password as something simple and easy to remember. Put some ""personal stuff"" in the account (music, pictures of your pet, ""work"" documents, etc).
The second account you give a generic family member name (""hubby"", ""the kids"", ""honey"", etc). Keep the default profile picture. Set a strong password. This will be the account with admin privileges on the laptop, and the account which you will use for your important/confidential work.
Now imagine a scenario in which you are forced to login...

You are in an airport in Oceania, about to fly home to Eurasia. Airport security stop you on your way through the terminal.
Security: ""Give us your passport and laptop!""
You hand them the laptop and passport. They turn on laptop, and try to login to the account which you named after yourself. Upon seeing they need a password, they demand you tell them the password.
You: ""The password is opensea. No spaces.""
The airport security enter the password, and successfully enter your fake account.
After looking around for a few minutes and not finding anything that interests them, they log out and try to login to your real account.
Security: ""Whose account is this? What is the password?""
You: ""That is my kids' account. The password is 123dogs.""
They enter the password, but are unable to login.
Security: ""That password is wrong! Tell us the correct password!""
You act surprised, and ask them to give you the laptop so you can attempt to login. They hand you the laptop, and you start typing in bogus passwords.
You: ""Those darn kids, I told them NOT to change the password! I'm sorry, they were only supposed to use that account for their stupid video games!""
The airport security confer with each other, and then let you go on your way. You safely return to Eurasia without having the confidential information on your laptop compromised.

",14,2018-06-01 21:45:19Z,218
97,"
It's not exactly the context you had in mind but there are in fact systems which implemented this idea. I used to work at a (somewhat sensitive) facility where each employee had two codes to disable the alarm system: The regular one and a duress code. If you used the duress code, the system would be disabled so as to not put you in danger but a silent alarm would go off at the monitoring centre. I am reading on Wikipedia that this was also considered for bank ATM in the US but ultimately ruled out.
Another similar concept is the “honeypot“. Some of them might in fact accept any credentials or serve dummy data when attacked, to be able to record what an attacker does next or otherwise exploit the situation (e.g. capture the payload of a worm).
As to why it's not more common in consumer products, online services, etc. there is simply a trade-off between the benefits (how likely a particular attack is, whether it would effectively deter criminals or just prompt them to slightly alter their technique) and the costs (more complex systems to develop, maintain and certify - which also means increased attack surface that could afford an attacker with an actual entry point, bandwidth and operating costs to serve the dummy data to all botnets constantly attacking online services, effort to create credible dummy data to fool more sophisticated attacks).
",12,2018-06-02 09:20:20Z,219
97,"
This is called 'Deception Technology' in the cyber world where the solution deceives cyber foes (attackers) with turn-key decoys (traps) that “imitate” your true assets. Hundreds or thousands of traps can be deployed with little effort, creating a virtual mine field for cyber attacks, alerting you to any malicious activity with actionable intelligence immediately. The traps would carry login details, dummy data, dummy system, etc to deceive the attacker by intimating like actual system.
Deception technology is an emerging category of cyber security defense. Deception technology products can detect, analyze, and defend against zero-day 
 (where the attack type/procedure is not known before) and advanced attacks, often in real time. They are automated, accurate, and provide insight into malicious activity within internal networks which may be unseen by other types of cyber defense. Deception technology enables a more proactive security posture by seeking to deceive the attackers, detect them and then defeat them, allowing the enterprise to return to normal operations.
You may refer below link for some solution providers:
https://www.firecompass.com/blog/top-5-emerging-deception-technology-vendors-at-rsa-conference-2017/
",4,2018-06-03 16:32:45Z,13
97,"
This has been done in the past, rather successfully, but depends a lot on what the system is. 
At one point it was not uncommon for paid access websites to auto detect when an account was logging in from too many IP addresses or with other suspicious patterns and redirect those users to a version of the site that was primarily ads and affiliate links to other sites. If done well the sharing of stolen login credentials could become a revenue center. 
There are also websites that direct users to different versions based on IP address or other criteria; the simplest version of this is targeted advertising. 
For shell access it's possible to direct a login to a chrooted jail that has only a small section of disk and specially provisioned binaries, which may not necessarily be the same as the general system. 
",2,2018-06-02 05:33:46Z,220
97,"
First off, I wouldn't call the attacker in this scenario a hacker. A hacker is trying to get around the security that the website offers, in your scenario the attacker doesn't care how secure your services are, he cares how easily the user is intimidated and possibly what to do with the body afterwards.
Secondly, alternate credentials that change your access has been done, but if it does more than present a restricted view of the truth, is a lot of work and of limited utility.
The reason it is of limited utility, is because your users know about it,  you must presume that any attacker knows about it as well.  Suppose you did this for an ATM card so that it showed a balance that was less than a hundred dollars in order to limit your loss. Either the attacker asks for both (in which case the victim has at best a 50% chance of not loosing more) or simply includes as part of his demands that it produces more than that -- ""if I don't get at least 200 you're dead"".
It's not totally useless, but is only effective against an ignorant attacker.  Relying upon the attacker not knowing something is called security through obscurity, aka ""they got it"".
",2,2018-06-02 12:16:20Z,221
97,"
For web and  a remote attack, as many folks here stated before, appart from the difficulty of creating fake user's content  , there is the problem of : how do you know it's a compromised login?
I mean, if you assume there is some sort of suspicious activity , like a brute-force attack, you can just block the login for that IP and maybe for that account itself for a while (until the real owner somehow validates its identity)
The only usefull cases are forced logins, that's another story and a pretty cleaver idea. Here is the implementation I imagine for a social network:

The user creates himself an account with dummy data, and set it as his dummy account.
When there is a forced login going on, like you jelous gf or bf extorting you to login , then you put the dummy password and there is! you are logged in your beautiful self created dummy account.

BUT Its  not a perfect solution either. The attacker probably would know you and, if it's your crazy ex  for instance, she may just check her chatlog with you and will know you just logged on the bogus account. 
This is specially relevant because it will be a public  well known feature of the platform you are , so anybody who forces you would be able to check wheter are you or not.
For banks or other sites, it's a pretty good idea.
",1,2018-06-02 19:10:47Z,222
97,"
The problem is that your users have to know about it, thus you must presume that any attacker knows about it as well. 
",-1,2018-06-02 12:53:22Z,223
97,"
To me, this sounds as if you were inviting the attacker to dance with you. 
""Hey attacker, you want to hack my website? Well here is a fake login web site for you!""
You certainly do not want to invite the attacker to dance with you because he might find it amusing and challenging which would give him even more motivation to try to hack into your website.
",-2,2018-06-04 15:22:26Z,224
98,"
The flow you describe is exactly what Google offers for mobile apps : https://developers.google.com/identity/protocols/CrossClientAuth
After looking in more detail the rfc 6749, we can note that this flow does not respect the recommendations.
https://tools.ietf.org/html/rfc6749#section-4.1.3

The authorization server MUST:
o  ensure that the authorization code was issued to the
  authenticated
        confidential client, or if the client is public, ensure that the
        code was issued to ""client_id"" in the request,

Indeed, the authorization code was issued to the native app, and it is the web application that tries to exchange the code.
It is up to the developer to decide if he wants to strictly respect the RFC and in this case use the flow Authorization code with PKCE then send the access token to the backend server so that it can access the resource server; or if he wants to use the same flow as google.
",1,2018-05-31 07:50:22Z,226
99,"
The IMEI is a global fix identifier for a phone. In some countries, if your phone is stolen and if you have noted its IMEI, you can communicate it to a central organization that ask all local operators to blacklist it, making the phone unusable on the entire country.
Allowing a user to simply change it would immediately break that feature.
",3,2018-05-24 15:07:30Z,83
99,"
Quite simply, because the standards for mobile networks require that the IMEI is unique. To allow users to change it would violate the standards. 

The International Mobile Equipment Identity number (IMEI) uniquely
  identifies an individual mobile station. The IMEI is unique to every
  ME and thereby provides a means for controlling access to GSM networks
  based on ME Types or individual units.

The assigning of the IMEI is strictly controlled:

The term Reporting Body shall refer to a Body that is recognised by
  the GSM Association as having authority or competence to allocate
  IMEIs to ME Types. These Bodies may have also been given authority by
  their national governments or industry bodies to perform a regulatory
  conformity assessment procedure or otherwise permit the use of mobiles
  on networks. There may be more than one Reporting Body in a country.

(source: https://www.gsma.com/newsroom/wp-content/uploads/2012/06/ts0660tacallocationprocessapproved.pdf)
In addition, some jurisdictions require that the IMEI be used to block access to mobile networks, and make the changing of the IMEI illegal:

Re-programming mobile telephone etc. 
(1)A person commits an offence if— 
(a)he changes a unique device identifier, F1... 
(b)he interferes
  with the operation of a unique device identifier. 
[F2(c)he offers or
  agrees to change, or interfere with the operation of, a unique device
  identifier, or] 
[F2(d)he offers or agrees to arrange for another
  person to change, or interfere with the operation of, a unique device
  identifier.]

(source: http://www.legislation.gov.uk/ukpga/2002/31/section/1)
",2,2018-05-24 15:39:51Z,8
99,"
The IMEI serves as a globally unique identifier, like a MAC address in IP-based systems. Think of the 

IMEI as the MAC address 
IMSI (SIM Card number) as the IP address

To arbitrarily connect to a globally routeable system, a device MUST be uniquely identifiable. Otherwise, collisions will occur.
Addendum as per the comments:
You CAN change the IMEI number with a hack, but you should not do that of course. Likewise you may also forge a SIM card, this will allow you to impersonate a user (and generate cost to them). I believe either of these would be a criminal act in most jurisdictions.
Additionally, creating duplicate identities may put these on a black list.
",1,2018-05-24 10:58:30Z,228
100,"
At the moment
Currently, the best way is to kill apps in the background that have microphone permissions, which is a bit of an ugly hack, and if you do not kill the apps fast enough they will be able to record snippets of your call.
https://stackoverflow.com/questions/17467120/exclusive-access-to-the-microphone-in-android
In the future
Android P appears to fix this, by making apps in the background act as if the microphone is silent, along with several other sensors.
https://www.theverge.com/2018/3/7/17091104/android-p-prevents-apps-using-mic-camera-idle-background
",2,2018-05-24 07:48:33Z,41
101,"
To make shoulder surfing for lockscreen pins harder, especially on mobile, I think you're going to have to do something that is not static - that is the content or challenge changes every attempt. 
For example, Blackberry used to offer a pseudorandomly generated lockscreen called picture password. It involved you picking some visual element on the picture and dragging that part over a number pad which was randomized every time. This made it so that the pin itself wasn't useful, rather the gestures to unlock them were which makes this more resilient to shoulder surfing. 
Another method might be to randomize your keypad for PIN entry. Some custom Android ROMs already offer this. So a shoulder surfer who only sees the pattern that your fingers punch down won't be able to unlock your device by replicating your finger movements. 
Other than these two options, other methods might be to use Biometric scanning like Facial recognition or Fingerprint readers. Both of these make it harder for someone to see tour actual pin and unlock your device. Alternatively consider using a paired device with 
Google's Smart Lock to unlock your device in front of others. 
",1,2018-05-22 15:20:39Z,129
102,"
TL;DR
They can in theory improve security a bit in general, but not if you are trying to protect from anyone you would actually call an attacker. In practice they can provide a bit more privacy in some situations, but then using your phone might become a hassle and in the end it might not be worth it.
Long version
If you use different passwords for each app, or at least different passwords for apps that are supposed to be accessed in different situations, then yes, I believe it can add some security, but it might not be worth the hassle.
I am thinking of this example: suppose you have three different passwords, one to unlock your device, one to unlock your email application that you only use for work, and one to unlock your picture gallery. Now, it's probably very easy to steal one of your passwords/pins/gestures by shoulder surfing, especially by people who are frequently around you like friends, partners, etc. When you are hanging out with friends the password you type most often is probably the one to unlock your device, then you might also type the one to unlock your picture gallery pretty often, and you probably won't type the password to access your work email at all. So a friend might grab your phone, unlock it, browse the gallery, but not read your work email. If you have a phone that requires your fingerprint to unlock the screen, then the above scenario only applies if somebody grabs your phone while it's unlocked.
So this was just to show that it can indeed add some security, and as usual it depends on the possible scenarios and possible ""attackers"". On the other hand, you will have to use several different passwords, enter them too often, and there are probably some applications containing potentially sensitive data that you need to access very frequently in a lot of different situations (whatsapp?), so protection becomes less effective even in the scenario I described above. I'm also not sure whether it can slow down a decent attacker (not your average friend, unless your friends are decent attackers), preventing them to access some of your application data at least for a while, at least until you discover your device has been stolen and maybe remotely wipe all your data, but the link provided in a comment to your question seems to imply that a decent attacker won't really be hampered by this application for long.
",1,2018-05-22 14:04:53Z,231
