Post ID,Answer ID,Answer,Vote Score,Date,User ID
1,1,"
If your devices need Internet access, use a three router Y-configuration to separate your 'ordinary' (home/work) network from the one containing your IOT devices.
There are plenty of resources about that on the Internet.
This article goes in-depth describing the disadvantages of simpler configurations, leading up to the Y concept. The author even has a walkthrough showing how he set this up for himself.
Note: Many modern router devices let you mimic this configuration with one box only, but talking about three physical ones makes you understand it better. You may even have some old routers laying around that you now have a purpose for.
",1,2018-02-05 20:59:37Z,0
1,2,"
You start by not giving them access to the Internet.  If your Intranet of Things devices have no direct contact with the outside world (eg. your IoT sprinkler controller is firewalled off from the Internet and needs to go through a proxy when requesting a weather forecast), it becomes much harder for an attacker to do anything.  If there's no connection whatsoever, any attacker needs to be physically present.
",2,2018-02-02 22:42:21Z,3
1,3,"
You didn't mention the first rule of IoT security. Change the admin password. If possible make it complex. I have witnessed hundreds, if not thousands, of penetration attempts and most of them are attempting to compromise some form of administration account with guessable passwords. 
Another thing that you can do is register it with a directory. You may have AD available to you but there are also dozens of online cloud versions available too.
If you are going to have multiple devices this can simplify any administrative
activities as well.
",1,2018-02-05 17:36:33Z,4
1,4,"
IoT is a loaded term.  I'm going to assume the topic is about ""How to keep self-configured IoT devices secure"" and disregard cloud-based IoT devices.
The best way to learn about securing IoT devices is to learn how generic security works.  IoT devices are by definition, internet connected devices.  They are no different than computers, servers, network printers, routers, switches, etc.  They face all the same problems that those devices face when it comes to security.
You can find a ton of information on how to secure network infrastructure online.  There are many other resources as well, such as academic classes.
I personally, run a firewall on my router that segments my network and white-list locations I access it from.
To disregard my disregard, cloud-IoT devices are at the mercy of their cloud services.
",1,2018-02-05 20:10:00Z,2
1,5,"

Lastly as an importart part of the threat model is physical security, because if I put a sensor outside with a solar panel to power it, how do I stop mischievous kids or crooks from destroying or stealing the technology?

You, essentially, have to build a better mouse trap. Consider your physical security situation - is the street side of your house safe, but kids out of view of the streets will wreck things? Perhaps the opposite is true - position the sensors within view of the street (or vice versa). Next, consider replace-ability - is this a cheap sensor or a $150 weather station type deal? Consider pairing down the features of outdoor sensors to lower the cost of replacement. If it takes you five minutes and five dollars to replace the sensor, who cares if it's broken or stolen every three months. Thirdly, consider hardening - is your sensor on the top of a 30 ft roof? Is it resistant to a baseball or rock? Do you need to walk up to it and wail on it with a golf club for a few minutes to break it? Can you make it that resistant? 
All security is about making compromise ""not worth it"" for the attacker. It's hard to overcome the ""worth it"" factor for teenagers - the destruction itself is the reward, and making a harder target makes the destruction more fun. You'll have to think critically about what you need to do to make the sensors unlikely to be damaged, or make them easier to replace when damage occurs.
",1,2018-02-05 20:34:31Z,5
2,1,"
Be very, very careful. It's not KRACK that is the problem, it is a lax attitude to security and privacy in general. So called ""smart"" consumer products can often be hijacked, accessed from the internet, or monitored. As a customer, it is hard to know if any specific product is safe or not.
The Norwegian Consumer Council has been on the case for a while, and produced a few horror stories. From a report, aptly titled #ToyFail, on three ""smart"" dolls:

When scrutinizing the terms of use and privacy policies of the connected toys, the NCC found a general disconcerting lack of regard to basic consumer and privacy rights. [...]
Furthermore, the terms are generally vague about data retention, and reserve the right to terminate the service at any time without sufficient reason. Additionally, two of the toys transfer personal information to a commercial third party, who reserves the right to use this information for practically any purpose, unrelated to the functionality of toys themselves. 


[I]t was discovered that two of the toys have practically no embedded
  security. This means that anyone may gain access to the microphone and speakers
  within the toys, without requiring physical access to the products. This is a
  serious security flaw, which should never have been present in the toys in the
  first place.

And from an other of their reports, again aptly named #WatchOut, on ""smart"" watches for kids:

[T]wo of the devices have flaws which could allow a potential attacker to take control of the apps, thus gaining access to children’s real-time and historical location and personal details, as well as even enabling them to contact the children directly, all without the parents’ knowledge.
Additionally, several of the devices transmit personal data to servers located in North America and East Asia, in some cases without any encryption in place. One of the watches also functions as a listening device, allowing the parent or a stranger with some technical knowledge to audio monitor the surroundings of the child without any clear indication on the physical watch that this is taking place.

And the FBI agrees:

Smart toys and entertainment devices for children are increasingly incorporating technologies that learn and tailor their behaviours based on user interactions. These features could put the privacy and safety of children at risk due to the large amount of personal information that may be unwittingly disclosed.

So unless you have a real need (other than ""this is cool"") for these kinds of products, I would say that your best approach is to simply stay away from them.
",92,2017-10-31 15:13:39Z,9
2,2,"
It really depends on your threat model.  I wouldn't be particularly worried about a particular sexual predator in your local area having the technical skills necessary to utilize Krack to inject voice into the toy. Unless it uses the vulnerable Linux driver, the key clearing won't work and the partial nature of the compromise for a general reset would make voice injection nearly impossible.  
Similarly, as a client device, it doesn't offer a whole lot of security risk other than possibly as a listening device, depending on if it is always on or activated by pushing a button.  Krack wouldn't make it usable as an entry point in to your network directly, so I don't see it as a particularly riskier device than any other IOT device.
As always in security, it comes down to your risk aversion though.  Personally, if I thought it would be valuable to my child (who is also 3) I don't think I would consider the local security implications as a reason not to get it for my home environment.  I'd be more concerned about the controls and security on the web side.  
My main concern for IOT devices isn't the local compromise so much as the web connected remote compromise.  The chances of a sufficiently skilled and motivated malicious individual in your direct proximity is pretty low.  The chances of a motivated and malicious user on the Internet trying to remotely access the IOT device is significantly higher and it's important to understand what holes the devices punch in your network protections.
Also, as Michael was kind enough to point out, the interests of such a broad hacker are much less likely to be concerned with your privacy and much more likely to either be interested in attacks on your other computers or on the computational capabilities of the device as an attack bot.
",15,2017-10-31 15:04:35Z,11
2,3,"
Welcome to the Internet of Things(IoT). This is a... thing. Therefore, it can be assimilated

Mirai is a type of malware that automatically finds Internet of Things devices to infect and conscripts them into a botnet—a group of computing devices that can be centrally controlled.

And

One reason Mirai is so difficult to contain is that it lurks on devices, and generally doesn't noticeably affect their performance. There's no reason the average user would ever think that their webcam—or more likely, a small business's—is potentially part of an active botnet. And even if it were, there's not much they could do about it, having no direct way to interface with the infected product.

The problem is that security is seldom a consideration when making toys like this. The technology to make all this work is fairly simple, but the companies aren't paid to think about this. It's a child's toy. It's meant to be cheap and easy. And you get what you pay for. 
Earlier this year, it was found that a similar child's toy had no security at all (emphasis mine)

A maker of Internet-connected stuffed animal toys has exposed more than 2 million voice recordings of children and parents, as well as e-mail addresses and password data for more than 800,000 accounts.
The account data was left in a publicly available database that wasn't protected by a password or placed behind a firewall, according to a blog post published Monday by Troy Hunt, maintainter of the Have I Been Pwned?, breach-notification website. He said searches using the Shodan computer search engine and other evidence indicated that, since December 25 and January 8, the customer data was accessed multiple times by multiple parties, including criminals who ultimately held the data for ransom. The recordings were available on an Amazon-hosted service that required no authorization to access.

I'm going to be honest. These things are scary powerful in what they can do. Even if it doesn't expose your messaging, it could still be used for something malicious like a DDOS attack. If I were you, I'd pass on anything like this unless there's something explicit about security.
",12,2017-10-31 18:09:50Z,13
2,4,"
This is pretty much the same kind of toy as CloudPets. Those were toys that allowed talking with the children (toy) by using a mobile app. The security was terrible. It turned out that both the user details and the pet recordings were available on databases with no password. And the company didn't even answer to the mails alerting them of the vulnerabilities.
You can view about this terrifying story on Troy Hunt blog: https://www.troyhunt.com/data-from-connected-cloudpets-teddy-bears-leaked-and-ransomed-exposing-kids-voice-messages/
Now, Talkies may actually have made the right choices (it's hard to do so many things wrong as CloudPets did!), but this showcases the level of security of this sector.
So no, I wouldn't trust this toy with my kids data. Not to mention how the toy itself could be compromised (eg. like Hello Barbie).
In fact, Germany went so far to ban the internet connected Cayla dolls over fears they could be exploited to target children: https://www.telegraph.co.uk/news/2017/02/17/germany-bans-internet-connected-dolls-fears-hackers-could-target/
",6,2017-11-01 17:29:50Z,14
2,5,"
Internet-enabled anything poses a risk.  As a rule, security is an expense and consumers as a whole really don't consider product security when making purchasing decisions.  For example, there was a thread on Reddit recently about a couple who got divorced and she didn't change the password on the Nest thermostat.  So while she was out, he would crank up the air conditioning or heat and cause massive utility bills.  We also know of baby monitors that have been used to listen in on neighbors without their consent.  I've attended IT security demos of internet-connected light switches, showing how easy it was to attack them.
krack is important, definitely, but when compared to a non-existent security posture is irrelevant.  Quite simply, if someone is concerned about security, I'd suggest not purchasing networkable anything unless they can identify a need for it to have a network connection and they have the skills to properly secure both it and their network.
WRT your trusted circle of phones: how often would you plan on managing that list?  What is the means to join that trusted circle?  Do you know when your friends sell their phones back so you can decommission them from your circle? (If your answer is not ""no"" to the last one, you're probably not being realistic with yourself.)
Encourage creativity.  Build skills.  Get the kid a bunch of building blocks or a train.  Get a Spirograph.  Play cards/games with them.  Find something that they will play with for hours that doesn't require your constant attention.  
",4,2017-10-31 17:22:04Z,15
2,6,"
This puts the ""IOT"" in ""IDIOT!"".
Most of the companies that make these have no clue how to prevent hackers from taking them over, sometimes programming comically stupid/obvious exploits into them.
The KRACK exploit might be irrelevant half the time since most of these manufacturers wouldn't figure out how to implement some form of encryption.
Any type of internet-enabled voice recording is potentially creepy and downright dangerous invasion of privacy. These devices likely use the cloud for sound processing and storage considering they are almost certainly based on low-grade ARM chips and minimal cheap flash storage at most.
Even if the device is properly made, there's no similar guarantee on the cloud app it uses. You'd be surprised how often researchers happen to stumble on valuable leftover data in the cloud that the previous user of a logical machine instance failed to clean up.
",2,2017-11-01 04:44:14Z,16
3,1,"
Some people argue that code which is open source can be audited by many and therefor contains little bugs. On the other hand, attackers have the same easy access and also look for these same vulnerabilities. There is definitely a tradeoff here which is not correctly described in previous answers.
Others mention that code should be inherently secure and therefor requires no obfuscation/encryption/hiding. It is true that a system should be designed to be secure even if you know how it works. This doesn't mean that this is always the case AND the implementation is flawless. In practice, code is never 100% secure. (Take a look at web app security: Why do we need security headers to protect us against XSS and CSRF attacks if there are no vulnerabilities in the web application?) Additional security measures can be taken by trying to hide the code through encryption and obfuscation. In the mobile world, reverse engineering is even seen as a serious risk: OWASP Mobile Top 10 risks. 
As no system is 100% secure we can only try to increase the effort required to break it. 
So now, the tradeoff between open source/easily available code VS encrypted and obfuscated code. Allowing public review on you source code can help reduce the number of bugs. However, if you are a small company where the public has little incentive to freely audit your code, there is no benefit from publishing your code as nobody will look at it with good intentions. However, it becomes much easier for attackers to discover vulnerabilities. (We are not talking about the newest iOS version which every security researcher is trying to crack)..
In this case we aren't even talking about open sourcing the code for public review. We are talking about encrypting the firmware in transit. Security researchers are not likely going to buy your device to obtain the code to discover and publish vulnerabilities. Therefor the chance of having the good guys finding the vulnerabilities VS the bad guys finding them decreases.
",5,2017-09-05 07:50:19Z,18
3,2,"
No. You should not rely upon the obscurity of your firmware in order to hide potential security vulnerabilities that exist regardless of whether or not you encrypt/obfuscate your firmware.
I have a radical suggestion: do the exact opposite. Make your firmware binaries publicly available and downloadable, freely accessible to anyone who wants them. Add a page on your site with details on how to contact you about security issues. Engage with the security community to improve the security of your product.
",86,2017-09-01 13:38:54Z,21
3,3,"
Doubtful it would be beneficial. It is by far a better option to push it open-source than closed source. It might seem silly and even controversial at first, but opening up a project to the public has plenty of benefits.
While there are people with malicious intents, there are also people wanting to help and make the internet a better place. Open source allows more eyes to look over the project, not only to view about potential features, bugs, and issues but also increase security and stability of the ""thing""
And to agree with Polynomial's answer, engaging in a community and building a base of people that help you out with security, will increase the client base by a significant margin.
",11,2017-09-01 13:52:35Z,22
3,4,"
A well-designed firmware should rely on the strength of its access key rather than relying on the attacker's ignorance of the system design. This follows the foundational security engineering principle known as Kerckhoffs's axiom: 

An information system should be secure even if everything about the
  system, except the system's key, is public knowledge.

The American mathematician Claude Shannon recommended starting from the assumption that ""the enemy knows the system"", i.e., ""one ought to design systems under the assumption that the enemy will immediately gain full familiarity with them"".
You may be interested to know that prior to the late Nineteenth Century, security engineers often advocated obscurity and secrecy as valid means of securing information. However, these knowledge-antagonistic approaches are antithetical to several software engineering design principles — especially modularity.
",6,2017-09-01 18:33:32Z,24
3,5,"
Are you sure you are not confusing two cryptographic methods?
You should certainly sign your firmware updates for security reasons. This allows the device to verify they are from you.
To encrypt them adds a little bit of obscurity and that's it. Since the decrypting device is not under you control, someone sooner or later will hack it, extract the decryption key and publish it on the Internet.
",3,2017-09-02 05:49:21Z,25
3,6,"
You need to ask yourself this question.  Is someone clever enough and interested enough to download your firmware and start looking for vulnerabilities going to be deterred by an additional firmware encryption layer where the key must be revealed?
This is just another hoop to jump through, no different than figuring out what disk format your firmware image is in.  This isn't even a particularly difficult hoop to jump through.  Keep in mind that all FAR more sophisticated methods of what amounts to DRM have all been broken.
Odds are someone determined enough to hack your internet connected coffee maker/Dishwasher isn't going to be deterred by an additional encryption layer.
",0,2017-09-01 19:40:07Z,26
3,7,"
In the sense of ""will the encryption of firmware prevent the detection of vulnerabilities in my code?"" other answers have addressed the core of it: although it may discourage some attackers, security through obscurity leads to a false feeling of invulnerability that is counterproductive.
However, I'd like to add an extra bit on the basis of my experience. I have seen that the firmware packages are sometimes encrypted, but the motivation for that is only to preserve a company's intelectual property, rather than be a control against attackers.
Of course, hackers often find ways around this ""control"", but that's a different story.
",0,2017-09-08 06:53:14Z,27
3,8,"
Several people said you shouldn't rely on obfuscating the code by encrypting it, but to just make it secure. Quite recently the encryption of some rather critical software in Apple's iPhones was cracked (meaning that hackers can now see the actual code, no more). It stopped anyone from examining the code for three years, so the time from release to first crack has been increased by three years. That seems like some very successful obfuscation. 
And encryption goes very well together with code signing. So when your device is given new firmware, it can reject any fake firmware. Now that part isn't just recommended, that is absolutely essential. 
",-3,2017-09-01 21:03:46Z,28
4,1,"
Almost all IoT devices of the sort we are talking here (ie, consumer level) are deployed behind home grade routers (or ARE home grade routers) and are using IPv4 and NAT/uPNP to reach the internet.  That means they are sharing an IP with legitimate traffic for one and for another are using a dynamic IP, not a static IP, meaning there's even more opportunities for innocent parties to be caught up in such a blacklist.
Once the glorious world of IPv6 comes, we can likely do just that - assign each one a routable IP address and blacklist them as they get shown to be vulnerable.
One legitimate thing that CAN be done is for ISPs to perform source IP filtering - that is, don't let spoofed packets out, only let packets from IPs within the network.  That will help. Some.
",14,2016-10-26 17:08:31Z,33
4,2,"

wouldn't it make since to start giving IOT devices a static IP

No, because IPs only work in the proper network context.
An device's IP needs to be routable, which is to say, sitting on a network with routers that know how to communicate with other networks and are willing to do so on behalf of the device.
First of all, if you hardwire IPs and send them out, you'd have to use RFC1918 IPs (non-routable) to avoid conflicting with IPs owned by other people.  So you're already stuck with a narrow slice of addresses you can use.  And those IPs would need to be translated when they hit the Internet, thus removing the ability of the target to filter on them.
Second of all, you'd be handing people devices with IPs that may or may not match their networks.  They wouldn't want to rework their networks to accommodate your cheap IOT devices.
In short, a device's IP is set to allow it to work on the network.  Setting the IP and expecting the network to work with it breaks things.
",6,2016-10-26 17:07:52Z,35
4,3,"
The way IP addressing is done for IOT devices doesn't matter in a DDOS attack. Here is the reason why.

IOT devices may not always be exposed to network with an external IP address. 
It can also be device behind a NAT which has only an internal IP
address.  
So irrespective of whether it is DHCP or some static IP, provided by  the router to the device, from the DDOS attack victim's
perspective, the source of the attack is the external IP of the
router. 
In other words, if you have an IP cam at home, The packets from your IP cam as well as your laptop will have the same source IP address, ie the public IP address of your router. 
So filtering the packets based on source IP of the packet can deny service for your laptop as well.

",4,2016-10-26 18:06:32Z,36
4,4,"
No, as dynamic addressing isn't the attack vector when IoT devices get abused. It's poorly written software! On the DoS side, we have the 'underpowered' device vs. powerful computer problem, and on the connectivity side we have the ease of use vs. security problem.
Ironically, the only way to do proper IoT is to not connect them to the internet! At least a gateway plus locally configured outgoing tunnel would be needed before they can be connected at some sort of safe level.
",1,2016-10-26 17:49:55Z,37
4,5,"
A static IP would not help; as crovers mentions, due to ipv4 exhaustion, its just not practical. Although IPV6 provides a large enough address space for each device to have a staic IP address, there will still be a lit of overlap with the IPV4 world - where a server will just see lots of traffic arising from a point of presence for IPV6 nodes.
But there are a lot of things manufacturers and governments can do (to make up for the lack of skills on the part of the buying public). Blocking traffic ther than from/to the local network on an appliance by default, forcing a change of admin password before the device, using encryption appropriately, supporting open standards.
The goverment part is more tricky - the obvious solution isproviding an accreditation framework which is easily auditable but minimally invasive with minimum standards, the problem is that they tend to listen to either the ""experts"" advocating (for example) that all such devices should be fips-140 certified or the vendors bleating about customer freedom and over-regulation rather than seeking pragmatic solutions.
Maybe someone might set up an IETF to look at the classification problem.
",1,2016-10-26 18:12:42Z,38
5,1,"
I would suggest that rather than trying to protect the camera from the outside world with a firewall that using a black hole router as the only gateway for these devices would be more restrictive - also if you have a physical device at the router's address then it would make monitoring of egress attempts more visible. Also, point the device to the same black hole for DNS queries.
If you need remote access then use a VPN with NAT.
",3,2016-10-25 15:28:05Z,38
5,2,"
The following controls should help secure your device:

Change the root password (if possible: See @crover's comment for more info)
Segment the network so that only the host administering the camera can connect to it
Block egress from the network from the camera's IP
Apply any vendor supplied patches or configuration changes

",2,2016-10-25 15:17:20Z,40
5,3,"
There are a few things to do to protect your camera:

Make sure you’re not using the default password. This way, even if you’re discovered, it’ll be harder to break in. Especially change any root passwords, where possible.
Incapsula has a Mirai vulnerability scanner, which scans your IP to see what may be at risk. https://www.incapsula.com/mirai-scanner.html They also offer a Web Application Firewall (WAF) to put your device behind.
Disable any unnecessary remote connections. 
Run the most up-to-date firmware to ensure discovered security vulnerabilities are patched. But realize that there are many unpatched vulnerabilities that remain. This is something that we are less likely to do for devices than our own home/work computer. 
Reboot and make sure everything still works.

And, as you mentioned, in the wake of Mirai, understand that your device is at risk even if you personally  aren’t a target.
",1,2016-11-13 13:19:44Z,42
5,4,"
Other than nagging your vendor for an update or replacing the cameras with something more secure, the only other option would be putting them on their own network, with limited and controlled access via another router.  You can do this with some higher-end switches which support segmentation or you can just plug them into their own switch, with a firewall/router between, which limits access to those cameras only to the 'secured' ports.
",0,2016-10-25 15:17:33Z,33
5,5,"
I would put them on a separate network segment with no access to the Internet. The only access would be through a VPN gateway. You not only protect them from attacks but also secure your connection to them (as those cheap cameras don't support HTTPS).
In the future I would also recommend buying cameras from a reputable vendor with a good security track record (a network company such as Cisco/Ubiquiti).
",0,2016-11-13 14:32:07Z,43
6,1,"
The devices are designed to be accessible from outside the home. To offer this service to their owners, they make themselves accessible through the homeowner's router/firewall. The way they do this is by sending a UPnP packet to the owner's router that tells the router to open a port that connects back to them. They then listen for connections that arrive directly from the internet.
In other words, the devices first hacked their owner's routers by design, which exposed their own vulnerabilities.  (This has nothing to do with secured, private, or open WiFi, other than many IoT devices connect via WiFi; UPnP exposes the exact same vulnerabilities on wired devices connected by Ethernet cables, too.)
To protect yourself, disable UPnP on your router.
",84,2016-10-25 02:59:31Z,48
6,2,"
Your understanding of the attack is not as clear as you think. In this article, Krebs mentioned that the attackers didn't really have to hack the devices. The vulnerability was well known, they just had to scan the internet for those devices.
Sure, if SSH/Telnet to the devices was disabled, the problem would have been solved easily. To make the matter worse, the hard coded credentials present in the hardware were not even visible to the web interface for the administrator.
Yes, it is absolutely imperative to know what are the devices present in your network and what are the services that you do/do not need.
EDIT : After @tlng05 's clarification about the question.
As already mentioned in other answers, you should disable UPnP on your router to absolutely make sure that your device is not straight forward configurable from the outside world.
",15,2016-10-25 03:03:16Z,51
6,3,"
Your misconception is here:

secured private wifi networks

Whilst many home WiFi networks are secured against unauthorised wireless devices connecting directly, many are wide open to access from the wider Internet.  It's this access (that's demanded by the IoT devices to perform their legitimate functions) that can be abused (and on a much bigger scale than physically visiting many WiFi networks).
The attack surface of a router is on both all networks!
",10,2016-10-25 09:00:47Z,53
6,4,"

What I don't understand is although easily hacked, most IoT devices are connected to secured private wifi networks.

Yes they are connected to your private wifi networks, But are they secured? Well not so much as pointed by you these device are unprotected by firewalls, IPSs unlike the enterprise networks. Some of them have ancient firmwares, which haven't been updated since ages. And yes some have default passwords still working, So that anyone can easily take access and exploit them for attacks.

So is it assumed that these thousands of IoT devices' networks were hacked first, then the device itself was hacked?

Well not necessarily, Although it may be possible in some cases. But mostly these devices are intentionally left exposed to the internet because they are needed to be accessed from anywhere around the world.
As pointed out by many examples above, If you want the CCTV footage of your house mostly you would want it live streamed on your handheld device and that is why they are needed to be accessible over internet. They are N number of other examples. 
Conclusion: To use IoT devices to attack, one doesn't need access to your network. These devices can be directly accessed from internet. What we need to do is protect these devices from such un-authorized accesses and keep our devices safe without having to use expensive devices like firewalls and IPSs.
",2,2016-10-25 08:26:14Z,55
6,5,"
UPnP can be an issue, but everybody seems to be missing the point that many of these devices make persistent standard outgoing NAT connections to the vendors' servers. All the attacker has to do is hack into the vendor's site to gain control of all of the attached IoT devices, and from there, since they are now inside home networks, to attack other computers inside the network or launch DDoS attacks. Direct HTTP, SSH or other UPnP-enabled access through your router isn't necessarily a requirement. 
",2,2016-10-25 14:45:22Z,57
6,6,"
While IoT devices are indeed within secure networks, they are largely made such that they are accessible from the internet. For example, the temperature setting of your home is accessible from your phone app when you're at work. This is enabled by a connection being opened up to the internet. This answers why they're able to access the outside world. 
Now, most IoT devices, or botnets, are not well patched and use loose security configurations. Parts 1 and 2 of the article found here explain this in detail, but to summarize, these devices are infected with malware. They are able to send outgoing messages to the internet (the outside world). And thus, they end up sending the ""DoS"" message to the target. 
",1,2016-10-25 03:14:10Z,58
6,7,"
Most IoT devices are on networks that are connected to the Internet by conventional SoHo NAT routers that typically have very limited firewall capabilities or where the firewalls are not enabled or maintained. There is a common myth that NAT is a security layer, it is not.

""NAT and firewalling are completely orthogonal concepts that have nothing to do with each other. Because some NAT implementations accidentally provide some firewalling, there is a persistent myth that NAT provides security. It provides no security whatsoever. None. Zero."" -- How Important is NAT as a security layer?

",1,2016-10-25 18:46:34Z,30
6,8,"
It may be worthwhile thinking about terminology and what is meant when people say that IoT things have been 'hacked'. In many cases, the devices have not been hacked at all - they are performing as designed. 
Broadly speaking, there are two types of network connections. The first type is a fully connected type connection where both parties need to be fully connected. Similar to a phone call, you need to have someone on both ends. With this type of connection, the initiating system makes an initial connection to the destination system and the destination system connects back to the initiating system. This type of connection is what normally occurs when it is important to be able to coordinate communications, track data packet order and request re-sending of any lost data. 
The other type of connection is more like a messaging connection (think of SMS or some other messaging In this type of connection, you don't have a bi-directional connection. The originating system sends a message to the destination system and, depending on the message, the receiving system may send back a response to the sender address in the initial message. This type of communication is good when order of data, loss of some data etc is not critical. 
The thing is, while fully connected connections are great for things like data integrity and because of the bi-directional nature, are difficult to spoof, they are more expensive in terms of resources and overhead. The second type of connection has less integrity and is easier to spoof because there is no bi-directional connection, but they are cheap - require less resources and have lower system overheads to process. 
Many IoT systems are small, lightweight and need to be efficient. They typically have less memory and less powerful processes and therefore tend to favour designs which use connecitonless protocols rather than more expensive connected protocols. However, this also means that it is easier for rogue systems to 'lie' and do things like spoof IP addresses. This is like me sending you a message, where the return address is false. When you reply to the message your reply will go to the address in the message, but that is not the real originating address. 
In effect, what is happening is that the IoT devices are being folled into sending data/responses to an innocent bystander who has not requested anything. The system has not been 'hacked', only fooled. 
Often, the situation can be made worse by using amplification techniques. There are some connectionless type services out there which, when asked a vary simple/short question, will respond with a vary long answer i.e. answers with lots of data. This can make it vary easy to create a situation where suddenly, a victim site (such as a DNS) suddently starts receiving large amounts of data it was not expecting or did not ask for. 
to do this, all you need to do is identify devices on the internet which support a connectionless protocol, send these devices a message which requests something which is likely to involve a large data response and spoof the IP address of the targeted victim. 
to make it worse, the targeted system doesn't even need to know or understand the data being sent to it. The idea is to just send so much data that the system becomes overwhelmed - that could happen when the system is forced to look at large amounts of incoming data simply to make a decision to discard it and take no further action. With enough data, even that process of working out you need to just ignore it can be enough to prevent the system from being able to process legitimate connections.  The fact that this data is comming from multiple different source systems i.e. all the IoT deices means you cannot just block an IP address because there are simply too many. 
So, while it is vary true there are far too many IoT devices which ahve been poorly designed and lack sufficient security controls, a part of the problem is the conflicting requirements to implement a light-weight resource efficient solution on one hand, but somehow deal with a world with too many malicious agents who want to exploit your good intentions. There is certainly a lot IoT vendors could do to improve the situation, but for most of them, this would just increase production costs and the reality is, most consumers are not aware of the issues, so failing to invest in the better solution doesn't affect market share and therefore doesn't result in sufficient financial benefit.  
",1,2016-10-27 23:11:01Z,60
6,9,"
XM actually disabled telnet/ssh on many of their IoT devices (they supply many for new DvRs, webcams, etc.) more than a year ago. So anyone who had actually updated the firmware (who knows) or had bought a more recent model of (IoT device whatever) since then likely would have been immune from that sort of attack.
My understanding is the Mirai (not sure about the other popular one Bashlight) connected to most IoT devices through GRE - an IP point-to-point virtual tunnel. GRE is kind of like a VPN of packet delivery - it can pass data through public network privately - without the actual data/headers being identifiable and with almost no protocol overhead. So once you have a master list of exploitable cams, home sec, connected whatever devices and the models, you can scan the whole internet and tunnel IPs accessible through open ports, run against passwords, etc etc. Hard for people to see it coming because GRE looks like regular IP transmission between devices calling home or streaming home video to app, etc. This is just my take...
",0,2016-10-26 01:31:37Z,61
6,10,"
New in this forum I thought I'd chime in from a hobby IoT device maker's perspective. I may very well be off topic, not least since I am not entirely sure what you guys even consider to BE an IoT device, but for what it's worth:
The ""IoT devices"" I create, which do useless things such as report whether or not someone has moved within a certain area in a certain time, could easily be ""hacked"" without accessing my WiFi. You could probably just put up a receiver of the right sort (we could be talking 433MHz) and eavesdrop all day. Then you could craft your own messages and send them to my stupid device and/or the server that collects that information, and have me running home in panic since my not-so-smart-home system says it's 200 degrees Centrigrade in my fridge and five thousand people have passed into my garage but noone came out.
Basically what I'm saying is that whatever flaws the IoT devices hardware exposes directly, and it's software doesn't guard against, could be an entry port for a hacker. Heck, based on where the device is placed you could even attach your own hardware to it and start making trouble. ""Here's my WiFi enabled ESP8266, go ahead and upload your own software to it via USB."" But I guess that's really out of scope.
",0,2016-10-27 11:40:59Z,62
7,1,"

... but they used their own infrastructure

It's not really their own infrastructure what they use. They use instead botnets consisting of  hijacked systems. These are systems which they p0wn but definitely not own. And thus it is very cheap for them.
Apart from that any VPS provider who would rent their VPS for DDoS attacks would quickly lose reputation and thus proper customers. And if a VPS provider then specializes on providing VPS for DDoS attacks to make up for the loss of normal customers it would be more easy to block such DDoS because they all origin from the same networks, i.e. simply cut off this provider from having access to major networks.
",40,2017-04-25 04:54:20Z,66
7,2,"
Cloud based DoS attacks are possible, and they do happen from time to time. But it's not a very popular option for a couple of reasons:

Initial setup - Deploying hundreds of VMs is not an easy feat, and paying for them isn't simple either. However if you're using someone else's VM, then this makes things a lot easier. 
Detection - Many providers including Azure monitor their services to check for any malicious activity. In fact, launching attacks from their systems violates their ToS, and will have you shut down very quickly.

However, the ability to have thousands of machines spread over the world, each generating some traffic can be very powerful. If you want to take it a step further, tunnel your traffic through the Tor network, to make it nearly impossible for a defender to stop. 
It's been done before though:

In 2012 group of cyber-criminals exploite  the CVE-2014-3120 Elasticsearch 1.1.x vulnerability, followed by the use of Linux DDoS Trojan Mayday and with that, they compromised several Amazon EC2 Virtual Machines. Although this vulnerability was not unique to cloud-based systems and could have been used against any server, including non-cloud based systems, it did open up some interesting opportunities to the attackers. They were able to launch a UDP based DDoS attack from the compromised cloud instances. They utilized the outbound bandwidth of the Cloud Service Provider, Amazon in this case. 
  Source: Infosec Institute

Getting caught - this is more difficult. Creating and deploying VMs these days is as easy as signing up for an anonymous email ID, registering and deploying machines. However, providers will notice large amounts of traffic from a system. Since you're​ violating their ToS, they will nearly always shut you down immediately. However since you're not ever revealing your actual identity (assuming you're accessing their services through an anonymizer and using stolen credit cards (no morals ;] )), they mostly will not be able to discover your real identity. But this does mean that you're flushing your money down the drain - which is why it's just simpler to set up your own infrastructure and offer it as a service. 
",9,2017-04-25 04:48:58Z,70
7,3,"
The funny thing is that what you describe is available right now. It's called Mirai, it's more or less open source, and chances are you've already been affected by it

Mirai is a type of malware that automatically finds Internet of Things devices to infect and conscripts them into a botnet—a group of computing devices that can be centrally controlled. From there this IoT army can be used to mount distributed denial of service (DDoS) attacks in which a firehose of junk traffic floods a target’s servers with malicious traffic. In just the past few weeks, Mirai disrupted internet service for more than 900,000 Deutsche Telekom customers in Germany, and infected almost 2,400 TalkTalk routers in the UK. This week, researchers published evidence that 80 models of Sony cameras are vulnerable to a Mirai takeover.
These attacks have been enabled both by the massive army of modems and webcams under Mirai’s control, and the fact that a hacker known as “Anna-senpai” elected to open-source its code in September. While there’s nothing particularly novel about Mirai’s software, it has proven itself to be remarkably flexible and adaptable. As a result, hackers can develop different strains of Mirai that can take over new vulnerable IoT devices and increase the population (and compute power) Mirai botnets can draw on.

There's lots and lots of IoT devices flooding the market. Everyone wants ""smart"" technology. But, as is usually the case, security is an afterthought. So we put that device out there and it's on the Internet for anyone to contact and use as they see fit. And most users (and indeed ISPs) probably won't notice 

Fast forward another 45 minutes. The router was reset, and the network was set up again. By the time I was done messing around, Peakhour had my traffic clocked at 470GB. But I'd gotten rid of the problem (or so I thought). The next morning, before I left for the weekend, I checked: the total traffic was at around 500GB. Maybe I'd defeated the hackers.
That night, I heard from Donna. She'd been monitoring traffic, which was now over 3TB. And, just to make sure we had no doubt, devices were dropped off the network again.

The tipoff that something was amiss? His phone used all of it's 4G allotment despite his being at home. His ISP never batted an eyelash at 3TB of bandwidth consumed

Ultimately, you can count on people who are technically illiterate and companies that don't care to provide all the botnet devices you'll ever need.
",2,2017-04-25 16:38:18Z,13
7,4,"
Cloud providers generally require their customer's identity. If an enterprising young hacker wished to rent Amazon Web Services or the like, they would have to provide a credit card number (or more) to the service, which can be traced back to the owner. Cloud services don't want to engage in DDOS because their networks would be blocked, and it would cost them money in bandwidth. 
There are services where you can rent a VPS anonymously in Bitcoin, but they are generally smaller and they also don't want to be blocked by their uplink or peers. 
So that is why it isn't common. DDOS generally considered anti-social behavior, and sociopaths only make up 4% of the population.
",2,2017-04-25 17:11:10Z,72
7,5,"
The key to your question is in the first 'D'. 
What makes a DDoS attack so effective is the distributed nature of that
attack. With an old style DoS attack, the victim would usually experience a
large number of requests or connections to a specific server or resource
originating from a single or small number of sources. To mitigate the attack,
you could simply block the traffic from the attacking systems. Often, this could
be done by the local firewall or similar. 
Under the DDoS attack, the victim is flooded with requests from a large number
of different sources. The number is too high to block individually and the
volume of attackers will typically overload all local infrastructure such as
firewalls and network switches. In this scenario, you typically need to work
with your ISP to have traffic to the target system sent to a 'black hole', which
will reduce the volume and allow local infrastructure to recover, but typically
does mean that the DDoS on the specific system is successful (because traffic to
that system is being sent to the black hole). The key point is that because the
attack is distributed, it is very difficult to block the attacking systems. 
So, with respect to your question regarding cloud based DDoS services - this to
some extent depends on your definition of cloud. One definition would be any
service that is not on your own infrastructure and is delivered from 'the
cloud'. In this sense, DDoS attacks are already cloud based. They don't use the
attackers own infrastructure, but instead, use hosts the attacker has either
compromised or hosts the attacker has identified which have either poorly
configured services or lack sufficient controls to prevent them from being used
as part of a DDoS attack. For example, one of the reasons there is so much
concern surrounding IoT is that many of these IoT devices include services which
can be exploited as part of a DDoS attack and lack sufficient controls to
prevent this exploitation by unknown remote uses. 
If you define cloud to be just IaaS, PaaS and SaaS providers, the situation is
slightly different. You are unlikely to see these services being used to perform
the actual attack simply because the DDoS attack relies on high numbers of
attackers and being able to use that number of cloud providers is prohibitive -
remember that the cloud providers are not going to welcome this sort of use of
their infrastructure, so you will have to do it in a 'stealthy' manner, which is
becoming increasingly difficult as cloud providers lock down what is considered
appropriate use of their infrastructure (remember, they have a reputation to
maintain - if they become known as a host for 'bad actors', ISPs and others will
just block traffic from their IPs). 
This doesn't mean attackers don't use cloud services. What  you will often find
is that DDoS service providers will use cloud services as the command and
control centre for their DDoS agents/bots. They still need to do this in a
stealthy manner as most reputable cloud services will deactivate any users they
detect doing such things, but this is much harder to detect and they only need a
few cloud providers. The agents/bots they use to actually perform the attacks
are usually compromised desktops and servers, often in home systems which have
poorer security controls and increasingly IoT devices, many of which are also in
home or small office environments which lack enterprise security measures or
skilled system administrators etc. 
",0,2017-04-28 01:22:35Z,60
8,1,"
I answered a similar question here.  For a normal hardware solution, a hardware security module (HSM) does this.  For software, .NET offers the Secure String mechanism to encrypt and protect sensitive data in RAM. Other platforms may offer something similar.  For an AWS solution, Amazon offers CloudHSM to do pretty much what you are asking for, I think.
From AWS:

HSM is short for Hardware Security Module. It is a piece of hardware —
  a dedicated appliance that provides secure key storage and a set of
  cryptographic operations within a tamper-resistant enclosure. You can
  store your keys within an HSM and use them to encrypt and decrypt data
  while keeping them safe and sound and under your full control. You are
  the only one with access to the keys stored in an HSM.
The AWS CloudHSM service brings the benefits of HSMs to the cloud. You
  retain full control of the keys and the cryptographic operations
  performed by the HSM(s) you create, including exclusive, single-tenant
  access to each one. Your cryptographic keys are protected by a
  tamper-resistant HSM that is designed to meet a number of
  international and US Government standards including NIST FIPS 140-2
  and Common Criteria EAL4+.

",6,2016-05-03 18:14:43Z,74
8,2,"
While everyone else has suggested a hardware solution, I'll suggest a software solution (though I don't mean to imply that it's any better or worse than using an HSM - that you'll have to decide on your own based on your own needs). This solution is a kernel patch which encrypts processes.
RamCrypt is a project which encrypts the majority of the memory of individual processes with AES128 in XEX mode, so all secrets in the processes are safe in memory, even if all of the memory is forensically acquired. The memory is encrypted with TRESOR, so the encryption key is never present in RAM. It keeps the key in the x86 debug registers, so they are out of memory, and does all the AES computation using AES-NI, general registers, and SSE registers, so it never enters main memory or CPU cache. The key is then used to encrypt and decrypt memory pages for an entire process. Only 4 pages are kept unencrypted, and all others are encrypted. When an encrypted page needs to be accessed, it is decrypted using the AES key, and one of the 4 unencrypted pages is encrypted. RamCrypt comes as a kernel patch, and you can adjust the number of pages which remain unencrypted to choose a trade-off between performance and security. See the project's main page, and the associated research paper.
",3,2016-05-03 22:17:47Z,76
8,3,"
I'm not sure if I get you right. But at some point the plain secret has to be in some sort of memory (most likely the RAM). If the attacker can read the memory at that point and if he is able to find the secret he can decrypt the data afterwards.
I think theoretically there can't be a way (maybe with some special security module that contains your program code) to save the secret in memory without any chance the attacker can find it. There is always an algorithm that can find the key (there has to be because you want to use the key in your own code). But as pointed out in the paper it can get really really hard to find the key. 
Just a side note: I also don't see the use case of generating a random secret that no one knows and then not store it anywhere.
Cheers
",1,2016-05-03 07:21:37Z,78
8,4,"
Fundamentally, there is nothing that can solve this issue in the absolute. You're running into the DRM problem, which is that you need plaintext access to some data on a system in order to fulfill some functionality, but you're also placing that system under the control of a party which is untrusted. The best you can do is make it incredibly difficult for all but the most determined and skilled attackers, essentially making it a poor a cost/benefit trade-off for them.
In terms of solutions which do make it hard, devices such as Hardware Security Modules (HSMs) should be pretty much top of your list. They utilise a range of features designed to make it exceedingly difficult to recover information from them illegitimately. For example, a common feature is to encrypt data with a key stored in volatile memory (e.g. DRAM) on a separate board, physically attached to the upper casing of the device. The power and data connections for this board are supplied via a contact connector (often made from conductive polymer or foam) via pads on the main board, which is physically attached to the lower casing of the device. If you attempt to open the device, you separate the boards, thus disconnecting the power from the DRAM and losing the keys. Additional sensors such as light, temperature, pressure, acceleration and position (incl. GPS) and even magnetic fields or radiation can be present in order to detect different types of tampering.
One additional potential system which may, in some circumstances, be of use to you is the concept of homomorphic cryptosystems. Homomorphism, in laymen's terms, is essentially the property of a cryptosystem that allows certain operations to be performed on data in its encrypted form, without needing to first decrypt the data. There are a variety of schemes available, some of which are even practical for certain types of scenarios, but they are usually quite complex in nature and can be particularly slow. I am unaware of any existing cryptosystem which provides homomorphism in a way that would be conducive to cloud storage applications.
",1,2016-05-03 19:20:38Z,21
8,5,"
It depends.
Other posters suggest an HSM.  This can be a good solution for a limited set of purposes where you temporarily need access to a key, it's erased from active memory after, and the key to the HSM itself isn't stored in memory long-term.  
It sounds like you need the secret to be active in memory all the time, and not stored in an HSM most of the time.  In that case, I don't see much value in the key storage capabilities of an HSM, since it can't protect keys already stored in memory.
And HSM can also perform the cryptographic functions in the module itself.  Whether this is useful to you or not would depend on your application.
",1,2016-05-03 21:09:27Z,26
9,1,"

Google has access (obviously).
The police will have access if they have a valid search warrant.
A national security letter will give the FBI secret access.
Various three-letter agencies may have access, depending on how they're doing at circumventing Google's encryption.  (Google started encrypting its internal traffic after it was revealed that the NSA was monitoring it.  Modern encryption, properly applied, is believe to be sufficient protection against three-letter agencies -- all known attacks are against the ""properly applied"" part rather than the encryption itself.)

As for deletion, Google uses a highly distributed storage system.  I don't believe they will intentionally keep data after you delete it, but because of how Google's storage works, residual copies may stick around for a while.
",48,2014-07-07 08:25:35Z,3
9,2,"
Not instantly. Although, that's what I want to believe. What you could do is the following. Download the Truecrypt version 7.1a and create an encrypted storage file (option 1 from the wizard) and choose 3 algorithm based encryption with a SHA-512 key. Put all your sensitive files in here and upload the encrypted file to Google Drive. When you want to work with your files. THIS IS IMPORTANT: Copy the encrypted file OUT of Google Drive, perform your work, encrypt and put the encrypted storage file back on Google Drive. This should keep your data safe.
If you want to delete it, just delete the encrypted storage file, nothing they can recover without your key.
PS: Since a short time you can ask Google to explicitly wipe your search data. If you just delete your search history through your user control panel in google than it is still used for advertisement. (scumbag, I know)
",15,2014-07-07 08:29:02Z,85
9,3,"
Any data you upload to Google Drive (or Skydrive, or Dropbox for that matter) should be considered duplicated by the NSA. Apart from arbitrary queries from the aforementioned secret service, law enforcement agencies from any country may gain access to them through legal means (subpoenas and so on). And of course, Google engineers could in theory browse your files.
As for what happens when you delete them, who knows? It is a good working assumption to consider that any data uploaded will stay on their servers forever. Realistically, I suppose it does get deleted at some point, but considering how much replication they must have, it may be a while. Your guess is as good as mine.
Bottom line, it is not trivial to set up off-site backups for confidential data. The only thing you can trust (or hope to trust) is cryptography. Look into duplicity and other backup tools which support strong encryption!
",10,2014-07-07 08:28:07Z,87
9,4,"
What about hosting those files on other hosting services which have client side encryption like:

Tarsnap, which has open source client and does client side encryption.
Tahoe-LAFS
SparkleShare, which has GUI clients for all operating systems.
more on https://prism-break.org/

",6,2014-07-08 15:03:40Z,90
9,5,"
Cryptography is a form of smart obfuscation,
it does not make things ""secure"", just ""secure enough, for now"".
If someone REALLY is out to get you, he would store your encrypted data until a later date when encryption can be broken - anything from stronger computers to software vulnerabilities like ""heartbleed"" will do the trick to decrypt your stuff in the long game.
Also, chances are, if someone really is out to get you it would probably try to find a way to hack your computer, not your cloud account, there's plenty of such software around - some of it available to random 3rd world governments under shareware trial licenses for free.
",2,2014-07-07 18:33:36Z,91
9,6,"
To answer one of your questions: any Google employee who has administrative access to the hosts (file servers) on which the drive data is stored will have read-access to your data: this includes the operations engineers, service engineers, system administrators: these type of employees are typically the ones with that type of access.
The answer to your other questions depends on what methods Google uses to remove distributed data across multiple hosts, and whether they are using any backup methods to store copies of the data. Nobody other than Google technicians will know the answer to this question, and of those probably only the engineers who maintain the filers and the data distribution structure. 
To give you an idea of how truly ""privy"" that information is: I used to work at Yahoo as a Unix sysadmin, but I didn't work with the filer infrastructure and so even I don't know what the answer to that question would be for Yahoo data, although I spent a year working in the operations group for two of the departments there.. Only the engineers directly responsible for that infrastructure are going to know if the data is truly ""deleted"" or not (and to what level of deletion, if that makes sense to you).
most likely, the file system pointers to the data are deleted within one or two days (depending on how long it takes to propagate through Google's infrastructure), while the actual data itself continues to reside on hard disk until it is overwritten: meaning the data (or parts of it) could theoretically (and fairly easily if someone made the effort) be recovered many months later. 
Regarding other agencies or companies with access to this data? Nah .... the only way the NSA or somebody could access it is if they have employees working at Google. Unlikely. And even if they do, do they have root access on the servers in question? Not likely. As someone else mentioned, theoretically the government can get access through a subpoena process. But from a practical perspective do these people have willy-nilly access to John Anybody's data? Nah. 
The only people who theoretically intercept your data when you upload it are the system administrators and operations engineers who work at the companies that provide the network backbone for the internet: such as UUNet. I know for a fact these people for many years have scanned internet traffic for illegal content, because years ago when I lived in Virginia I interviewed with UUNet for a job which involved this work. They offered the job to me, but I didn't take it because I accepted a job elsewhere at the time.
",2,2014-07-08 00:06:37Z,94
9,7,"
This is more of a comment than an answer, but apparently I'm not allowed to comment yet. If you're concerned about the privacy of your cloud storage, you may want to consider Tresorit as an alternative to Google Drive. It offers client-side encrypted storage & syncing, so the Tresorit engineers aren't even supposed to be able to tell what files you're storing there, let alone their contents. It's also hosted in Switzerland to take advantage of that country's strong privacy laws.
Of course, it has the same downside as all true encryption, which is that if you lose your password, there's nothing they can do to help you.
",2,2014-07-08 18:26:21Z,95
9,8,"
Apart from statements like NSA has your data when uploading it is based on the assumption that your data being on your disk is not somehow accessible. Like others stated, if your data is encrypted in a strong manner on your disk and uploaded in the same way I would regard the online version more safe in terms of redundancy (google is managing the replication and backup of their cloud). Stealing or breaking your machine with your data then has no effect.
If your data is not encrypted you still assume at this moment, that your data is totally secure on your disk and not accessible from the NSA. Depending on your operating system this might not even be the truth. Even if you can guarantee that your OS is free of any spyware, there is a chance that you cannot say the same for the channels through which you obtained the data. Being uploaded to google certainly allows easier access to data but I would not regard it as a ""none to full exposure"" in that way.
",1,2014-07-07 13:28:53Z,96
10,1,"
Let's clear up a couple of misconceptions:

DDoS mitigation partners cannot work unless they can decrypt traffic

Yes, they can - they just can't do it as well, as the content is not visible, so the solutions which work on traffic flow will still work.

Handing your key to a 3rd party is a bad thing

Not necessarily. In fact this is extremely common for large corporates - who typically have their infrastructure managed by 3rd parties anyway. What it does do is require that you replace some of your technical controls with contractual controls, and that you take greater oversight/audit responsibilities over the 3rd party.
Mitigation of the threat of the 3rd party misusing the key, or not protecting it is a straightforward one to assess and then provide a solution for. 
",5,2013-11-13 15:34:13Z,99
10,2,"
SSL DDoS Attacks should be divided into two - 

Protocol misuse attacks - 
Such attacks exploit the protocol being used and can cause denial of service effect without completing the creation of secure connection (SSL in our case). A good example can be the THC-SSL-DOS which can be used to create repeating 'renegotiation' in the same connection, though never completing the creation of a secure channel.
Protocol attacks, like the one mentioned above, do not require having keys. They can be mitigated with relatively simple protection measures as IPS signature, enforcement of server settings, or even dedicated DDoS appliances Pravail/Riorey/DefensePro/NSFocus/etc. 
SSL Traffic Floods - 
Here I refer to data that is being passed over the created secure channel. Without further information, these magnificent mitigation devices cannot distinguish between valid connections to malicious connections. They cannot even issue web challenge in an attempt to try and assess source legitimacy. So you are either stuck with nothing or some rate-limit protection - prone to false actions.  

So, what can be done - 
Some appliances can be fed with the certificates or be connected to another appliance from same family (DefensPro-Alteon / Pravail-VSS) - where the sister product opens the encryption, and ddos device does what it does and the flow is allowed to continue or is blocked. 
Radware can issue a challenge on the initial encrypted request (and only on it). Arbor can continuously inspect encrypted traffic, and block suspicious traffic (no challenge). As long as you keep these machines at your place, the risks are rather low. 
Some cloud services offer you to give them your keys. Not very recommended practice. 
However, Prolexic lately published they can live with temp-short-lived keys to achieve the same. This might be a sufficient solution, considering all other limitations and restrictions when dealing with secured (encrypted) traffic. 
And last -  is it bad to provide your key? 
In a very basic manner - Your key is important to secure your communications. If someone has your key she can theoretically 'read' your communications, or impersonate to you while communicating with others. 
However, as someone answered before me, you can practice sharing or having copies of your keys at different places. Depends on your infrastructure, on who manages your network, etc. 
Above all it relates to TRUST. If you trust this 'born yesterday' DDoS protection in the cloud service - go a head, give them your keys :). There are companies that can be trusted and that put many efforts into complying with all regulatory requirements. It doesnt provide 100% insurance, but frankly, nothing does.   
",2,2013-11-14 13:14:09Z,100
10,3,"
All the vendors are going to give you the same answer: you need to hand over your key to them either for a cloud-based security product, or for a box that they supply that sits in your network. Either way you cannot completely mitigate the threat that the have your keys. 
The alternative is to mitigate the threats posed by individual attacks by:

Keeping your systems and applications up to date: many previously serious ddos attacks have been eliminated by OS and software updates
Modifying your configurations: you can often tweak OS and applications to be much more resilient
Offload SSL computations: there are several solutions in this area, often load balancers have crypto engines that can do the work

What you really need to do is perform (or have somebody perform) a threat analysis to find out your biggest concerns, then look for solutions to those. It may be a vendor solution is your best way forward, or you may be able to do just fine using tactical, zero or low cost solutions. 
",1,2013-11-13 12:40:02Z,101
10,4,"
Ichilov's answer is more relevant.
Two non-vulnerable SSL Attacks

Send garbage data in place of pre-master secret (SSL Handshake flood)
Continuously re-negotiate the per-master secret from the handshake again & again

Hope Ichilov's explination is quite enough
Mitigation:

Client puzzle can help to mitigate DDoS attacks. The client puzzle
mechanism could be developed for protocol. It's based on
Proof-Of-Work systems. For SSL an IETF Draft is available at
http://www.ietf.org/id/draft-nygren-tls-client-puzzles-00.txt .Client-Puzzle mechanism is an intent-based security mechanism that
doesn't simply eliminate malicious clients, but increases the
attacker's cost. Also these mechanism are more effective against
signature based mitigation, which employs IDS/IPS systems with
enormous amount of rules.
The other method is to outsource the computation of modular exponent,
a highly resource intensive operation. A partial computation can be
outsourced to a daemon system(s). Otherwise, a system with
crypto-chip can be used. Outsourcing can improve the performance of
the server, but doesn't eliminating malicious client.

I already developed client-puzzle mechanism to work with HTTP. Also I planned to develop for SSL. Hope, you might feel the article interesting.
https://blog.cloudflare.com/keyless-ssl-the-nitty-gritty-technical-details/
",1,2015-07-23 05:19:03Z,102
10,5,"
There are ways and means to monitor TCP and SSL handshakes to determine bad behaviour ahead of actually decrypting / offloading.
Look at the client/server hello for non-RFC compliance and consider rate limitation for 'hello' floods based on this.
HTH.
",1,2017-01-23 15:00:07Z,103
11,1,"
You can do network scans from AWS services, but you need to fill out a request form; otherwise, you will breach their acceptable use policy.  You can find more information about the actual request process here https://aws.amazon.com/security/penetration-testing/
",4,2013-02-20 15:25:17Z,104
11,2,"
NMAP is available for Linux, Windows, Mac OS X, BSD, Solaris, and other platforms as well. If the ""cloud service"" can provide one of these environments, you can use it to run nmap. You will also want your own public IP, not something behind a NAT.
I think AWS is just web hosting, but maybe Amazon offers a service that could be used it this capacity. 
EDIT: AWS apparently provides a full virtual server, but it is behind a NAT, not conducive to port scanning. 
",1,2013-02-12 16:44:57Z,106
11,3,"
A VPS (virtual private server) is probably what you are looking for.  This would give you an actual VM to work from in a remote data center for a reasonably cheap price.  A VPN could also work (which would allow you to tunnel to an external IP with your own local hardware).
",0,2013-02-12 17:04:50Z,11
11,4,"
You are probably going to need to ask permission from any cloud provider to do this.  They generally ban this for obvious reasons and you need to get a waiver.  I'm not sure they'd provide a waiver in this direction of scanning.  Usually their customers want to scan the services running in the cloud and the IaaS provider will give them a narrow window to do it in.
",0,2013-02-12 20:50:23Z,108
11,5,"
u do not need a powerful machine to do portscanning, I do very large networks and use something simple like https://clientarea.ramnode.com/cart.php?gid=1 to do it. 
",0,2013-02-15 15:03:59Z,109
12,1,"
Start with a network diagram based on how a corporate network would look like. For example the DMZ is going to contain the webservers and mailservers. Then there will be firewalls to protect the userland from the DMZ, also you are going to have domian controllers, database servers which are going to be part of a secure network. Hope you get an idea on what am trying to descibe. You can find a sample network diagram here on a for a pentest lab. You need not follow the same thing, but use it to design your own.
As for actually building the lab I would suggest using vSphere server which allows you to virtualize the operating systems and build a network. Check this blog on setting up a virtual environment.
Check out the following resources to help you in setting up a lab
http://ist.bk.psu.edu/cvclab/wp-content/uploads/2011/12/BuildingLab1.pdf
http://www.irongeek.com/i.php?page=videos/pen-testing-practice-in-a-box-how-to-assemble-a-virtual-network
http://www.jasonjfrank.com/wp-content/uploads/2012/10/Building-your-Own-Penetration-Testing-Lab-on-the.pptm
",4,2013-01-03 11:47:36Z,119
12,2,"
This presentation at OWASP AppSec USA 2012 is about a collaborative university program that teaches practical security. It is based on OWASP Hackademic Challenges Project. He starts talking about the pentesting lab at 06:30. 
Books about building you own lab:

Build Your Own Security Lab: A Field Guide for Network Testing by Michael Gregg, 2008
Professional Penetration Testing: Volume 1: Creating and Learning in a Hacking Lab by  Thomas Wilhelm, 2009

",3,2013-01-03 15:58:28Z,121
12,3,"
You need to be looking in the direction of vSphere and ESXi, Hyper-V, or the AWS. You are creating a cloud environment that can spawn new pre-configured networks and hosts. The licensing costs can be high but it will accomplish what you want.
Alternatively, you could make VirtualBox and pre-made VMs available to students along with virtual network settings. That way they could work on the labs at home. 
EDIT:
My source at VMWare suggests that vCloud is the preferred approach, because it allows for the segregation you need to isolate student environments (isolated vlans, etc.). Other than that, a VPS (like EC2) can provide what you need cheaply.
Either solution can be scripted for automated management.
EDIT2:
Found this open source virtualization option that allows for complex VLAN networking: ProxMox
",3,2013-01-03 18:52:01Z,123
12,4,"
I would also recommend hackthissite.org.  They have a large selection of free demonstrators that vary in complexity.
",2,2013-01-03 14:35:16Z,11
12,5,"
SANS also provide a paid-for service similar to what you're talking about called Netwars - the site is here https://www.sans.org/cyber-ranges/netwars
",2,2013-01-03 15:56:36Z,124
13,1,"
While Thomas Pornin is correctly pointing out that the only way to trust a host under attack is using fully homomorphic encryption in practice you can try to work around this requirement. 
A potential attacker has full control of CPU, memory and disk. So it is not possible to do any calculations on valuable data in a VM that might not be under your control. On the other hand it is often not necessary. If you want to use your VM as a database or storage / backup service the VM never needs access to unencrypted information. You could store files or entries in a database, store hashes of the encrypted files or file names and e.g. sort for file size or retrieve a certain file where the client supplies the file name as a hash value. 
Such a scheme limits what you can do with your data but given a good encryption it is impossible for an attacker to steal your data. In the worst case they can modify it, so you need encrypted signatures to prevent tampering of the data.
A popular example is boxcryptor, which uses an encrypted container to store files securely at different cloud providers. Another is duplicity, which allows encrypted backups using rsync and GnuPG. 
Encrypting the storage of the VM itself by, e.g. truecrypt or Bitlocker does not increase the security level significantly. It might thwart an attack if the VM is powered off but as soon as you run it the OS needs the key to access the encrypted volume and at that moment a potential attacker can get hold of the key as well.
",8,2012-10-06 16:21:59Z,126
13,2,"
If the host is hostile then resistance is futile.
The host can read the disk, RAM and CPU state of your VM. Only fully homomorphic encryption would save you, but it does not work yet (Science has not uncovered an efficient solution yet; but that's just a factor one billion or so, therefore we can still hope for something... later on).
",7,2012-10-06 13:12:34Z,128
13,3,"
Other people are speaking in absolutes (in theory). Yes, if attacker has access to the host OS, then your VM can no longer be considered secure, in theory. However, to do this in practice, it requires some non-standard hacking work, which implies this is a targeted attack against YOU, because there will be plenty of other VMs that will not be encrypted and would be far easier targets than your VM. Full disk encryption in the cloud will protect you in the crowd of other VMs, but if you are targeted specifically, you're dead. There is no absolutely secure system, just different difficulties in accessing it. Disk encryption increases the difficulty. If you want better security, run your own host, but only if you are competent, otherwise it could be worse.
",4,2012-10-06 14:30:43Z,129
13,4,"
Your problem here is the same old one- an attacker who has access to the physical box, or in this case the host, can gain access to your VM. 
This is one of the risks you need to take into account if you outsource things- you use your contract with the provider to give you the assurance you need. 
",2,2012-10-06 13:08:18Z,99
13,5,"
Split your data across many VM's, hosted by different hosts (and providers, if possible) so data from single VM are useless. Of course that could kill usability and/or performance. Very rough idea.
",0,2012-10-09 19:03:05Z,130
13,6,"
Use full disk encryption if you can, where you can. Full disk encryption will only mature, and precludes physical attack, which is time off in security work for the real issues with logical access. 
By physical attack, I mean from the VM BIOS, keystroke loggers. Also would protect the drive from being mounted from elsewhere if the surrounding infra was stolen and taken off line, as opposed to actively being attacked. I think that particular threat model sounds major enough to me for it to swing the decision to use FDE if possible and if you have covered off any bigger security issues.
",0,2013-10-17 21:08:00Z,131
13,7,"
I believe protecting VM in a remount host is not easy. The best that you need to do as follows:
First: 
Establish mutual trust between you and the visited host.
Second: 
Add verification tools within your VM to detect abnormal behaviors in the Visited host.
Third: 
Add protection mechanisms to  protect the most sensitive data that VM deals with. 
You might need to put ""VM Obfuscation "" under your consideration.
Thanks 
",-1,2014-01-31 21:36:47Z,132
14,1,"
The most sensible approach is to assume you cant rely on their privacy - it isn't their responsibility, although there are some services whose selling point is securing this data.
If you take that stance, as long as you encrypt all data before it goes to the cloud you can be safe (decide on what level of encryption you need in order to be safe)
This approach gives you a very practical backup, just make sure you protect your encryption keys.
",16,2012-04-25 14:08:35Z,99
14,2,"
I would say no its not suitable for storing criticial information,
From the sound of their terms Google essentially owns everyting you upload as well as anything derivitive of your data as well.
Here is an excerpt from the verge.com explaining the differences of the 3 major players, notice Google is very liberal with what they can do with your data.
Dropbox
https://www.dropbox.com/terms
""By using our Services you provide us with information, files, and folders that you submit to Dropbox (together, ""your stuff""). You retain full ownership to your stuff. We don’t claim any ownership to any of it. These Terms do not grant us any rights to your stuff or intellectual property except for the limited rights that are needed to run the Services, as explained below.""
SkyDrive
http://windows.microsoft.com/en-US/windows-live/microsoft-service-agreement?SignedIn=1
""Except for material that we license to you, we don't claim ownership of the content you provide on the service. Your content remains your content. We also don't control, verify, or endorse the content that you and others make available on the service.""
Google Drive
http://www.google.com/intl/en/policies/terms/
""You retain ownership of any intellectual property rights that you hold in that content. In short, what belongs to you stays yours.
When you upload or otherwise submit content to our Services, you give Google (and those we work with) a worldwide license to use, host, store, reproduce, modify, create derivative works (such as those resulting from translations, adaptations or other changes we make so that your content works better with our Services), communicate, publish, publicly perform, publicly display and distribute such content.""
",13,2012-04-25 15:48:19Z,135
14,3,"
Consider this from an Information Management or Information Assurance question rather than an Information Protection question. To the question if a service provider's level of security is ""safe"" (sufficient and appropriate), the answer is YES and NO - depending on the level of protection the specific information requires.
My suggestion is you create three big categories of information, Public, Personal and Private. You can ask yourself how much damage would I be willing to endure if I lost ""personal"" information to help you decide if a specific item of information should go in ""private"" or not.  Private is the category where you need the most burdensome protections, and you don't want to put less valuable information in that category because those extra security measures cost you time, sometimes money, and generally some frustration.
All private information should be stored encrypted. It does not matter if you store it on a local drive or a cloud service, presuming the encryption is the right type (for example AES with 256 bit keys) and the pass-phrase sufficiently complex and is kept private. Odds are your home network is far less protected than networks managed by Microsoft or Google.
All personal information should be stored under the protection of network credentials by a service provider who publishes their security standards.  In the case of Microsoft, all of their SkyDrive infrastructure is now on their Windows Azure platform, which meats very stringent protections such as HIPPA.  Google, by contrast, is very upfront about scanning most information stored to target ads to you.
If you want to store personal information you can use an enhanced service from Microsoft called Office365.  For $6 per month you get a privately segmented Exchange service for email and a full SharePoint (SkyDrive plus many extra features) that is designed to protect your information from being shared outside of your ""namespace"" (think of it as your domain name). Those Microsoft services have been reviewed in depth and (outside of government classified information which generally requires dedicated hardware), Office365 is approved for all highly regulated environments such as SOX, HIPPA, etc.
If a piece of information is not Private and not Personal, it should be public and then any of the services should be safe for you. Microsoft is hyper-vigilant about not losing customer data as they spend billions of dollars on Windows Azure, so my opinion is they are safe as a backup location.
Let me leave you with a way of thinking about information that was shared by a crusty old IT guy that knew (kids avert your eyes):
Public is a picture of your best friend.
Personal is a picture of your wife in lingerie.
Private is a picture of your best friend with your wife in lingerie.
How much you protect each level of information depends on how embarrassed you would be seeing the picture (document, etc.) on the cover of your local newspaper.
",9,2012-09-25 20:05:14Z,138
14,4,"
My concern with any cloud service would be that one day you'll wake up and they'll be gone.
-- so long as they're a backup, not primary storage, it's an ordinary risk.
",5,2012-04-25 18:50:27Z,139
14,5,"
As others have mentioned, general-purpose cloud storage providers, like Microsoft, Google, Apple, and DropBox are not completely safe, since although they encrypt your files, they have copies of the keys (needed so they can index your files for search purposes).
And as Rory points out, you can make this super-secure pretty easily: encrypt the backups yourself before putting them in the cloud. This is the best option.
But there is also a middle-ground. There are some cloud storage providers who focus on security by not keeping copies of your encryption keys. SpiderOak are a popular one for backing up documents. Carbonite are also popular, and also offer an option to back up the whole of the machine (i.e. OS as well as documents.)
",5,2012-04-27 14:37:12Z,141
14,6,"
Why not using a combination of truecrypt volume and those cloud storages! truecrypt for confidentiality and the cloud services for redundancy and backup
",4,2012-04-25 18:42:11Z,142
15,1,"
If you are already settled on using Dropbox, then your only choice at the moment is to use a 3rd party program such as PGP (or free/open GPG) to first encrypt the file and place it into your drop box. I know you said you wanted to use public/private keys, but as an alternative, you could also use 7-zip to create secure archives with AES encryption based on a pre-shared key (password).
If you have not yet decided on Dropbox, you can look at similar online storage systems such as SpiderOak, Cryptoheaven, Mozy or similar, which perform client-side encryption of the files prior to uploading them into the cloud. You control the encryption key, and therefore the storage providers cannot access the data. I am not sure whether you can import existing private or public keys into those solutions or not, however.
",9,2011-12-30 16:01:21Z,148
15,2,"
I personally have this problem which I solved it myself.
I created a program which works with GPG4Win, and encrypt all my files with my PGP Key. I can also specify which folder encrypt with which user key, quite good in my opinion.
It is a 1 way sync + encrypt from my data folder to Skydrive folder (local), then Skydrive will sync to the SkyDrive Cloud. The best part is that this program does not store your PGP key, or any cloud account password.
Experience it!
Feel free to drop a visit @ http://successreality.blogspot.sg/2013/10/encrypt-sync-4-cloud.html
",4,2013-10-30 14:55:00Z,149
15,3,"
True Crypt may suit your needs. It offers the ability to use  keyfiles to encrypt your files which can then be stored in your drop box and shared.  It also offers the ability to use tokens and smart cards to secure your files.  This lets your key be passed in an out of band manner.
In my opinion the best feature of true crypt is the ability to create hidden volumes.  This creates what the creators of true crypt call plausible deniability.  This basically means that there is a second encrypted volume that appears to be comprised of nothing more than random data.  This is created inside of your encrypted file using using the free space at the end of the first volume. It is also worth noting that several different encryption  algorithms are available.  
",2,2011-12-31 02:36:38Z,150
15,4,"
Give Wuala a shot.
Data is encryted on the users device before uploaded to the cloud. It does not come with key-files but passwords. Maybe this is sufficient for you?
",2,2013-02-04 16:51:45Z,152
15,5,"
You can try:
AxCrypt by Axantum. It's free, but provides only AES-128 (not too hard). This tool may be suitable for you because it provides a portable version. Encryption and decryptionis done by two different programms (AxCrypt2Go and AxDecrypt respectively).
SafeBox is a light tool, supporting asymmetric and symmetric enctryption. Has a clear interface and detailed instructions on the site. This tool has more strong encryption algorithms. Files can be shared via any channel that supports text, or wrapped up in a container. 
",2,2017-09-20 11:26:40Z,153
15,6,"
ESecureDOX provides just about exactly what you're looking for. It's a cloud storage system run by a digital certificate authority, which means that each user is assigned a free digital certificate and PKI key set when they sign up, and each document is automatically encrypted by the storage system when you upload it to the cloud. Your document can only be accessed by your user account because of the technology behind the individual encryption (asymmetric PKI), so that the encryption code for each account would have to be separately compromised in order for any data to be compromised. And that's unlikely: the encryption that it uses would take 20 mainframe computers 20 years to brute force, according to NSA estimates. This opposed to something like dropbox, which only uses SSL encryption. What that  means is that if someone were to gain access to the blanket encryption that dropbox uses, every file on the system would be compromised.
As far as your concerns about sharing a document, when you share a document through ESecureDOX, only the single document is decrypted by the system, and that's done in a way that whomever receives your document is unable to edit it. This allows the sharing of documents without the concern that they'll be tampered with, and without allowing access to any files besides the one you want to share.
I apologize if this is overly dumbed down, but I just wanted to make sure that the advantages of the system were very clear. From the user interface ESecureDOX seems almost identical to already present cloud storage systems, but that's only because the PKI encryption and decryption happen behind the scenes. It can do that because the company that runs it is able to issue its own root certificates and integrate them into their storage solution without the user needing any technical knowledge of PKI or requiring them to keep track of their own keys.
Edit: for full disclosure, I do work in marketing for the company that runs ESecureDOX, Image-X Enterprises. But I haven't misrepresented how the service works in any way, and I do believe in its advantages regardless of my affiliation with the product.
",1,2015-04-29 21:49:01Z,154
15,7,"
Use an S3 bucket from AWS; server side you can have it encrypted and then to connect you can provision each other secure keys and use them for auditing and logging. With the use of Transmit you can add the bucket as a device in finder on OSX and treat it as a local drive which stores and pushes straight to the cloud. 
http://aws.amazon.com/s3/
It's the back end of dropbox, but it's dirt cheap and you can even choose the geographical region for your data centre.
Edit; For the purpose of securing communication client side while integrating with an S3 bucket, you can use the S3 client side encryption mechanisms. 
",0,2013-10-30 16:22:41Z,155
15,8,"
I created a small tool to do this. http://www.itsencrypted.com It uses the user's public key to encrypt a key file, which unlocks the encrypted file which is also in your Dropbox. Then later he'll use the program to download it directly and unencrypt it.
",0,2013-11-10 18:56:52Z,157
15,9,"
Another good solution is Boxcryptor. It encrypts all your data before they are send to Dropbox (or other providers). Boxcryptor also allows you to share your data in a confidential way. Therefore it makes a public/private key pair for each user. When you want to share a file with a friend, Boxcryptor downloads the public key of your friend, encrypts the file and your friend receives and decrypts the file.
",0,2013-11-10 20:02:19Z,159
15,10,"
A bit late to the game, but try Turtl. Uses client-side encryption and allows easy sharing/collaboration for both files and notes. Disclosure: I'm the founder.
",0,2014-03-07 08:42:42Z,161
15,11,"
pCloud Transfer is also a good option. It is a simple system for securely transferring files between two parties. The files get temporarily stored in the ""cloud"" but you can be sure that they will be unreadable to anyone who doesn't know the password you have to provide your recipient with. It's free and requires no registration to use it. I've been using for some time now and I'm really happy with it.
",0,2014-07-30 07:02:09Z,162
15,12,"
Even though this is an old question, it's on the first page of search results. Here are some updated free and low-cost options for 2017. 

SendThisFile. FREE forever. No file size limits, no credit card required, no software to install. All transfers include our comprehensive encryption and security, creating end-to-end 128-bit encrypted file transfers, and you won't have to configure a single setting. We use SAS70 type II / SSAE16 compliant data centers to ensure that private data is protected. It is the industry gold standard for the robust delivery and security of data. https://www.sendthisfile.com/
SpiderOak protects your group messaging, file sharing, and file backups with end-to-end encryption to keep you safe from privacy intrusions, ransomware, and data loss.
It keeps your uploaded material safe and private. They can’t even see what you upload, because you are the sole holder of the keys to unlock the material. The only downside is that it is a complicated for a new user, as it does have a steep learning curve.
100GB storage: $5 per month.
https://spideroak.com/one/ 

",0,2017-10-06 12:32:02Z,151
16,1,"
At 16-core GPU maximum, I'd build my own, unless oclHashcat can distribute work loads (at first look it doesn't seem like it does). That is assuming this thing is going to pound passwords all day most days. If you can scale it more (or want to run a lot in parallel) or wont use it all day long, pay for it by the hour.

Now, after some sketching and brain storming we concluded that GPU is the best way to go (contrary to CPU or rainbow tables).

I'd argue that you should probably download and archive some rainbow tables anyway, though it's less likely they'll come in handy, a lot of poorly written software still runs silly stuff like MD5, which will crack a stupid amount of time faster on a rainbow table. Multi-pronged approach for multiple environments.
Barebones EC2 GPU cloud will run you about $7+k on metal, depending on RAM/CPU/Disk requirements:
http://www.google.com/products/catalog?q=1026GT-TF-FM209&um=1&ie=UTF-8&tbm=shop&cid=16863135665868024811&sa=X&ei=4_HDTsPIFcff0QHjh7XmDg&ved=0CDoQ8wIwAQ
http://aws.amazon.com/hpc-applications/

Cluster GPU Quadruple Extra Large
33.5 EC2 Compute Units (2 x Intel Xeon X5570, quad-core “Nehalem” architecture)
22 GB of memory
2 x NVIDIA Tesla ""Fermi"" M2050 GPUs 1690
GB of instance storage 64-bit platform I/O Performance: Very High (10
Gigabit Ethernet. Full-bisection bandwidth with Placement Groups) API
name: cg1.4xlarge

",8,2011-11-16 17:21:47Z,165
16,2,"
This is a simple cost / benefit analysis, so it's a business question rather than a security one since you've already decided the merits of how to approach the security aspect. Write your code, benchmark it, and compare numbers in a spreadsheet.
For the metal
* Cost of metal
* Cost per hour in terms of electric bill (assume under full load) 
* Budget for parts replacement
* Operations per second

For the EC2
* Amazon rate per hour
* Operations per second

Compare the numbers on a month by month or year by year basis. Pick the one that makes more sense based upon expected life. Front-loading cost, net present value, expected utilization, risk, etc are all accounting problems.
",15,2011-11-16 16:49:19Z,170
16,3,"
This link about GPUs and EC2 password cracking gives you much of the information you are looking for:
http://erratasec.blogspot.com/2011/06/password-cracking-mining-and-gpus.html
The upshot is this. Radeon GPUs are about 3 times faster than equivalent nVidia GPUs for password cracking. But, EC2 uses nVidia GPUs, because they are better at most other GPU tasks. Moreover, EC2 doesn't just use nVidia GPUs, but the expensive ""Tesla"" versions. You can buy a $250 Radeon GPU with the same password cracking performance of a $10,000 Tesla system. Whether your buy it or rent it from Amazon, it just doesn't make sense.
Because GPUs can make you password cracking 20 times faster than a normal CPU, there is a great benefit to using one of them. But, because there are decreasing marginal returns, there isn't a lot of benefit to investing a lot of money in GPU number crunching.
The upshot is that it's worthy spending $500 for two Radeon 6850 cards and sticking them in your desktop for use with oclhashcat, but not worth spending more.
",3,2011-11-17 16:59:29Z,171
16,4,"
Did you try the new GPU rainbow tables from freerainbowtables.com ? 
You don't even need a supercomputer / cloud to check the hashes. 
You could donate to the freerainbowtables.com project some hardware and request your custom rainbow tables. (they have over 2000 machines, in their distributed project that generate rainbow tables on the CPU as well as the GPU).
For salted hashes I would go with an array of fpga cracking machines.
I suggest you post this question on the forum ( http://freerainbowtables.com/phpBB3/ ), there are a few tech freaks like Sc00bz, that would provide a very insightful answer. 
",3,2011-11-22 13:42:18Z,173
16,5,"
It is about risk management and cost/benefit analysis. The more risk that you are willing to take, the less it will cost you to run your application. There are some basic and somewhat obvious threats that could disrupt or compromise your application's operations:

Physical security risk

This refers to the physical security threats to your application's infrastructure, such as unauthorized physical access to hardware, natural disasters, power outages, network disruptions, terrorism, etc. 

IT security risk

IT administrators require physical access to the  hardware, as well virtual access the operating systems running on the hardware. IT administrators will require such access to servers, network equipment, power distribution circuits, monitoring software, etc. There are many security and operational risks associated with having so many different hands on your hardware and software systems.

Technical failure risks

The hardware and software can and will fail from technical reasons and these are everyday problems. When it comes to GPU computing, you will want to have a large cluster of GPUs (as many as you can get). But the more servers you are running, the higher the odds of a hardware failure randomly occurring.
For example, a network card or power supply could burn out. Or a memory leak could crash a server.  
A more subtle but also common problem is electromagnetic interference from the cosmic background radiation that can cause memory corruption.

Capacity risk

As your computational requirements fluctuate, you might have a sudden need to increase the scale up by adding capacity to your GPU cluster (i.e. ingadd more GPUs). There is a risk that your infrastructure will not be able to scale up with your application's requirements.


To help understand the situation, let me give you three examples of different GPU cluster setups with different cost/benefit profiles:

Tier 4 private data center

You build an underground bunker in a friendly sovereign country with strong privacy laws, in region of the planet that is safe from common natural disasters
Your data center is  physically guarded by surveillance systems, and armed security professionals
Bio-metric security access systems in every server rack in the data center ensure that only authorized personnel can ever physically access particular hardware
You have two redundant mains power circuits coming from power generation plants on the grid to the data center
Each power mains circuit is backed by a UPS backup power supply and a backup generator (you have contracts with fuel supply companies to guarantee a supply of fuel to run the generators over an extended period of time)
Each device (server, switch, etc) in your data center has dual redundant power supplies, with each power supply connected to a separate mains circuit
The data center is connected to the outside world through multiple, redundant fiber optic links to backbone telecom carriers
The local network in your data center connecting the servers is fully redundant, with multiple, bound network interface cards in each server, connected to multiple, redundant switches, firewalls, etc.
The local network in your data center connecting the servers is fully redundant, with multiple, bound network interface cards in each server, connected to multiple, redundant switches, firewalls, etc.
Virtualization technology is used to run Operating Systems in Virtual Machines interconnected through Virtual LANs, to protect against physical hardware failures. The physical servers are configured as a Virtual Machine cluster, so that virtual machines can be seamlessly moved from one physical machine to another. 
You only use server hardware with Error Correcting Code (ECC) memory, to protect against electro-magnetic interference from cosmic background radiation. For GPUs, this means that you must use NVIDIA's expensive Tesla series of cards which have ECC memory. The less expensive gaming cards do not have ECC memory so they are prone to data corruption.
You have a 24/7 Security Operations Center, doing things like monitoring network traffic and server activity to detect and respond to intrusion threats
You have a 24/7 Network Operations Center, monitoring and maintaining the status and health of systems, etc
You have redundant cooling systems for the data center
Air filtering and air quality monitoring systems are in place to avoid dust from entering the hardware
Fire detection, fire alarm and fire suppression systems with dry suppression agents are in place
Water pipes and other water systems are avoided - no water near the servers
You keep spare hardware components on-hand in case they need to be replaced
There is plenty of spare rack space, power and cooling capacity to scale up your data center if required
All aspects of the data center are continuously audited by an internal audit team
For added security redundancy, you build one or more similar secondary data center underground bunkers in a different countries and split your cluster across multiple regions. You use Global Server Load Balancing and Failover technology to seamless run your application across multiple data centers.
This is least risky but the most costly solution
Note that there are many other things involved in building a Tier 4 data center (see http://www.adc.com/Attachment/1270711929361/102264AE.pdf)

Third-party data center

Instead of building your own data center, you could co-locate your data in a third-party data center. Co-location in a Tier 4 data center facility would be very expensive (especially if you are co-locating in multiple redundant, data centers), but not as expensive as building and owning your own facility. This is less secure that owning your own private data center, but also less costly. 
You still own your own and run your own hardware that is co-located in the racks. You can secure them yourself and you can upgrade the hardware whenever you need to. There are many Tesla GPU server options available on the market from the different hardware vendors, like IBM, Dell, HP, etc.

Cloud based solution (e.g. Amazon EC2)

You do not have any physical control over the hardware and server virtualization systems. All of your servers are virtual machines running in Amazon's data center. Your application's security is at the mercy of Amazon and its employees.
You can setup a Virtual Private Network within Amazon for additional security. You can also run your own physical connections to Amazon's EC2 data centers.
Amazon offers three types of GPU instances: Spot instances, On-Demand instances and Reserved Instances. Spot instances are the cheapest, but you are not guaranteed capacity - you might bid on 10 GPUs in the spot market and only get 2, and you might lose your GPUs if the spot price exceeds your maximum bid. On-Demand instances are more expensive but cost a fixed rate and you can run them as long as you need to, but capacity is not guaranteed. Reserved instances are the most expensive and require paying an annual reservation fee for each server that you want to reserve, but this guarantees that you will always have access to the number of servers that you require.
The Amazon EC2 cloud is distributed across multiple geographic locations. This allows you to have multiple, redundant data centers 
You don't have the buy and maintain your own hardware, and you pay for metered usage by the hour which can be very cost efficient if your have large fluctuations in your computational demands. If you need 100 GPUs for a few days out of the year, then wit would be much more cost efficient to use the cloud than to purchase and run your own hardware all year round.
Amazon only offers the NVIDIA Tesla C2050 Fermi GPUs. There are more GPU hardware options available if you own your own hardware, including the new Kepler GPUs from NVIDIA that are not yet available on Amazon
Renting Amazon GPUs is very cheap if you go with spot instances (these typically cost 60 cents per hour, with no upfront fee). But if you need reserved instances then the cost rise dramatically to around $5.00 per hour on average.
Working with a cloud like Amazon is more complicated than any other solution. It requires you to spend a lot of time learning on tangential issues about how the Amazon cloud works instead of spending time figuring out how to use GPUs to solve your actual application.

Build your own low-cost, high performance GPU supercomputer in your basement

Let's say that most of the risks I've talked about are not not worth mitigating for you. You are willing to put a GPU cluster in your basement to save on the data center real estate costs, and you are willing to accept risks, such as the of a pipe bursting and flooding your basement ""data center""
Your application is not mission-critical, it does not require 24/7 guaranteed uptime.  If a natural disaster befalls your house then you will probably be worried about bigger things than the security of your GPUs. 
You don't even care about electro-magnetic interference from cosmic background radiation. If your memory gets corrupted and a GPU hangs then you can just reboot the server (and you have a cluster of GPUs anyway, so losing one or two is not a big deal). You can use the latest NVIDIA GeForce Kepler based gaming cards that are  cheaper and faster in single precision that the Tesla cards offered in Amazon. You are such a risk taker that you are willing to do all your computing in single precision instead of double precision, to take advantage of the cheaper gaming cards. So you can buy a high end gaming GPU from an OEM for $400 per card, instead of buying a Tesla GPU from NVIDIA for $2,500 per card and your $400 GPUs will be significantly faster than the more expensive Tesla GPUs, meaning you can buy more GPUs with your budget.
You can take a page out of the Google playbook ... instead of buying fancy servers from companies like IBM or Dell, you can build your own bare-bones, ultra low-cost GPU servers. This means you can buy even more GPUs with your budget. For example, you can skip all the bells and whistles like a case for the server, dual power supplies, hard drives, etc. Since your servers won't have any cases they will be easier to cool. You can also build your server with an ultra low-cost, low-power CPU (instead of spending money on big, expensive multi-core Intel CPUs that you won't use anyway since you are using GPUs), and instead spend money on more GPUs instead of useless CPUs. You can run everything on Linux to completely avoid software licensing fees, leaving you with more money to spend on GPU hardware.
This is not a very secure solution at all. However, this will get you hands-down the fast GPU supercomputer per dollar that you spend. This is your biggest bang for the buck if you're not worried about running a data center in your basement. And it is the easiest environment to maintain and develop on, there is no complicated cloud technology or security hindrances.


CONCLUSION:
For a password cracking machine, I would recommend going with Amazon EC2 if you plan to use large numbers of GPUs for small amounts of time. If you expect that you will only need to crack passwords once in a while, but when you do need to crack a password then you immediately want as many GPUs as you can get, then I would recommend that you go with Amazon EC2 spot or on-demand instances. However, if you plan to be cracking passwords 24/7, i.e. maximizing your utilization of your infrastructure, then I would enthusiastically recommend the DIY route of building a GPU cluster in your basement (just be sure that you have enough power capacity or you will trip a circuit). I should also say that the DIY route is much more fun.
",3,2012-04-05 06:54:18Z,174
16,6,"
You need to run the numbers; however, what are your time constraints?  
As a consulting firm, you might end up with a client that wants to see results by tomorrow. The only way to scale that large is to use Amazon. Building your software so you can scale out to a thousand nodes is the only way you can do that (without owning all that hardware).  
Using 1000 nodes for 1 day is the same cost as 100 nodes for 10 days, but you'll have the results a lot faster.  
",3,2011-11-16 19:29:46Z,108
16,7,"
Brute force password cracking depends on the algorithm in use.  For example are you trying to break a SHA1, SHA2 or BCRYPT password?  Or is the underlying algorithm SCrypt?
It is not possible (as far as I know) to GPU accelerate a SCrypt based password hash, and only CPU optimizations are available due to the additional demands on RAM with the cryptography.
Next, are there libraries that support what you need to do?  OpenCL is popular, but performs differently on NVidia GPUs vs AMD GPUs.  In addition, you may get better performance with vendor-specific APIs.
Finally, know that each video card performs differently and a $700 NVIDIA card is feature rich but not too efficient in SHA2 hashing.  A similar AMD video card will outperform an NVIDA card for SHA2 hashing by about 40% or more depending on the card.
If you choose to use the Amazon GPU cluster, know that the only OS is CentOS and you will have to have python skills (or similar) to get your code up and running.  It may not be worth it in the long run since the GPU farm offered by Amazon includes old NVidia cards that are good for general math offloading, but not efficient for password hashing.
",0,2013-01-02 15:20:59Z,175
17,1,"
Figure out the cost level where you start getting uncomfortable.
Calculate the GB that would need to be transferred to reach that cost level.
See if you think an attacker will be willing to spend that much effort to hurt you for that amount of money.
Every time I run this calculation I end up thinking that if somebody hated me that much, they could certainly find an easier way to hurt me.
For example, a million downloads of your 2MB document is going to cost you about $240 in data transfer charges plus $1 in request charges.  To create this cost to you, the attacker is going to have to download 2,000 GB (2TB).  That's weeks of completely filling up a 10Mbps line.  Just for a measly $240 impact.
Amazon generally doesn't discuss publicly all of the security measures they have in place to stop DOS, DDOS, and other attacks against their customers.  In one whitepaper, Amazon says: ""Proprietary DDoS mitigation techniques are used.""  Of course, it's not always easy to differentiate between a DDOS and a popular resource :-)
You can read more about Amazon Web Services security on their site:

http://aws.amazon.com/security/

",23,2011-11-02 03:04:12Z,181
17,2,"
Agree with Eric.
Another point: DOS/DDOS are nasty attacks that relatively easy to make and hard to protect on application level.
Think about what other option you have.
Let’s say you will go to some other hosting provider and will get same attack, will it be better or worse?
With Amazon at least you can be sure that you will continue to have service and it’s not going to crash under DOS.
Other thing that you may want to check is if you can make your document size smaller (use pdf and not word, change image quality etc.)
",9,2011-11-02 06:13:06Z,182
17,3,"
Although S3 don't allow you to set a cap on costs, there's nothing to stop you setting a cap yourself.
Add into your application soft limits which alert you if you are going well above your historical high points, and hard limits set at a reasonable level loss. That way you get to decide what is reasonable and what isn't, according to your budget.
",6,2011-11-02 13:23:39Z,183
17,4,"
In order to monitor your costs more than once a month you can enable what is called Bucket Logging. With this enabled you get the equivalent of web server access logs for your bucket delivered to you. You can then keep track of how many bytes have been transferred and by whom so there are no surprises at the end of the month.
",6,2011-11-03 15:37:04Z,184
17,5,"
You can add logging to your account, and then monitor the logs for activity spikes. 
Still, though, wish that Amazon had some sort of limits on S3. When I set up a client with their own private S3 account, this is a small problem - users can generate public URLs to say some funny cat video.... - But this has never actually happened.
Another thing you can do is to set up the account with a credit card that has a low credit limit. (Don't use the platinum card).
I have had some small MB sized assets in the public on S3 for a few years, and the bill on them is always microscopic. 
",1,2011-11-10 14:13:01Z,186
18,1,"
PCI compliance is a bit complicated. Firstly, with shared hosting, you need the provider you're on to meet all their relevant requirements. See Has anyone achieved PCI compliance on AWS?
Second, whoever has a terminal that is processing cards is responsible being complaint. It sounds like in this circumstance, you're not the processor and don't have an agreement with a credit card company. In that sense, you don't need to be compliant (HUGE caveat following)
However, the person with the terminal does need to be compliant, and everything they use in the process of handling cards (including you as a 3rd party) needs to achieve compliance. Compliance requires active review -- the depth of which depending on the volume of transactions handled.
Most critically, I wonder what circumstance you're in that you would be storing credit card numbers and not actually processing them.
PCI compliance has to do with volume of transactions processed. Smaller volume shops don't have to worry about a lot. Large volume companies are only PCI compliant if they meet all the standards specified and they are signed off on by a PCI council approved auditor. VISA, for example, specifies those as anyone handling 6 million transactions a year.
There's a guide for merchants at https://www.pcisecuritystandards.org/merchants/how_to_be_compliant.php
",6,2011-08-20 03:15:41Z,170
18,2,"
Absolutely Positively Not
Storing card details is only approved for certain setups, far more advanced than this. 
Doing this as described would not only be not PCI complaint it would be illegal in many countries under various privacy and commerce laws (many of which defer to the PCI SSC for policy and regulation.)
Furthermore it's almost certainly a server breach of the MSA to process payments through a terminal which were obtained in this way. Such a breach may be governed by a remedy of fine or legal action.
",4,2011-08-21 04:29:25Z,189
18,3,"
Its not ideal,  and to be honest it sounds like you are violating OWASP a9.  The checkout process is meaningless,  the problem is spilling your authentication token (THE COOKIE) over plain text. 
",2,2011-08-20 18:58:30Z,190
18,4,"
If you are taking the CC info and then running it through a terminal does that mean you are storing the CVC/CVV? You are not allowed to store that after authorization for ANY reason under the PCI DSS. 
",2,2011-08-21 02:56:24Z,191
18,5,"
It'd complicated, but in short, this setup would not be compliant.  For a start, based on the limited information provided, you are storing the card data in an internet facing zone, rather than an internal system.  There are numerous other things wrong with this setup (management interfaces, key management for encryption etc), but the data storage is a glaring one.
There are also additional requirements for shared hosting providers that would need to be validated.  Under PCI, shared hosting is not really appropriatefor anything other than a merchant page that then redirects to a compliant payment gateway.  Why is your client not doing this?  
",1,2011-09-07 22:20:28Z,192
18,6,"
All good answers. PCI says if you are storing credit card data in any form you need at the very least an ASV to scan your system for vulnerabilities. If you get to a level 1 or 2 (based on your transaction level) you will need a QSA to do it. The bottom line is if you are storing credit card data in any fashion you are a risk to everyone. You can do it but the costs to validate your systems could be prohibitive, depending on the volume you are doing.
",1,2012-03-09 17:34:40Z,193
18,7,"
Unfortunately, as previously stated, the answer is ""No""
To achieve full PCI compliance you need to fulfill many, very specific, requirements. 
Full list can be found here: 
https://www.pcisecuritystandards.org/documents/pci_dss_v2.pdf
Some of these requirements deal with the way you manage your servers, other talk about internal role distribution and so on... 
For example section 9.1.1 states:
""Use video cameras and/or access control mechanisms to monitor individual physical access to sensitive areas. Review collected data and correlate with other entries. Store for at least three months, unless otherwise 
restricted by law."" [this obviously talks about serves access]
I`m choose to focus on this section to show that PCI complicancy is not a ""purely technical"" issue.
Having said that, on a technical site, you probably will want to take notice of section 6.6 that calls for one of the following :

Reviewing public-facing web applications via manual or automated
application vulnerability security assessment tools or  methods, at
least annually and after any change. 
Installing a web-application-firewall in front of public-facing web applications.

This is probably the most demanding section in the whole bill as it requires you to either have a WAF in place or to perform routine checks after (and this is important) EVERY CHANGE made to your web applications.
In other words - pure nightmare...
This section is what drives many SMB website owners away from PCI DDS, and into an arms of a 3rd party billing providers, as the setup and maintenance costs are simply just too high. (WAF will cost thousands of dollars,even before maintenance costs, and routine checks after every change are not an option, and even if it is - it comes with an even greater accumulative cost) 
Recently a affordable solution to this issue was made available via Cloud-based PCI compliant WAF.
The idea here is that of ""shared-usage"" and ""economy of scale"". In this scenario, WAF protection is distributed via Cloud to a community of users (websites) and each member community gets full WAF features and updates but needs to pays only a fraction of the full price.
This cuts heavily on initial setup/purchase costs and also eliminates all additional maintenance costs (as a centered security team operates and updates the WAF for all Cloud users.) 
Also this provides full standardization and promises a very high upkeep quality (very important in an ever-changing security landscape but not achievable without a dedicated security person/team)
(disclaimer - I work for the company which provides this solution)
",1,2012-07-17 07:42:54Z,194
19,1,"
This may not be the answer you will be happy with but how about abstaining from having any undesirable data inside your phone in the first place and instead using the right tool for the job? 
According to Wikipedia:

The app records information about the device it is installed on, including its [...] IMEI, the phone's model and manufacturer, and the phone number. The app searches the phone for images, videos, audio recordings, and files [...]

So, instead of trying to tamper with this spyware in any way (which can get you in a much bigger trouble), simply don't do anything suspicious on this phone and let this app do its job. Prepare against it by not having any photos, videos, audios, file, etc., and instead use the right tool for the job. Use some other secure software/hardware to connect to internet, use encrypted email provider and do all of your communication through the computer where you can do communication safely, and store all of your files somehow in a safe place (encrypted, somewhere on computer or USB, etc). Pretend to be an obedient citizen and use the right tool for the job to do whatever it is you don't want your government to find out. 
Some people may wonder why bother having a phone in the first place (and FYI, I asked the same question under OP's question, for clarification). My answer is: 

to make phone calls (and have conversations which are not going to be considered by Chinese government suspicious, in case they are tracking that too)
to use it as a ""red herring"" - if police asks you to give them your phone you won't have to lie to them that you have no phone, or worry that they will find out that you tampered with app, or get in trouble if you don't have app, etc. You'll just confidently give them phone, with no ""illegal"" information on it, they will check it, and walk away. You may, actually, even have some ""red herring"" files: pictures of nature, shopping list (milk, eggs, etc.), etc., just so that they wouldn't suspect that you deliberately not using your phone for such purposes, and harass you farther. 

I mean, not long ago mobile phones didn't even have the ability to store pictures, videos, files, etc. 
Are you willing to put your life in danger simply because you want to have some files on your phone?
Tough times require tough decisions.
",335,2018-09-24 18:47:56Z,206
19,2,"
Get a phone which doesn't support Android apps.
Why are so many of the answers complex? And not just complex, fragile and suspicious and downright dangerous to the questioner? 
You want to use your phone to send messages and make calls, right? You don't want this app installed, right?
Say hello to your new phone:

Good luck getting an Android app running on this. 
It's probably not illegal to have an old phone.
",177,2018-09-26 11:47:30Z,209
19,3,"
This is a tricky one.  It goes without saying, but it's also a dangerous one.  Attempting to circumvent these restrictions and getting caught doing so will potentially cause a lot of legal trouble.  If they throw people in jail for refusing to install the app, I wouldn't want to figure out what they do to people circumventing the app restrictions.  It is especially relevant because even experts in tech security have gotten caught by their governments despite extensive safeguards (the founder of Silk Road is a great example and is now serving a life sentence).  Granted, evading this app is most likely a much less serious ""crime"", but the Chinese government isn't exactly known for lenience here.  So while I would like to answer your question, please don't take this as me suggesting that you actually do any of this.  I consider myself a tech-expert, but I still wouldn't do it.
Still, to answer your question, you have a few options.  I won't bother mentioning the ""Get a second phone"" option because you've already ruled that out.
1. Virtual Machine/Dual Boot
There are some options for ""dual booting"" android phones.  I don't have any examples to immediately link to (software suggestions are off topic here anyway) but there are options.  If you can get your phone to dual boot then you can install the tracking software on one ROM and then do all your personal stuff on the other.  You may need to put some basic information on the ROM with the tracking app installed just so you don't raise too many flags.
Of course there are still risks here: risks that they might reboot your phone and notice, risks that they might realize you have a completely different system installed next to the tracked one, and the simple risk that you would go out and about and forget to reboot into the ""tracked"" system, allowing a police officer to find and install the tracking app on your actual system.
2. App modification/interceptors
If this app creates enough bad press it is possible that anti-tracking apps or hacked versions of this app may start floating around that try to automatically protect you from it.  I would not expect there to be any general tools already available that would protect you from this, so this is something that would simply take lots of googling or (perhaps) requests to the right people.  This has a major downside that unless you are an expert at reverse engineering, there isn't much to do to make this happen.  It's also hard to estimate what the risks of detection are.  That will obviously vary wildly depending on the skill level of the person who put it together.
3. Server Spoofing
Depending on your level of technological know-how you might be able to put something together yourself (note: this is not for novices).  Based on what I know and my experience in this area, I'm going to try to summarize some details about what a server-spoofing measure might look like.  Again, I'm not summarizing this because I think you should do it, but because understanding how things like this operate can be generally informative and also help understand the risks there-in.
Built-in security
First, we need to understand how this spying app might secure itself.  From all information available so-far, the answer is ""it doesn't"".  This is a pretty simple conclusion to come to because the app communicates exclusively through http.  It is very easy to intercept http requests, either from the device itself (if your phone is rooted) or with network sniffing tools on a computer attached to the same network as the device.  Most likely it is also very possible to easily figure out how the app authenticates itself with the end-server and how the end-server authenticates itself with the app.  In all likelihood there is no authentication in either direction, which means that spoofing requests in either direction is trivially easy.  This might be hard to believe (given that a country like China sets aside lots of resources to invasive technology like this), but the reality is that if the people who developed this app wanted to secure it from outside tampering, using HTTPS for transit would be the very first step to perform.  It is cheap, easy, and very effective.  The lack of HTTPS means that it is very likely that there is no actual security in this ecosystem, which is a plus for anyone trying to evade it.
Sniff all traffic coming out of this app to determine what requests/responses it makes
This is the first step.  By watching the traffic leaving this app (which can be easily intercepted in the network itself since there is no SSL encryption) you can figure out what requests it sends to the destination server and what responses it expects back.  Understanding the underlying API is critical, but easy due to the lack of encryption.  This will also let you know if there is any authentication happening in either direction.  If there is, you can at least see the full request and responses, so you can most likely figure out how to spoof it.  It is possible that there is some hard-to-reverse-engineer authentication going back and forth, but again, given the lack of basic encryption, I doubt there is any such thing built in.
Figure out if the app is talking to a domain name or IP address
The destination server the app is talking to is either found via a DNS lookup or has its IP address hard-coded in the app.  In the event of the former you can edit the DNS for your android phone to repoint it to a different server, including one running on your phone.  In the event of a hard-coded IP address you will similarly have to redirect all traffic to that IP address to your local android phone (presumably you can do this with Android - you can with other operating systems, but you would definitely have to root your phone).
Setup a replacement server
You then setup a local server that responds to all requests just like the server did in your initial spoofing.  You would have to get this server to run on your phone itself, that way it is always available.  This doesn't necessarily have to be complicated (although that depends on how detailed the actual server interaction is), as you don't actually care about keeping any data on hand.  You just need to make sure that you provide valid responses to all requests.
Risks:

The app may auto-update itself (although your mock-server may make this impossible) and point to new domains/ip addresses, suddenly removing your protections
If there is an auto-update functionality and your end up unintentionally killing it (which would be good per point #1 above), a police officer may notice that it is not properly updated, flag you for ""extra"" checking, and discover what you are doing.
They may do server-side tracking and discover what you are doing because they don't find any data on their end for your particular IMEI (because your mock-server acts like a black-hole and sucks up everything).  Even if you send spoofed requests there will be easy ways for them to determine that (imagine the police copy a blacklisted image to your phone and discover that the app doesn't block/report it)
They may have root-checking in the app itself, which will cause you problems

Actually, that's it
I was trying for a longer list but that is really what it all boils down to.  Short of not carrying around a phone or purchasing a separate one, these are about your only options.  For reference, I haven't gone into details about the server spoofing because I think you're necessarily going to go out and do it.  If anything, I've gone through it because it gives opportunity to talk through the risks in more detail, and those should make it clear that there are a lot of risks.  Even if you find a solution from someone, they have to deal with all of these same risks (or ones like it).  Right now this app sounds like it is poorly executed and easily fooled, but depending on how much the Chinese government decides it cares, that could change very quickly.  At that point in time not getting caught basically turns into a cat-and-mouse game with the Chinese government, and that isn't realistically something that someone can continue to win for an extended period of time.  There are a lot of risks, so tread lightly.
",79,2018-09-24 18:55:17Z,214
19,4,"
They can execute code on your device while they have physical access to it. And you can't refuse it. I'm sorry to say that but you are basically doomed. There's no way to trust this device anymore. That's part of the 10 immutable laws of security. In your case the rules #1, #2, #3, #6 and #10 are applicable.
But when you act like you don't trust the device you could raise their suspicion. Maybe. Because nobody knows what they are actually doing with the collected data. Maybe nothing at all. In the ""best"" case it's primary for spreading FUD.
But when they are actually using the data it's easy for them to spot burner phones and all kind of tampering. As far as I know you only get a SIM card by identifying with your ID. Since the spyware reads identifying information like IMEI and IMSI they can simple compare the collected data from the phones with the purchase records. They can combine this with behavior tracking based on metadata collected on the phone (Which apps are used and how often, how long the screen is on etc.) and the mobile network (usage of data, location based on cell tower etc). Since they can do that on a large scale, they can spot strange usage patterns by your usage history or how a ""average"" user is behaving. Of course there's a vast amount of ambiguity and such in this data but they have the ultimate interpretational sovereignty.
You must also keep in mind that you need to keep your measures working all of the time because it could always happen that you get stopped on the street again.
I'd like to emphasize on rule #10. You are basically trying to solve a social problem with technology - just as your government does.
",54,2018-09-24 21:51:43Z,216
19,5,"
Use a custom ROM (two, to be correct).
Android phones can have more than one ROM installed, and you choose one or the other. So install two copies.
On the clean ROM you install the spyware, anything not dangerous, games, whatever you feel clean. On the secure ROM you install things you don't want anyone to know about.
Keep the clean ROM running almost all the time, specially when you are out of home. Boot on the secure ROM only when you really need.
You will need to keep a secure mindset too, to have awareness of what ROM you are using and what content you can create or access. That is the main point of failure. Using a different keyboard on each ROM, or different OS languages can help: Chinese on the clean, English on the secure, for example.
But first you must weight the risk/reward of doing so. If the risk of getting caught plus the mental effort to keep activities and files containerized is worth the benefits of bypassing the spyware, do it. Don't do otherwise.
",28,2018-09-24 20:42:44Z,219
19,6,"
I'd recommend you just go with it. The Chinese police doesn't just stop any random person in the street and asks for their phone. They stop Uyghur.
This happens for reasons which are somewhere in between ""mitigate a real threat"" and ""Woah, no go, dude"", but whatever it is, it's what the government does, so it's legal and ""right"". No benefit of doubt, and no assumption of innocence, no Sir. By Western standards, it's kind of unthinkable, but you cannot draw to the same standards there.
So the situation is that you are easily identified as Uyghur, both from looking at your face, and from the fact that police knows. They know who you are and where you live. And sure they know whether you've been stopped before. Again, you're not being stopped at random. You're stopped because you are already a well-identified target, on their screen.
It isn't even unreasonable to expect that your Internet traffic is monitored (targetted) and even asking about how to circumvent the measures may move your name onto a different, more high priority list.
You can bet that police keeps a list of people where the spyware has been installed (with device IDs), too. If no data comes in from your device, well, guess what. You'll be stopped again by police, and they will look very carefully why this isn't working.
Insofar, it is kind of unwise to try and circumvent (and risking being caught) what police wants. From their point of view, you are a possible criminal, and a possible terrorist. By trying to circumvent the measures you prove that you are a criminal.
The surveillance happens on the base that if you have nothing to hide, then you need not bother if they're watching you. Again, by Western standards, this stance would in no way be acceptable. But whatever, in China it's perfectly acceptable.
I wouldn't want to risk disappearing in a detention camp if I was you. Rather, let them have their spyware, and simply don't do anything that isn't opportune to the system.
",15,2018-09-25 16:46:31Z,221
19,7,"
First of all, I think you should search for solutions that are already implemented by other people. For instance, what do other people in your case do to prevent the spying activities?
One possible solution would be to have a man-in-the-middle implementation analyzing the information that is being sent, altering it, and sending it to the same server and port the spyware is trying to connect to.
I read a bit about the functionality of the app, and the information it gathers is, and I quote from the Wikipedia source you provided:

sent in plaintext

Hence, after doing some tests with a packet sniffer tool and clearly understanding how the spyware and server exchanges made using the HTTP protocol work, you could, if you have root access to your Android phone, redirect the traffic of the spyware app to a process that is running on the background of your Android OS. This process would change the data that is going to be sent to the server the spyware is trying to connect to. That way, you can send data that matches another cellphone (maybe, literally faking the data is a bad idea, because that can trigger alarms).
You should also take into consideration any kind of validation processes that the spyware has implemented so you do not alter them. More specifically, the data that is in the HTTP packets’ headers and that is sent from the spyware app to the server to gracefully initiate an upload.
Of course this is theoretical, but it is a realistic thing to do. Also, you probably will require knowledge of Android programming (mostly in C or Java) and IT.
This approach is stealthy and will not require an uninstall of the spyware app. There is always a risk, but in this case, depending on the data that is actually spoofed, the risk is minimal.
",11,2018-09-24 15:43:41Z,224
19,8,"
Due to the nature of the spyware, they will be able to detect any mitigation techniques which will make you a person of interest to them.
I know you said you can't afford two phones but it really is the best advice - why not clean and refurb an older phone if you have one around?
A burner phone doesn't need to be anything special and even better if it isn't a smartphone.
",9,2018-09-24 13:41:15Z,229
19,9,"
One idea is to think of your phone as a networking device and nothing more.  If you carry a secondary ""tablet"", that is NOT a phone, you can tether through your phone, and use a VPN on the tablet to protect your data.  Now you can hand out your phone to be inspected as all of your actual important data and work is on your tablet which is clearly not a cellphone.  If traditional hotspotting is too dangerous, you may want to consider alternative methods such as using a USB2Go cable or bluetooth pairing. If hotspotting is detected or blocked, you might also be able to use an app like PDANet to bypass those restrictions.
",8,2018-09-25 13:52:59Z,231
19,10,"
Disclaimer: I live in North America.

Knowing that I may be forced to install it sooner or later, what are my options to prepare against it?

Remove illegal items and anything else that you think that the government wouldn't approve of from your property - your clothing, phone, desk at work, home, etc.
If the government can't find you doing anything wrong then you only have to worry about someone planting false evidence on you - you need to search your own stuff from time to time.
When I go through Customs I've made absolutely certain that I have checked everything for anything I shouldn't have, because you know that they will likely check when you go through the port of entry.
As a result of my efforts they've not found anything to object to and have even simply waved me through a few times. Nothing to see here, move along.

Ideally:

Make it appear like the app is installed and working as intended, without having it actually spy on me.


We have different ideas of what's ideal.
Ideally I'd prefer to be paid millions of dollars per hour.


I don't know whether it includes sophisticated anti-tampering features or not. I can't afford two phones nor two contracts, so using a second phone is not a viable option for me.



It wouldn't make any sense for it not to detect tampering.
The APP is probably an excuse.
If they obtain the information by another means (like monitoring the cell phone towers and WiFi, along with all Internet traffic, and then there's your neighbors whom earn a healthy living turning people in) they can say in Court that they obtained the information from the spyware - that way you don't know how the information was actually obtained.
This happens in more places than just where you are, it's different where you are in that roving gangs force you to install the APP. In other places (including North America) they get by without using an APO and rely on other techniques.
Proof: In the last few months people whom have a lot of contact with children (Coaches, High School Principals, etc.) have had their work, home and computers searched for possession of inappropriate images. This appears in the news monthly.

Moral of the story: If you are poor and unsophisticated don't fight the rich, powerful, intelligent army trying to do something that they can find someone to back their actions to prevent you from doing it.
If you can't afford a second phone and contract that's a hint that you couldn't afford trouble in the first place, and the fine.
In your country they are upfront about it and demanding because objections fall on deaf ears. It's not much different in North America, just that they are sneaky to avoid complaints and only focus on major infractions so people don't suspect that they can see the lessor consequential things just as easily.
",4,2018-09-30 13:09:39Z,233
19,11,"
While other answers do provide useful insights, my alternative would be to use the phone as you normally would but isolate the app access to data
One method has been suggested by rooting the device, installing Xposed and corresponding modules of Xprivacy / XPrivacyLua. While I personally use this, rooting the device, managing additional risks due to unlocked bootloader and malicious apps gaining root access is yet another challenge - IMO beyond the scope of average user and hence easy alternative approach below
There is an open source app called Shelter that does two things

Use the ""Work Profile"" available in Android from version 5.0 (Lollipop) onwards
It allows you to install the app of your choice, in this case, your spy app in the work profile. Once it is installed on the work profile (by cloning) , it essentially is on an island and can't access any data that is outside your work profile (your photos, emails, SMS or any other app information). You can safely uninstall the original app and if police check, show them your app, which has a padlock icon indicating it is on your work profile. You can keep the app always in your recents or Overview and pull from there so that they don't even see the icon

In simple terms, the spy app has nothing to spy on!
Download Shelter - an open source app from F-droid or GitHub. Quoting the developer, relevant use case

Run “Big Brother” apps inside the isolated profile so they cannot access your data outside the profile

(Emphasis supplied) 
I am currently using this, in addition to XprivacyLua. For more details see WhatsApp: How to isolate contacts and photos - Privacy concerns 
",3,2018-10-01 09:03:49Z,235
19,12,"
For a rooted phone (without you will have a hard time to stop the app from doing what it is doing), you can use AFWall+ to prevent the app from phoning home and XPrivacy.
XPrivacy on Android up to 6 is better than XPrivacyLua (Android 6 and newer), because the old version can block more different things.
This setup works best, if you can install the app voluntary at a time nobody is watching you configuring the security solutions. If you can only get the app when a policeman stops you, it is a bit harder to hide the security apps.
",2,2018-09-26 09:49:13Z,236
19,13,"
I wouldn't be shocked if there isn't a fake version somewhere--looks exactly real but doesn't actually spy.  Unless the police are actually checking functionality in some fashion this would stop them.
As others have said, though, getting caught fooling the cops would not be a good thing!
",2,2018-09-29 02:59:38Z,237
19,14,"
There are a lot of great answers, but most of them give solutions that are beyond the ability of the average Android user.
I would like to offer a compromise between absolute privacy and getting spied upon.
I assume that your government has other means of acquiring the IMEI and phone metadata. You can stop the app from acquiring other information by denying it permission for:

Body sensors
Calendar
Camera
Contacts
Location
Microphone
Phone
SMS
Storage

Here are instructions on how to do it.
(I am not sure of your Android OS version, but the steps should be similar.)
This way the spyware would not be able to collect information about you, and therefore I assume that it will either not try to send information to servers, or it will send empty information.
However, this means that you install the app before police stops you and makes you install it. You can let the app download, kill the network connection while it's installing, and then remove permissions before it starts.
Risks
You should be aware that if the police does a deeper investigation, turned off permissions would be evidence that you have willfully tampered with the spyware. You should calculate if this risk is worthy.
This will also be a useful method of stopping other spyware (such as your flashlight app that requires contact information) from breaching your privacy.
",1,2018-10-02 04:29:12Z,238
19,15,"
You can simply resolve the servers it attempts to connect to back to localhost. 
This can be accomplished by modifying the /etc/hosts file on your device if you have root. 
Alternatively, you can use a local VPN which doesn't require root, such as this. 
",0,2018-09-25 21:42:49Z,239
19,16,"
Based on your question I cannot be sure whether you are a Chinese resident, or someon who is planning to visit (for instance a tourist).
It was already mentioned earlier that so far it seems that only certain minority citizens people are targeted (and if they were to start targeting foreigners that news is sure to spread very quickly).
Regardless of what I think about that, the following bit may actually help most tourists:
The google play store is currently blocked in China, so though it is easy to overcome, this likely means that you will not be able to get the app through the standard procedure.
",0,2018-09-27 14:53:24Z,240
19,17,"
(radically edited) 
I had proposed that one idea to consider was that

Playing dumb can be a good strategy.  
For example, ask the phone vendor, :""Hi.  My phone is working really slow and keeps crashing.  Can you show me how to do a factory reset?""  

However, I've been informed (see comments below) that even if this led to it being documented that someone else had removed the spyware, the person would likely still get in deep trouble when it was noticed that the spyware was no longer installed.   It sounds like the answer to the question, ""Is there any excuse for not having the spyware installed that would be considered valid?"" is 'NO!'  So it wouldn't matter whether the factory reset is more effective at wiping the system than the spyware is at staying installed.  I would say don't risk it, especially if you think you'd make a high-value target.  
Sadly, the software is effective at repressing dissidents and dissent even if technically it is 100% defective.  
So my answer is that secretly helping to evangelize, build or crowdfund efforts to make it not work or prove it doesn't work well or to break it or make the phone to appear broken when spies try to install spyware would be better than a more direct approach, which would paint a target on your back.  Good luck, and may the force be with you.
",-5,2018-09-25 00:03:02Z,243
19,18,"
I agree with Doomgoose. Get a burner phone for this. Alternatively, get an app called Orbot (it's Tor for Android) which can get you a bit of the privacy you so desire by encrypting your online activities. Another alternative is adding a VPN on top of the equation.
",-15,2018-09-24 15:12:23Z,245
20,1,"

That way the hacker won't know if they have got the login details correct or not.

If the information presented after login has no relationship to the person who the login should be for, then most hackers will quickly recognize that the login is probably not the real one. 
But, in order to show information which looks like it fits the user, considerable effort could be needed. It also needs to be created specifically for each user and show some true information about the user so it does not look fake but not too much so no important information is leaked. 
You cannot expect your provider to do this for you but you might try to do this yourself in many cases, i.e. add another email account, another facebook account etc. 
",66,2018-06-01 12:58:38Z,66
20,2,"
The concept you're describing is called Plausible Deniability and methods to provide it have indeed been implemented in some software, VeraCrypt being one example.
One problem with implementing it in websites, as you suggest, is that it's very hard for the website developer to come up with fake data that is realistic enough to fool an attacker while not giving away any sensitive data about the user. In encryption software like VeraCrypt, that task is shifted to the user, who is obviously in a much better position to do that.
",50,2018-06-01 12:58:54Z,258
20,3,"
Because hackers don't attack login forms
The flaw is that you assume hackers get into accounts by brute-forcing credentials against remote services. But that's futile anyway.
Any website with decent security (the ones without decent security wouldn't care about your idea either) will have a limit imposed on how many failed login attempts can be made in a certain timeframe per IP address, usually something like 5 failed attempts every 6 hours. If security is a bit stronger, accounts might also need action from the owner after a number of failed attempts, and/or the owner might be notified of failed login attempts or even all logins from new devices.
So while brute-force attacks may well be feasible against plain data (such as password hashes exposed in a breach), they are nowhere near feasible against any service with even a bit of security.
For attackers, it is thus much easier to go phishing, or better yet set up a genuine free service themselves and work on the assumption of password reuse:

",31,2018-06-02 04:07:43Z,261
20,4,"
I have never heard of any service or device implementing this either.
The case where an attacker is present and forcing you to login is pretty unlikely. They are more likely to just take your $1000 iPhone and run.
However, it is very plausible for this to happen if the ""attacker"" is a security guard/TSA officer at an airport security checkpoint. Especially if you are in a foreign country. (There was a PHENOMENAL Defcon talk on this subject a few years back.)
Websites
It probably wouldn't make much sense to implement this on a website. If you (the admin) are certain that someone who is attempting to access an account is a hacker, just block them/lock the account. Problem solved.
If the attacker is trying to access multiple accounts, they will probably know something is fishy if they are able to ""successfully"" login to multiple accounts on the first or second try.
Phones
While phones don't allow fake logins (?), but you can set them to lock after the password isn't entered correctly n times.

Attacker/TSA agent tells you to unlock phone. You intentionally enter wrong password on 1st try.
""Oh, oops, wrong password...""
You enter the wrong password again on the 2nd try.
""Sorry, my hands get sweaty when I am nervous...""
You enter wrong password on 3rd try. Phone is now locked for 30 minutes!

This of course will not work if you are reciting the password to the attacker, and they are entering it in the phone. And I think most phone lockouts only last for 30 minutes (?), during which time the attacker/TSA agent will do their best to ""convince"" you to remember the password in a back room.
Laptops
Your suggestion would be relatively easy to implement on a laptop...
Create 2 or more user profiles.
The first profile you name after yourself (first and last name). You set a picture of yourself as the profile picture. This will be your ""fake"" account. Set the password as something simple and easy to remember. Put some ""personal stuff"" in the account (music, pictures of your pet, ""work"" documents, etc).
The second account you give a generic family member name (""hubby"", ""the kids"", ""honey"", etc). Keep the default profile picture. Set a strong password. This will be the account with admin privileges on the laptop, and the account which you will use for your important/confidential work.
Now imagine a scenario in which you are forced to login...

You are in an airport in Oceania, about to fly home to Eurasia. Airport security stop you on your way through the terminal.
Security: ""Give us your passport and laptop!""
You hand them the laptop and passport. They turn on laptop, and try to login to the account which you named after yourself. Upon seeing they need a password, they demand you tell them the password.
You: ""The password is opensea. No spaces.""
The airport security enter the password, and successfully enter your fake account.
After looking around for a few minutes and not finding anything that interests them, they log out and try to login to your real account.
Security: ""Whose account is this? What is the password?""
You: ""That is my kids' account. The password is 123dogs.""
They enter the password, but are unable to login.
Security: ""That password is wrong! Tell us the correct password!""
You act surprised, and ask them to give you the laptop so you can attempt to login. They hand you the laptop, and you start typing in bogus passwords.
You: ""Those darn kids, I told them NOT to change the password! I'm sorry, they were only supposed to use that account for their stupid video games!""
The airport security confer with each other, and then let you go on your way. You safely return to Eurasia without having the confidential information on your laptop compromised.

",14,2018-06-01 21:45:19Z,266
20,5,"
It's not exactly the context you had in mind but there are in fact systems which implemented this idea. I used to work at a (somewhat sensitive) facility where each employee had two codes to disable the alarm system: The regular one and a duress code. If you used the duress code, the system would be disabled so as to not put you in danger but a silent alarm would go off at the monitoring centre. I am reading on Wikipedia that this was also considered for bank ATM in the US but ultimately ruled out.
Another similar concept is the “honeypot“. Some of them might in fact accept any credentials or serve dummy data when attacked, to be able to record what an attacker does next or otherwise exploit the situation (e.g. capture the payload of a worm).
As to why it's not more common in consumer products, online services, etc. there is simply a trade-off between the benefits (how likely a particular attack is, whether it would effectively deter criminals or just prompt them to slightly alter their technique) and the costs (more complex systems to develop, maintain and certify - which also means increased attack surface that could afford an attacker with an actual entry point, bandwidth and operating costs to serve the dummy data to all botnets constantly attacking online services, effort to create credible dummy data to fool more sophisticated attacks).
",13,2018-06-02 09:20:20Z,262
20,6,"
This is called 'Deception Technology' in the cyber world where the solution deceives cyber foes (attackers) with turn-key decoys (traps) that “imitate” your true assets. Hundreds or thousands of traps can be deployed with little effort, creating a virtual mine field for cyber attacks, alerting you to any malicious activity with actionable intelligence immediately. The traps would carry login details, dummy data, dummy system, etc to deceive the attacker by intimating like actual system.
Deception technology is an emerging category of cyber security defense. Deception technology products can detect, analyze, and defend against zero-day 
 (where the attack type/procedure is not known before) and advanced attacks, often in real time. They are automated, accurate, and provide insight into malicious activity within internal networks which may be unseen by other types of cyber defense. Deception technology enables a more proactive security posture by seeking to deceive the attackers, detect them and then defeat them, allowing the enterprise to return to normal operations.
You may refer below link for some solution providers:
https://www.firecompass.com/blog/top-5-emerging-deception-technology-vendors-at-rsa-conference-2017/
",5,2018-06-03 16:32:45Z,268
20,7,"
This has been done in the past, rather successfully, but depends a lot on what the system is. 
At one point it was not uncommon for paid access websites to auto detect when an account was logging in from too many IP addresses or with other suspicious patterns and redirect those users to a version of the site that was primarily ads and affiliate links to other sites. If done well the sharing of stolen login credentials could become a revenue center. 
There are also websites that direct users to different versions based on IP address or other criteria; the simplest version of this is targeted advertising. 
For shell access it's possible to direct a login to a chrooted jail that has only a small section of disk and specially provisioned binaries, which may not necessarily be the same as the general system. 
",2,2018-06-02 05:33:46Z,269
20,8,"
First off, I wouldn't call the attacker in this scenario a hacker. A hacker is trying to get around the security that the website offers, in your scenario the attacker doesn't care how secure your services are, he cares how easily the user is intimidated and possibly what to do with the body afterwards.
Secondly, alternate credentials that change your access has been done, but if it does more than present a restricted view of the truth, is a lot of work and of limited utility.
The reason it is of limited utility, is because your users know about it,  you must presume that any attacker knows about it as well.  Suppose you did this for an ATM card so that it showed a balance that was less than a hundred dollars in order to limit your loss. Either the attacker asks for both (in which case the victim has at best a 50% chance of not loosing more) or simply includes as part of his demands that it produces more than that -- ""if I don't get at least 200 you're dead"".
It's not totally useless, but is only effective against an ignorant attacker.  Relying upon the attacker not knowing something is called security through obscurity, aka ""they got it"".
",2,2018-06-02 12:16:20Z,270
20,9,"
For web and  a remote attack, as many folks here stated before, appart from the difficulty of creating fake user's content  , there is the problem of : how do you know it's a compromised login?
I mean, if you assume there is some sort of suspicious activity , like a brute-force attack, you can just block the login for that IP and maybe for that account itself for a while (until the real owner somehow validates its identity)
The only usefull cases are forced logins, that's another story and a pretty cleaver idea. Here is the implementation I imagine for a social network:

The user creates himself an account with dummy data, and set it as his dummy account.
When there is a forced login going on, like you jelous gf or bf extorting you to login , then you put the dummy password and there is! you are logged in your beautiful self created dummy account.

BUT Its  not a perfect solution either. The attacker probably would know you and, if it's your crazy ex  for instance, she may just check her chatlog with you and will know you just logged on the bogus account. 
This is specially relevant because it will be a public  well known feature of the platform you are , so anybody who forces you would be able to check wheter are you or not.
For banks or other sites, it's a pretty good idea.
",1,2018-06-02 19:10:47Z,253
20,10,"
The problem is that your users have to know about it, thus you must presume that any attacker knows about it as well. 
",-1,2018-06-02 12:53:22Z,271
20,11,"
To me, this sounds as if you were inviting the attacker to dance with you. 
""Hey attacker, you want to hack my website? Well here is a fake login web site for you!""
You certainly do not want to invite the attacker to dance with you because he might find it amusing and challenging which would give him even more motivation to try to hack into your website.
",-2,2018-06-04 15:22:26Z,273
21,1,"
Let's go through the process of what actually happened:

Telegram requires a cell phone number to be linked in order to create an account. 
To verify that the number exists, they send out a verification code for you to enter in the app while creating the account.
This person obviously didn't want their own cell phone number to be linked to the Telegram account, which is an indication they might use it in malicious ways (terrorism, hacking, …), so:
They tried to set you up to get that code.

So to answer your questions:


Was this a scam?

I'd call it gray zone. But yes, someone did try to scam you for a Telegram verification code.

If yes, did I prevent the scam by denying this user the code?

Yes, you did well, using common sense :-)

If not, what's at risk? Is my bank account at risk, for instance? (I only gave the user my cell phone number. No name, no address, nothing else whatsoever)

That depends, how visible are you on the internet? If you have domain names registered with that number, they can find that. Facebook linked? They can likely find that depending on privacy settings. In brief, if you've linked that number to a lot of accounts, they might find those accounts and the information that's publicly in them.

If I am at risk, what do I need to do to prevent any theft of my information, passwords, money, etc.?

""At risk"" is a heavy word in this situation. They just want a Telegram account; I find it likely they just moved on to one of the other 50 people they PMed while talking to you. 

Just keep an eye open for ""strange things"" (like hacking attempts to other accounts etc.), research yourself with that phone number and see if what you can find needs changing. 
",108,2018-03-29 16:41:00Z,276
21,2,"
It could easily have led to a scam. The request was to get access to Telegram using your phone number. 
Works just like Whatsaap, when you register, you enter your number and a code is sent to your phone. 
This could easily be removed by re-registering Telegram on your phone where it would repeat the process but make the other user using it unusable. 
You may not use Telegram but some of your contacts may use it, and may try to contact you because they see you on the app. They may be conned in the process as they could think its you. You were clever to think twice but next time avoid giving your number so freely.
",16,2018-03-29 20:49:58Z,279
21,3,"
There's also the risk of targeted attacks to your smartphone by having your number (e.g. Android or iOS vulnerabilities) through MMS or SMS. But the risk is low.
If changing your number is not to burdensome, you might want to change it.
And kudos on your quick thinking!
",6,2018-03-29 18:32:21Z,284
21,4,"
Contrary to what one of the other answers said, I wouldn't change your number over this. Most of the nuisance stuff from having your number comes from bulk data collection services who resell it. This girl, whomever she is, just wanted to create an account for potentially negative purposes and thought you were a useful patsy (props for having some common sense to not feed her what she wanted). I seriously doubt she'll contact you again, or even share your number with anyone.
Had you given in, I suspect she would have strung you along and tried to get you to do other unwise things (you'd be amazed what people will do for a female voice and a pretty picture, not having met the person).
",5,2018-03-30 12:21:00Z,13
21,5,"
I understood her intention i.e she wants to open a telegram account with your number. Telegram is a good app to maintain privacy but remember criminals also use telegram to maintain their privacy. Hence why she wants to do such a thing so that no one can discover her identity.
Note: Telegram is encrypted so what I mean by identity is her location.
You are completely safe because you didn't give her the code. As I earlier said, telegram is best for maintaining security, privacy.
But next time don't give your number to an unknown person :)
",2,2018-03-31 21:08:59Z,285
22,1,"
Unless you can come up with some other explanation of how this happend, it sounds like your phone has been infected by some malware. It's impossible for us to say if the infection was the result of something the factory did or something you did. Either way, you should be very concerned. I'd recommend the following course of action:

Make a backup of any data you have on the phone. (The backup could be infected, see this question, but if you have no earlier backup you either have to take the risk or lose your data.)
Do a complete factory reset, wiping the phone clean.
Change any passwords that has been stored on or entered on the device.

While you are only seeing one app, it might just be a symptom of a deeper infection.  Just removing the strange chinese app may not be enough.
",103,2018-03-27 17:13:16Z,9
22,2,"
I would be very concerned. One thing I have learnt about security, in all the years I have been trying to understand it, is that if you didn't put something there, somebody else did, and if you don't know what it is doing then the only thing to do is reset, reinstall, and be happy with everything. If you don't know why that software is there, be assured, somebody put it there for a reason and it may be a good reason but it isn't your good reason so if that phone was mine I would save all my data, and only my data, and completely factory reset it.
",31,2018-03-27 18:32:02Z,290
22,3,"
Yes, and be concerned about more than your phone. 
I can’t imagine a situation where an app was installed without your pin/password. without them first jail-breaking or otherwise significantly compromising your phone's OS. I think the only safe assumption is that anything your phone had access to, they also had access to. 
So any account your phone has access to is suspect. Particularly any email account you have setup. Look for password reset emails, check your sent items etc. If you had any kind of financial app ( bank etc) check for odd transactions etc.
If that all looks fine,  I’d follow the advice of others in terms of resetting the phone, but also go beyond the phone and reset all your important passwords etc.
",22,2018-03-28 02:29:18Z,292
22,4,"
The most concerning point is that the repair shop employee claims not to know anything about it.
The app may or may not be okay. If someone in the repair shop installs a new app, i.e. for testing if installing works again, this can be a problem, but doesn't have to be.
But if they tell you they do not know about it, either it was some malware installed it, or they are lying to you. And either way you cannot trust the phone anymore not to be infected with something which should concern you.
",11,2018-03-28 08:50:23Z,236
22,5,"

when the guy in store suggested to just delete it I did it without thinking. Here's the link to icon I found that looks the same, I suppose it's UC Browser. As I mentioned, everything was in Chinese, including name of the app. – Rafi Rosa

Yes, it was UCBrowser. This is a popular browser in China (since Google Play/Services are not available there) most Chinese phones actually come with this browser pre-installed.
I believe it was perhaps a way for the repair guy to test the phone in a familiar app... UCBrowser was found to have several flaws that would allow others to exploit, but mainly they'd only happen if you actually used the browser.

I have done a complete factory reset of my phone and changed all passwords that I used while using it. After that all of apps I have previously installed were restored by iPhone and all of my files were stored on iCloud, is it all I can do? – Rafi Rosa

I believe you did well. Even if it was just an app that the repair guy decided to use for testing, he might have done a lot more than just installing and using the browser. Not being honest/upfront about what they really did when they just needed to repair a battery is certainly not professional.
A factory reset and passwords change should be safe enough regarding your data. If the store does not have ways to profit from replacing other hardware in your phone, you won't notice any difference. However, if you need to be completely sure about its integrity perhaps a certified Apple store may perform a check and tell you what was really changed (not sure if they do this though).
",10,2018-03-29 09:23:21Z,295
22,6,"
I didn't see this addressed in the previous answers, so I thought I'd add:
If you are going to surrender control of your device, you should perform a backup and a factory reset beforehand. You should reset it again and then restore your backup when returned. It only takes a few seconds to copy your files or install malware.
It is too late in this case, but the mystery about ""what happened"" applies every time it leaves your control---not just this time because you noticed something wrong. Intruders usually try to avoid detection, so a good intrusion would be unnoticeable.
",6,2018-03-29 18:37:49Z,296
22,7,"
As already stated, anything done to your phone while out of your sight is definitely cause for concern and probably worth restoring from backup, as well as resetting passwords / 2FA for anything tied to that phone.
However, the exact way this icon got on your home screen would say a lot about how bad of a breach may be involved and how concerned to be. I can think of 2 options not already discussed:

An employee installed the app, but removed it from your list of purchased apps. You could check your purchase history in iTunes for hidden purchases to verify whether this has happened, even for a free app.
The icon was not actually an installed iOS App, but a bookmarked website / Progressive Web App that was pinned to the Home Screen for some reason to help with diagnostics. You could potentially verify this by checking your Safari history for any activity during the time your phone was at the shop. Clearing the history/cookies would erase the tracks for this, but if that's what happened then the 'app' itself isn't really a cause for alarm.

",4,2018-03-28 20:47:56Z,291
22,8,"
Yes you should be concerned. 
Do a factory reset. 
If you have backed up your iPhone since before the app, your in luck. If you didn't, less great but still preferable to wipe it clean and start over.
",2,2018-03-28 01:50:05Z,297
22,9,"
if you gave access to the phone to the store, the new app might not be in the list of your purchased apps, simply because an employee in the store logged off, and installed it with their user.
Nonetheless, I would still handle the phone and passwords used/cached there as potentially compromised.
However, as mentioned in my first paragraph, the app installation might be just routine, and someone forgot to delete it.  
",1,2018-03-30 02:04:04Z,298
22,10,"
I would recommend you to replace your phone instead of resetting it. The store has had the access to the phone's hardware and firmware and could have replaced them with malicious counterparts in addition to installing this app.
",0,2018-03-31 16:12:23Z,299
23,1,"
They will most likely be able to see an itemised bill showing who you called and when. They will also be able to see mobile data usage.
If the phone is enrolled in the organisations mobile management system, they may be able to monitor and control app usage as well as monitor and control internet traffic.
UPDATE: In a worst case scenario they could potentially install full monitoring and install things like key loggers, this is very unlikely however. Additionally, depending on what part of the world you live in, there are restrictions on what they can legally collect, especially without consent.  
The best thing to do is ask your company what you can and can’t use the phone for. If it is a large organisation they should have a security policy and an acceptable use policy.
",33,2017-07-28 10:32:03Z,301
23,2,"
In addition to the other points that have been made, many websites use an SMS message to your mobile phone as a form of two-factor authentication, even though the best practice nowadays is not to do that. So they could at any time divert messages to your number to themselves, and use that to take over your Internet accounts, even accounts that you never use with the phone. 
",5,2017-07-28 14:42:27Z,249
23,3,"
My thoughts about having a company-managed phone or a company-paid contract, and your company Exchange. I include relevant information for iPhone devices (since the amount control varies across mobile platforms).
Phone
If you are using Exchange ActiveSync to access your company email/contact/calendar, note that ith ActiveSync, your company can:

Configure a Mobile Phone for Synchronization
Disable a Mobile Phone for Exchange ActiveSync
Enable a Device for Exchange ActiveSync
View a List of Devices for a User
Configure Device Password Locking
Recover a Device Password
Perform a Remote Wipe on a Mobile Phone
Install SSL Certificates on a Windows Mobile Phone
Configure Mobile Phones to Synchronize with Exchange Server

Apple and Google also have similar centralised management options. There may be other 3rd party services or software that perform similar tasks.
If your phone has been set up with Google G Suite, your company can:

Automatically synchronize email, calendars, and contacts with users’ devices.
Turn on or off features, such as lock screen widgets, Siri, My Photo Stream, Handoff, and iCloud Photo Sharing.
Protect your organization’s managed data by controlling which apps can be used to open documents and attachments.
Control Apple® iCloud® backup and sync, and turn on backup encryption.
Apply device-management controls, such as account wipe, encryption, and screen lock.
Keep work data secure with G Suite apps, such as Gmail, Google Drive, and Calendar. For details, see Get mobile apps for iOS devices. 

If your company was using Apple Device Enrolment, they can access, amongst other things:

Global network proxy for HTTP
Allow iMessage, Game Center, iBooks Store, AirDrop, Find My Friends
Allow removal of apps
Allow user-generated content in Siri
Allow manual installation of configuration files
Allow conﬁguring restrictions
Allow pairing to computers for content sync
Allow account modification
Allow cellular data settings modification
Allow Erase All Content and Settings
Restrict AirPlay connections with whitelist and optional connection passcodes
Enable Siri Profanity Filter
Single App Mode
Accessibility settings

Note that a remote backup includes a lot of your phone information (installed apps, etc). I'm almost certain your company could access this information if they wanted to.
Contract
This depends on whether your employer simply pays for your contract or they are the contract owners and therefore have access to the mobile operator online tools.
If they own the contract, they can definitely access the detailed list of phone calls (including numbers, time and duration). They may also be able to access your Internet data pattern usage (which may reveal some of your habits).
I know of no provider (Europe / Spain) that allows customers to access a detailed list of websites visited, or IPs accessed, but I might be wrong here. I doubt this since it would require your phone provider to do deep packet inspection and maintain quite expensive log and data mining facilities... it is definitely doable but I never heard of this.
General info
Anyone using their phone to access their company resources (even simply checking mail via POP3/IMAP) is usually revealing their approximate geographic position to their company in a periodic fashion.
If you use your company proxy or VPN to access any of their resources, note that your Internet traffic or browser behavior may be being forwarded through your company servers, which would allow them to track which sites you visit (and the content if those sites don't use HTTPS).
If your company has installed custom certificates on your phone, they could potentially also view any HTTPS traffic if you are using their proxy.
TL;DR
In summary, I'd recommend you to:

Find out if your phone has been enrolled to a remote management system.
Check what kind of information your phone company provides to their (business) customers about their contracts.
Check if your company has installed any extra software, proxy/vpn settings or  certificates on your phone (in case you handed your phone to them at any time OR allowed them remote administration).

",5,2017-07-28 14:39:45Z,228
23,4,"
The holder of a contract can almost always see the activity on that contract, that would be numbers called and very possibly the internet usage as well, that is the sites you visit and how much data you have used going to them. In most parts of the world your employer has the right to monitor your activities on business supplied services, and many use that right. Some industries are regulated and have an obligation to monitor what you do, and in these cases your calls may be recorded and your internet traffic actually saved so your actions can be reconstructed - this is a rare case. 
If you control your actual device then they can't see what you are doing on it, however you cannot assume that anything you do on your company phone is private as the connections it makes will give them a great deal of information. You could use TOR, but personally if you do things you really want to keep private then you're better off getting your own contract for your phone. 
",2,2017-07-28 10:40:09Z,101
23,5,"
There are two levels of access (okay, make it three), depending on the carrier-offered options (with ""offered"" meaning ""offered to your employer"").

Certainly all billable traffic gets forwarded to your employer, so:

numbers called and, almost surely but not necessarily, calls received even when not actually billable.
SMS/MMS sent (but not their contents)
network traffic statistics (i.e., volume in gigabytes, but not its contents).

The SIM could route your traffic to a private MPLS group and have it analysed/filtered. How much or how little, it depends on lots of factors. Potentially everything (except for HTTPS encryption, and even then, barring MitM attacks) could be accessible with you being none the wiser. This also includes SMS/MMS contents. Your employer must not simply pay your navigation, he must supply a tailored SIM.
More simply, all the traffic could be routed through a dedicated VPN, no matter what the device settings are. So your navigation effectively happens from a device inside your employer's company. Same rules apply as for #2, but this situation might be apparent through tracerouting or simply checking your apparent source IP address and/or comparing the internal device IP address (e.g. 10.123.45.6) and what appears to a third-party site such as WhatIsMyIp.com (e.g. $IP_IN_YOUR_EMPLOYER_NETWORK). This can be done even with your own SIM inside.

I strongly agree with @TheJulyPlot's answer: ask your employer
a. What is permissible on that device,
b. Whether private (i.e. third-party VPN) navigation is allowed.
If the answer to (b.) is a NO, then you should purchase a second device or look into a dual-SIM device -- but, first, ask whether that is permissible. I had a position once where I was not allowed to bring my own phone - or any other kind of electronic gadget, including my non-networked PDA (!) - inside the premises. You don't want to shell out good money for a device you then can't use.
Final note (especially for case #3): in some circumstances you might be getting flak not just for something you did but for something that was sent to you. So consider who you give (or already gave) that phone number to. ""Moving"" your own SIM with number inside your employer's accounts could backfire spectacularly if they objected to some SMS sent by an Overly Attached Girlfriend or similar.
",1,2017-07-29 13:12:37Z,0
24,1,"
Users should not hash their passwords before sending it to a website. If the user hashes and sends the hashes credentials, the hash is acting exactly like a password and actually defeating the purpose of hashing. If an attacker steals your password hashes, they can just send those to your server to login.
You might want to read about Pass the hash attacks.
You must hash server side.
",7,2016-10-11 01:21:41Z,306
24,2,"

When a user logs into their account on my server, should they send
  their raw data to me, and then I bcrypt compare them, or are they
  supposed to hash and and I directly compare?

You take the data they send you, hash it using the unique salt stored in your database next to their username, and compare that to the salted-hashed password stored in your database next to their username. This is an absolute MUST for security.
It's fine if they hash their password before sending it to you (assuming they always do that; probably because the client does it automatically). This is a nice step if you are trying to protect your password-reusing users, but the hashed-before-it-gets-sent-to-the-server password is really just acting as a pseudo-randomly-generated password.

I ask this because someone posted about hashing on mobiles being slow. It would make send to take load off of the server, and make sense about the slowing down brute forcing, and also protect the user from snooping.

Hashes aren't ""slow"" by any definition for users. They take a very small fraction of a second. Unless you're getting hundreds of login attempts per minute you shouldn't need to worry about the server load from hashing passwords.
Hashing the password on the mobile doesn't do anything for the security of this account (all it does is keep the original password from being transmitted over the wire, which is only terribly useful if the wire can be intercepted and the user is sharing passwords between services (in which case the user's other accounts are now compromised). If the user isn't reusing passwords and the wire can be intercepted then it doesn't matter if the original password or a hashed version is being sent, because what is being sent is what's used to authenticate the user to this server).
When we talk about a hash being used to slow down brute forcing, it's with the assumption that the attacker stole your database (containing the salts and hashed passwords) and is now trying to hash common passwords against each of the salts and looking for matches. A hash that's slow because it's happening on a mobile doesn't do anything to protect the user, since the attacker will undoubtedly be using faster hardware. A slower hashing algorithm and/or one with more rounds of hashing means the attacker has to wait a tiny tiny bit longer for each crack attempt. So a slow hash that takes, say, 0.0001 second instead of 0.00001 seconds would mean they could only try one-tenth as many hashes in a given amount of time. Note that if you hash the user's password at the client and then again on the server, that's just adding one round to the overall hash time (presumably the attacker grabbed a copy of the client code and has noticed that the passwords get hashed before being submitted and has written their attack code to take that step, too).

If I were to do this, wouldn't I run into the exact same problem -
  slow experience for mobile users, if so is it something that is just
  expected as part of logging in. Also, wouldn't the user generate a
  random hash which wouldn't match the one stored in my database?

The hashing on the mobile side is still pretty fast, like I said, so it shouldn't have a noticeable delay for the user.
The hashed password becomes the password used to access your server, so to your server it just looks like everyone is really good at generating random passwords. But that's why you still need to hash the ""password"" they sent you with a salt (and that will match what you've stored in your database) because otherwise once someone gets your database they can just directly send those hashes to you (bypassing the hash function in the client) to authenticate to your server.
",1,2016-10-11 05:15:27Z,307
24,3,"
This probably is a duplicate question. But I seem to keep seeing people recommend hashing passwords client-side while ignoring the true complexity of the issue and the near impossibility that anyone other than an information security scientist is going to get it right.
Bottom line, if you're making a web or mobile app, use HTTPS, configure it correctly and hash passwords on the server. Use bcrypt or scrypt, or if it's the easiest library for you to use, use pbkdf2. Those libraries are well designed. You're not going to invent better. BUT; if you use them to hash the password on the client, then send the hash, then you just made the hash your password. And if you're a mobile app developer, don't you dare set the flag in your HTTPS setup that tells it to skip checking the certificate chain.
It's dang complicated
Occasionally people mention using nonces, etc. Pass-the-hash, challenge-response authentication is nothing new. And various half-baked pass-the-hash schemes have been broken more than once. Consider Microsoft Lan Manager: LM, NTLM and NTLM with session security are all broken. NTLMv2 is apparently still secure, but it's the fourth in the chain. Microsoft has a few resources to throw at the problem. You're going to do better on your own? Kerberos is an example of a solidly secure authentication and authorization system with a lot of science behind it, which generates tickets and applies nonces and passes those around instead of transmitting the passwords.
But Kerberos is quite complex, and relies on computers having trust relationships with each other, and on computer clocks being in sync. It's complicated.
Half-baked attempts to pull off something similar between a web browser and web server, unless you're using the actual Kerberos protocol supplied by a competent, well-vetted vendor, are bound to end in tears.
Security is complicated, subtle and hard.
HTTPS is pretty darn good
Perfect? Maybe not, but current versions aren't broken. If the server is using a strong certificate (2048 bit key) and strong cryptography (TLS 1.x or greater), it's pretty darn solid. Sending your password via HTTPS is not the same as passing it in plain text.
Users should not use the same password for multiple sites
Really, seriously, if they do, it's ultimately on them. They've been warned so many times now. Even the Today Show anchors know not to use the same password on multiple sites now, for crying out loud.
Don't make the hash the actual password, believing you have implemented security
If you just hash the password on the client side and store that value in the database for comparison, then the hash itself is the password. Period.
Are you going to build a full challenge-response pass-the-hash system using nonces, and do it without introducing fatal bugs? Do you really have the time, budget and expertise for that?
There may still be a benefit for the user to hash their password before sending it. If the hash is intercepted, the attacker couldn't use it to log in to OTHER sites.
However; if that is done, and you don't have the chops to create the actual equivalent of Kerberos or NTLMv2 just for your app, it should be done completely independent of anything the server does or expects.
The server must compute the hash
Given all the preceding, the server still needs to hash whatever is passed to it (call it a hash, call it a password, same thing in this context), with salt that is stored on the server, completely blind to anything the client is doing. 
In other words, if the client is sending a hash, great, but the server doesn't know or care. It's just a password.
You're not trying to prove that the client knows the hash value. You're trying to prove that the client knows the original secret (aka ""the password"").
So the server hashes the incoming password fresh each time, using salt that is stored on the server, and compares the computed hash to the stored hash. If they match, authentication has succeeded. Otherwise, authentication fails.
So very many things can go wrong if you try to be clever
You're going to get it wrong. You just are.
Imagine this example:

Your client hashes passwords
Your server stores said hashed passwords
Attacker social engineers your CEO and steals your account database
Attacker has a database full of passwords, not hashes, but actual passwords to actual accounts on your system.

Part of my point, I guess, is that the very question ""Should I hash on the client or the server"" belies the true complexity of the issue and far too many answers either fail to address the complexity at all, or gloss over it in ways that make a system less safe, not safer.
",1,2016-10-11 06:05:48Z,57
24,4,"
Preferably both should be done, for different purposes.

The user hashes their password with a salt specific to the website, to prevent the (possibly hacked or misconfigured) server stealing their password if it is too similar to the user's password on other websites.
If you want better security, you can let the user hash their password more times, in case the server is hacked and the hacker was listening to the communication, and doesn't want announce the fact it is hacked.
The server hashes the password with a salt before saving to or comparing with the database. So it won't be too disastrous if the database file is stolen.
In addition, someone could hash the password using a slow algorithm, the earliest the possible, to slow down brute-forcing if the user is using a weak password.

But in practise, web browsers don't have the support to do 1 and 2 right. And for 1, even if a service doesn't use web login, it doesn't help much if only one service is doing that. So usually only 3 and sometimes 4 is implemented.
The user could use password management softwares to do 1. But most people don't. So sometimes we think 3 is also mitigating the problem 1 could solve. That may lead to some confusion.
For your question, it makes sense to move 4 to the client side. But 3 on the server side should be considered more important, and shouldn't be removed. In this case you are going to hash the password using a slow algorithm on the client side, and hash the result again using a fast algorithm on the server side. But I guess most services just don't use an algorithm which is that slow that is worth the hassle.
",0,2016-10-11 04:26:13Z,308
24,5,"
Already answered on StackOverflow. You want hash passwords on the client device because you want to touch the user's password as little as possible. 
1) If the user ever complains about being hacked you can be confident that your server didn't give up the user's password, because your server never sees the user password.
2) You want to minimize the amount of time that a user's password spends on any storage device- yours or theirs. If you copy the plaintext to your device then their password exists in your memory and disks. You want to avoid this.
3) Transport security isn't perfect, so if you transmit the user's password you're making them vulnerable in the event of a compromise. If you hash on the client side then you ensure that a man-in-the-middle will only ever get hashes. This is enough to allow the attacker to break into your server, but your user is protected in the (common) event that they re-use their password on multiple sites and services.
4) You can address Stoud's concern by reversible-hashing the user's hash while in transport. 
5) Then hash it again once it gets to your server. 
https://stackoverflow.com/questions/3391242/should-i-hash-the-password-before-sending-it-to-the-server-side
",-2,2016-10-11 03:17:40Z,303
25,1,"
It might improve security but it's a very strange way of doing things.
What you are suggesting is to start with an image with ""unnecessary"" software installed and remove some of the stuff you don't need (bash, echo,...). This does not only risk breaking things, it also increases container size because the base layer with the unnecessary tools is still shipped.
A better approach is to start with a minimal image. Not alpine, that's still a full os. The more minimal approach is to use a base image that only contains the required language runtime like googles distoless images https://github.com/GoogleCloudPlatform/distroless . For static binaries, you can even start with an empty image.
That way you get an image that is as small as possible and does not provide the ""attack surface"" of bash.
",6,2018-03-22 13:59:51Z,309
25,2,"
You shouldn't remove bash or any executables that exists in the base image. A lot of executables are interlinked, and removing one executables may cause other apparently unrelated executables to fail to run correctly on some corner case. Instead you should use a base image that doesn't contain anything that you don't really need. alpine makes a great minimalist base image, weighing at 5MB, it contains just the most useful stuffs and nothing more.
If you know that your application doesn't use any external binary or shell, or if you know exactly which external binary you will be needing, you may want to consider compiling into a statically linked binary and then basing your container of the scratch image.
",1,2018-03-22 10:53:34Z,310
25,3,"
The answer is: it depends. Removing bash and all other interpreters reduces the attack surface of the container. The same applies to e.g. compilers, nc, wget, ...
The real question is: is this the best way to spend the money?
E.g.

Testing and production images probably diverge 
creating a patched container gets (slightly) harder
Troubleshooting gets way more difficult 
....

A better way to spend the money might be to harden the application itself by investing in other security measures like regular penetration tests, security monitoring, automated patching, ...
",0,2018-03-22 11:15:18Z,311
25,4,"
It depends to what extent you are following the Container philosophy. If you don't need it, you should avoid using anything not strictly necessary. You should define and refer to everything you need in the Dockerfile, and the entrypoint of a web server should be the apache (or any other web server you are using) binary itself.
But more importantly, take into account that many binaries (as ls, find, etc.) are linked to the base image layer of the container, so you should leave them untouched, or make a more custom base image. Any ubuntu, debian, etc., base image will have this binaries, and tampering the base image is not recommended.
It would be a much more secure approach to define an isolated network for databases, or other backend entities, leaving just the port you are exposing to the world as attack surface.
Obviously, hardening the machine you into what you are containerizing your services is critical.
I would not worry much because of these binaries. I would worry much more about securing the access to the Docker commands to avoid invoking these binaries during the container runtime.
",0,2018-03-22 10:17:05Z,313
25,5,"
I wanted to add some more resources to the answers so far. In general, it's considered best practice for an OS container image to be as basic as it can possibly be. Removing unneeded shells is just a start. There are multiple hardened OS images out there specifically for a secure container deployment, but there are also many other things to consider, like lowest possible privileges, Read-Only file systems, and much more.
This page: https://access.redhat.com/articles/2958461 Has multiple links to many good articles. Dan Walsh's Red-Hat blog is also very good for deep level information. https://rhelblog.redhat.com/author/dan-walsh/
This page has lots more that is directly helpful to this question in the sense of practices even more useful than removing un-needed applications: https://access.redhat.com/blogs/766093/posts/2334141
And here is a hardened OS for containers. Full disclosure: I haven't used it though, our Systems Engineering worked with my security team to build a gold image for us on our cloud deployment. It's more of an example: https://coreos.com/os/docs/latest/
",0,2018-03-23 01:37:46Z,314
26,1,"
XSS is a form of code injection, i.e. the attacker manages to inject its own malicious code (usually JavaScript) into trusted code (HTML, CSS, JavaScript) provided by the site. It is similar to SQLi in that it is caused by dynamically constructed code, i.e. SQL statements, HTML pages etc.
But while there are established techniques to solve SQLi (i.e. use parameter binding) it is even harder with XSS. To defend against XSS every user controlled input must be properly escaped, which is much harder than it sounds:

Escaping rules depend on the context, i.e. HTML, HTML attribute, XHTML, URL, script, CSS contexts all have different escaping rules.
These contexts can be nested, i.e. something like <a href=javascript:XXX> is a JavaScript context inside a URL context inside a HTML attribute.
On top of that you have encoding rules for characters (UTF-8, UTF-7, latin1, ...) which again can be context specific.
The context and encoding need to be known when constructing the output: while most template engines support proper HTML escaping they are often not aware of the  surrounding context and thus cannot apply the context specific escaping rules.
On top of that browsers can behave slightly different regarding escaping or encoding.
Apart from that you'll often find in grown code with no proper separation of logic and presentation that HTML fragments gets stored in the database and it is not always clear to the developer if a specific value is HTML clean already or need to be escaped.

And while there are some simple ways to protect against XSS by design (especially Content-Security-Policy) these only work with the newest browsers, have to explicitly enabled by the developer and are often only effective after large (and expensive) changes to the code (like removing inline script). 
But still, there is a good chance to do it right if you have developers with the proper knowledge and are able to start from scratch using modern toolkits which already take care of XSS. Unfortunately in many cases there is legacy code which just needs to be kept working on. And development is usually done by developers which are not aware of XSS at all or don't know all the pitfalls. 
And as long as web development is done in environments very sensitive to cost and time the chances are low that this will change. The aim in this environment is to make the code working at all in a short time and with small costs. Companies usually compete by features and time to market and not by who as the most secure product. And XSS protection currently just means additional costs with no obvious benefit for many managers.
",32,2016-07-07 11:54:36Z,66
26,2,"
It seems easy, but is hard
First of all, the attack surface is huge. You need to deal with XSS if you want to display user input in HTML anywhere. This is something almost all sites do, unless they are built purely in static HTML.
Combine this with that fact that while XSS might seem easy to deal with, it is not. The OWASP XSS prevention cheat sheet is 4 000 words long. You need quite a large sheet to fit all that text in. 
The complexity arise because XSS works differently in different contexts. You need to do one thing if you are inserting untrusted data in JavaScript, one thing if you insert into attribute values, another thing if you insert between tags, etc. OWASP lists six different contexts that all need different rules, plus a number of contexts where you should never insert data.
Meanwhile developers are always searching for panacheas and silver bullets. They want to deletage all the hard work to a single sanitize() function that they can always call on untrusted data, call it a day, and go on with the real programming. But while many such functions have been coded, none of them work.
Let me repeat: XSS might seem easy to deal with, but it is not. The main danger lies in the first part of that sentence, not the second. The percieved simplicity encourage developers to make their own home brewed solutions - their own personalized sanitize() functions, filled with generalisations, incomplete black lists, and faulty assumptions. You filtered out javascript: in URL's, but did you think about vbscript:? Or javaSCripT   :? You encoded quotes, but did you remember to actualy enclose the attribute value in quotes? And what about character encodings?
The situation is quite similar to SQLi. Everybody kept looking for the one sanitation function to rule them all, be it named addslashes, mysql_escape_string or mysql_real_escape_string. It's cargo cult security, where people think it is enough to ritually call the right function to appease the Gods, instead of actually embracing the complexity of the problem. 
So the danger is not that developers are blissfully unaware of XSS. It is that they think that they got the situation under control, because hey, I programmed a function for it.
But why is it hard?
The web suffers from the problem that structure (HTML), presentation (CSS) and dynamics (JavaScript) are all mixed into long strings of text we put in .html files. That means that are many boundaries you can cross to move from one context to another.
Again, this is the same as with SQLi. There you mix instructions and parameter values in one string. To stop SQLi, we figured out that we have to separate the two completely and never mix them - i.e. use parametrised queries. 
The solution for XSS is similar:

Separate the scripts from the HTML.
Turn off inline scripts in the HTML.

We created an easy way to do the second part - Content Security Policy. All you need to do is to set a HTTP header. But the first part is hard. For an old web app a complete rewrite might be easier than to surgically disentangle the scripting from the HTML. So then it is easier to just rely on that magic sanitize() and hope for the best.
But the good news are that newer apps often are developed after stricter principles the use of CSP possible, and almost all browsers support it now. I'm not saying we are going to get the percentage of vulnerable sites down to 0%, but I am hopeful that it will fall a lot from 60% in the coming years.

TL;DR: This turned into a rather long rant. Not sure if there is much useful information in here - just read Steffen Ullrichs great answear if you want a clear analysis.
",22,2016-07-07 13:07:29Z,9
26,3,"
My set of opinion on security and XSS:

Rule of programming: You can't know everything. Sooner or later you are going to make a mistake.
Rule of programmer: A programmer works 12h a day: 3 is discussing with other programmers random things, 3 is thinking at other things, 3 is discussing on what it should code, 3 it's programming .... projects are made for 12h a day of almost non stop programing.
XSS is simple. And there are a lot of inputs waiting for XSS.
Security still comes last and most of the time is seen as a deficient.

",7,2016-07-07 11:31:39Z,322
26,4,"
As mentioned in the answer to a similar post of yours (SQL injection is 17 years old. Why is it still around?):

There is no general fix for SQLi because there is no fix for human stupidity

Developers sometimes get lazy or careless and that causes them to not check the application they are developing. 
Another popular reason is that the developers aren't aware that there is a security issue. They never went through any security training and so they don't know what is a potential security threat and whats not. And for company's that don't understand that a penetration test is important this lack of security knowledge is a potential threat by itself. 
Simple security issues such as SQLI and XSS will only be fixed when a company decided to spend time on code review and testing. Until then you have 65% of all websites globally suffering from the XSS vulnerability.
",6,2016-07-07 11:31:46Z,321
27,1,"
Consider this a very, very, very short description. The answer to your question can encompass an entire set of books.
Security is about risk management. Making something secure is about lowering risk. The lowering of risk goes together with putting controls into place, each control may have a certain price and can mitigate a certain risk, by lowering the likelihood a vulnerability can have a negative impact on your assets. Assets in this context can be anything a company considers valuable, this includes systems, applications, but also people, infrastructure etc...
A and B are both valid, except A would probably depend on cost. Security costs money and it doesn't make sense to spend loads of money to secure an asset if the security costs more than the asset is worth. If the value of an asset increases over time it may make sense to ""make it more secure"" over time by spending more on extra controls.
",1,2016-03-20 12:39:51Z,116
27,2,"
Risk management is the key to answering this deceptively simple question. To make data (or a device) perfectly secure, one would have to erase all knowledge of it. Destroying the data/device to make it secure makes it unavailable, violating the CIA triad cited by @SilverlightFox. By using the Enigma encryption device, the Nazi made it vulnerable. 
As long as the future (and life) is uncertain, ""as secure as possible"" is a never ending and fruitless task. So making something more secure is more to the issue of securing something. However, that raises the question: how much more secure does the data (device) need to be? How much more effort and ingenuity is appropriate to ""secure the asset?"" 
This reminds me of the following joke:

Two men are walking through a forest.  Suddenly, they see a bear
  running towards them.  They turn and start running away.  But then one
  of them stops, takes some running shoes from his bag, and starts
  putting the on.
          “What are you doing?” says the other man.  “Do you think you will run fast than the bear with those?”
          “I don’t have to run faster than the bear,” he says.  “I just have to run faster than you.”

So ""how much security is enough"" depends on the context. That's why most homes have key locks and banks go to the trouble of installing vaults. The impact of a breach on the former - though quite personal to the homeowner - is likely less costly in total that the impact to the bank. Thanks to the measures banks take to protect the contents of their vaults (steel, alarms, guards, etc), homeowners pay banks to hold their valuables in a lock box rather than replicate the bank's security measures for their homes. The exception that proves the point is the visibly wealthy homeowner who declines to put all her valuables into the bank vault. What is the point of valuable paintings, sculpture, and jewelry if you can't enjoy it?
So how does one decide how much effort or money spent on security is enough? One tool is Annualized loss expectancy. In short, ALE is the cost (tangible and intangible) of a loss times the probability that incident will occur in a year. If the bank has $10,000 in the vault and there is a 10 percent chance of getting successfully robbed, the bank should no more (or less) than $1,000 to protect the vault. Bruce Schneier points out that the calculation get complicated and messy very quickly. 
Another approach is return on security investment (ROSI). Mirroring the business principle of return on investment, The cost of a bad thing happening is called risk exposure. A security solution mitigates that risk by some percentage. Multiplying the exposure by the percentage mitigated gives the expected return. Thus ROSI is used to identify the appropriate expenditure on securing an asset. However ROSI is not without its detractors, notable among them Bruce Schneier. 
Andrew Jauquith's Security Metrics is another worthy read on the subject. Jaquith writes on p 33 ""...practitioners of ALE suffer from a near-complete inability to reliably estimate probabilities [of occurrence] or losses."" You can read his clear and cogent alternatives to ALE and ROI. 
",1,2016-03-20 16:45:58Z,326
27,3,"
Both are applicable.
Securing something means reducing the risks associated with running such a network, system or application.
In information security the risks are often in the CIA triad - Confidentiality, Integrity and Availability. For example, an application may have a security vulnerability that affects the confidentiality of data. Securing the application against this risk would involve remediating the security vulnerability or removing the threat. However, it is not usually possible to achieve the latter. It may not be practical to make this vulnerability ""as secure as possible"", however the risk can be mitigated to a level that is acceptable by the business, so often the action that is taken is to make it ""more secure"".
",0,2016-03-20 12:43:27Z,327
27,4,"
Security usually refers to secure sharing, meaning techniques of sharing certain information only with the people you want to share it with.  If you do not have anything to share, then it is easy to be secure, just do not give anyone any information about yourself.  However, that approach is quite impractical, as living and doing business require sharing certain information: financial information with business partners, medical information with doctors, identification information with business partners and government, personal information with friends and family, etc.
Usually when people talk about a 'lack of security' they are referring to a demonstrated vulnerability and usually a sever vulnerability.  In some sense, hackers, cons, fraudsters, and thieves are always testing your security, both electronic and physical.  Most of the time you, your computers, and your life are okay, you loose no information to the wrong person, and we call this 'secure'.  At other times, there is a problem, such as a break and entry in your home, or perhaps a data breach on your computer.  In this case we call this a security problem.
The process of recovering a secure situation after a breach involves risk analysis and vulnerability repair.  If you are interested in legal action, then forensics comes into play.  Fixing the security holes is fundamentally different than getting justice which involves law enforcement.  Most computer security professionals focus on fixing security holes first, and forensics second.  Although, during the vulnerability analysis phase, there is a certain amount of forensics used to discover the vulnerability, but usually it does not pass the standard used in a court of law.
",0,2016-03-20 15:07:32Z,328
27,5,"
In terms of cryptography, secure means not broken. And broken means, there is a method to decrypt better than brute force.
And as you know, brute force is the method of desperation where you need to try all possible keys. 
",0,2016-03-20 16:17:23Z,329
28,1,"
If you are attempting to be compliant and/or avoid the breach notification rule under HIPAA in the United States, then obfuscating data relationships within your database will not help you at all.
Under HIPAA, if data is encrypted you need not notify patients/insurance providers/etc about the breach.  However, encrypted means that it was encrypted using a FIPS 140-2 compliant cipher and only this encrypted data was obtained by the third party.
On a database, you can make use of tools such as transparent database encryption, or third-party encryption suites.  This would mean if someone got a copy of the database, it would be FIPS 140-2 compliantly encrypted, and thus not triggering breach notifications or other consequences under HIPAA. 
However, simply having a hard to understand database is covered nowhere under HIPAA, and security-by-obscurity doesn't win you any points on a CMS audit...even if you explain obscurity is a ""compensating control"" for a HIPAA requirement.
Also while security typically means keeping unauthorized individuals out, the next worst thing to be able to happen to an organization keeping PHI data is losing that data.  Having a DB with complex relationships makes it easy to lose that data, and integrity of your data should be considered a security concern when dealing with HIPAA.
Note that transparent database encryption or other on-line encryption methods will do little against the online compromise of the database, e.g. through SQL injection.  If someone gets a DB dump by running SELECT * statements, this data wouldn't be encrypted.  For this, youl'l need to secure/segment your database users and also look into best practices for securing the frontend web application. 
In short, encrypt the database somehow; but DB encryption should only be a small part of a larger HIPAA compliance strategy.  Security by obscurity, however, should not be any part of your security strategy. 
",4,2015-11-03 18:39:01Z,339
28,2,"
You asked for best, let me start by qualifying my answer as best in a sense that it is my best effort to allow you to make your own choice perhaps with more confidence. Since 'best' can be quite subjective, and I am not the all powerful knower of things, I'm quite sure another best answer can be found.
I also want to quote AviD's Rule of Usability:

Security at the expense of usability comes at the expense of security.

(While there is no direct connection to this, I do want to point to it if only to make mention that if you break usability, then all is for naught)
My first thought is ""Oh no, your entire DB is stolen!"". I would hope that your first priority is to prevent this (I don't know if you're actually worried about this or if it's just planning for a worst case scenario). Then make every necessary effort to hide all required data. Any additional data that you can hide without a noticeable performance hit is gravy.
I've done a very small amount of HIPAA in my time and have found some rules to be perhaps intentionally vague. I assume this is to allow new techniques to be used without having to change the rules or something to that effect.
In short, if obfuscating ids is required in your situation, then you must do that (requirements are requirements), if not, then you can do that if you feel the difficulty and performance hit is tolerable. Follow the requirements first. Go above and beyond where it makes sense.
In your case, it seems as though it doesn't make sense to go to all that extra trouble only to make future maintenance that much more difficult. It might be a better use of time to focus on securing the DB itself while maintaining encryption/obfuscation only on the required fields. You can obfuscate ids all you want, but if the DB is easily stolen, then what is the point?
PS: I'm not a lawyer, nor do I play one on TV. Make sure you check HIPAA requirements yourself if you are in fact dealing with HIPAA.
",3,2015-11-03 15:29:47Z,340
28,3,"
This is ultimately a terrible idea, and will cost you in terms of data integrity.  Data relationships that are obscured are data relationships that will become corrupted, and unreliable.  It doesn't even make you more secure.  Obscurity protects only from the mildest of curiosity.  Look at the long history of cracked obscured systems for evidence of this.
This approach will lead to lost appointments, possibly appointments made with the wrong doctor, and a generally unreliable system.
In general, these sorts of requirements (HIPPA, PCI, etc) are written by non-technologists, and are up for broad interpretation.  
Ask the person driving the requirement if he/she is willing to sacrifice the integrity and reliability of the system for security through obscurity.  That's essentially what's being asked for.  My guess is someone is interpreting HIPPA is a very narrow way if they're asking for crazy things like ""obscuring database table IDs"".
My general advice is find someone with expertise in interpretting HIPPA requirements of what you can actually do rather than someone trying to interpret it in the most restrictive sense possible. 
",3,2015-11-03 15:45:34Z,26
28,4,"
Just started looking into HIPPA compliance for an upcoming application I'm building, and from what I see you're taking the rules/regulations a little overboard. As far as obfuscating your data goes, one of the main requirements is protecting your Electronic Protected Health Information or (ePHI) This includes stuff like Names, SSI numbers, or private or patient sensitive data. One of your internal Id's in your database wouldn't fall under this category, and trying to obfuscate that would just create a whole world of problems for you while not accomplishing anything. For example, if some data breach were to happen and they see that Id 5 is tied to id 8, it would not mean much to anyone without some other personal data, which would be encrypted.
Again I may not be the best source since I am just learning about HIPPA compliance also, but I know a good amount about PCI compliance and they seem pretty similar. 
",2,2017-01-17 21:43:14Z,341
28,5,"
I agree with the others: this is bad practice, from every POV, including design, audit, recovery, security, performance.  The list goes on.
If a hacker gets into the database, the game is over.  It doesn't matter if you obfuscated data, it will be a matter of time before that can be de-obfuscated.
RDBMS these days allow column-based permissions, and so, using real keys to join to other tables will allow standard SQL queries to be used, without the overhead to de-obfuscate.  Moreover, your auditors will appreciate the attention to ability to scale, upgrade, troubleshoot, and still maintain integrity and security.
Having said that, some vendors, like Microsoft and Oracle, maybe others, have stepped up and allowed very focused encryption.  Keys are stored in a separate system, and access to columns are protected.  Microsoft SQL Server, for example, uses TDE (transparent data encryption) and extensible key management (EKM) to do just this.  Therefore, your table structures don't change, and you don't compromise the overhead of joins with obfuscation, et al.  So, while you might have a doctor and a patient, you won't necessarily know the names or other details of either, since TDE can allow encryption of these fields.
RDBMS systems also allow for encryption of the entire database, to handle instances where the database itself is stolen.
In short, don't obfuscate the keys to the data.  Rather, encrypt the data that is sensitive.  You can look at like this: Don't obfuscate the metadata; rather, only encrypt the data.  Think of the keys and foreign keys as metadata.
Be careful, too, not to fall victim to the practice of placing strong security on the front door, all the while leaving the back door unlocked: your transaction logs, error logs, application, application logs, backup systems, and people who are trusted to have access to this data and applications are all sources for leaks, so, shore up permissions, and be careful what you log, and what you do with those logs.
Once you apply these principles, it becomes less important to obfuscate keys.
",0,2017-01-18 15:30:59Z,342
29,1,"

Is there such a thing? 

Absolutely.  Feeding malicious input to a parser is one of the most common ways of creating an exploit (and, for a JPEG, ""decompression"" is ""parsing"").

Is this description based on some real exploit?

It might be based on the Microsoft Windows GDI+ buffer overflow vulnerability:

There is a buffer overflow vulnerability in the way the JPEG parsing
  component of GDI+ (Gdiplus.dll) handles malformed JPEG images. By
  introducing a specially crafted JPEG file to the vulnerable component,
  a remote attacker could trigger a buffer overflow condition.
...
A remote, unauthenticated attacker could potentially execute arbitrary
  code on a vulnerable system by introducing a specially crafted JPEG
  file. This malicious JPEG image may be introduced to the system via a
  malicious web page, HTML email, or an email attachment.

.

This was published in December 2006.

The GDI+ JPEG parsing vulnerability was published in September 2004.

Is it sensible to say ""the operating system"" was decompressing the image to render it?

Sure; in this case, it was a system library that required an OS vendor patch to correct it.  Often such libraries are used by multiple software packages, making them part of the operating system rather than application-specific.
In actuality, ""the email application invoked a system library to parse a JPEG,"" but ""the operating system"" is close enough for a novel.
",177,2015-08-26 19:10:55Z,35
29,2,"
Agreeing with others to say yes this is totally possible, but also to add an interesting anecdote:
Joshua Drake (@jduck), discovered a bug based on a very similar concept (images being interpreted by the OS) which ended up being named ""Stagefright"", and affected a ridiculous number of Android devices.
He also discovered a similar image based bug in libpng that would cause certain devices to crash. He tweeted an example of the exploit basically saying ""Hey, check out this cool malicious PNG I made, it'll probably crash your device"", without realising that twitter had added automatic rendering of inline images. Needless to say a lot of his followers started having their machines crash the instant the browser tried to load the image thumbnail in their feed.
",47,2015-08-27 03:18:34Z,347
29,3,"
Unrealistic? There was recent critical bug in font definition parsing: https://technet.microsoft.com/en-us/library/security/ms15-078.aspx and libjpeg changenotes are full of security advisories. Parsing files[1] is hard: overflows, underflows, out of bounds access. Recently there were many fuzzing tools developed for semi-automatic detection of input that can cause crash.
[1] or network packets, XML or even SQL queries.
",10,2015-08-27 15:48:14Z,351
29,4,"
As others have pointed out, such attacks usually exploit buffer overflows.
Regarding the nuts-and-bolts of how, it's called a stack-smashing attack. It involves corrupting the call stack, and overwriting an address to legitimate code to be executed with an address to attacker-supplied code, which gets executed instead.
You can find details at insecure.org/stf/smashstack.html.
",1,2015-08-30 19:42:25Z,352
30,1,"
You can't.
To securely send information over an unsecure channel, you need encryption. 
Symmetric encryption is out, because you would first need to transport the key, which you can't do securely over an unsecure channel[*].
That leaves you with public key cryptography. You could of course roll your own, but you don't want to be a Dave, so that's out, which leaves you with HTTPS.
[*] You can of course try to use a secure channel to exchange the key, for example physical exchange of a key. Then you can use secret key crypto, like Kerberos.
",109,2018-11-09 14:08:48Z,332
30,2,"
TLS is really the only way to do it.
But what if I encrypt it with JavaScript?
The attacker can change the JavaScript you send to the client, or simply inject their own JavaScript that logs all information entered.
Then I'll use CSP and SRI to prevent the addition of scripts and the modification of my own scripts.
Glad you're using modern tools to help protect your site, but SRI in a non-secure context really only protects against the compromise of an external resource. An attacker can simply modify the CSP headers and SRI tags.
There's really no way to do this without using encryption from the beginning, and the only standard way to do that is to use TLS.
",45,2018-11-09 14:09:52Z,220
30,3,"
While I agree that you should use HTTPS, you can make it even more secure by using the Salted Challenge Response Authentication Mechanism (SCRAM, see RFC 5802) for the actual authentication exchange.
It's not trivial to implement, but the gist of it is that, rather than send the password to the server, you send proof to the server that you know the password. Since deriving the proof uses a nonce from the client and server, it's not something that can be reused by an attacker (like sending the hash itself would be).
This has a few added benefits to using HTTPS with the basic authentication you describe:

The server never actually sees your password as you log in. If someone has managed to insert malicious code onto the server, they still won't know what your password is.
If there is a bug in the HTTPS implementation (think Heartbleed), attackers still won't be able to acquire your password. Because, even once TLS is bypassed, the password isn't available.
If, for some reason, you can't use HTTPS, this is better than sending the password in the clear. An attacker will not be able to guess your password based on the exchange. That said, you should still use HTTPS, because defense in depth is better.

Please note that this is only secure if the client is able to perform the SCRAM computations without downloading code from the server. You could provide a fat client, or users may be able to write their own code that they control. Downloading the SCRAM code over HTTP would give an attacker an opportunity to modify the SCRAM code to send the password in cleartext, or otherwise expose it.
",26,2018-11-09 18:01:00Z,358
30,4,"
You should definitely deploy TLS in this case.
If you decide to try to invent a transport security system over HTTP, ultimately you're going to end up implementing a susbset of the TLS functionality anyway. There are many pitfalls to be aware of when designing and implementing such a system, which are further amplified when you choose to try to implement the system with a language that doesn't offer many safety features.
Even if you did implement a correct implementation of a cryptographic protocol in JavaScript (which is already a big challenge) you can't rely upon that implementation being resistant to timing attacks, and the whole thing breaks if someone has scripts disabled (e.g. with NoScript).
As a general rule you should choose the implementation that consists of the most simple, well-understood, trusted components that require you to write the least amount of security-critical code. To quote Andrew Tannenbaum:

A substantial number of the problems (in application security) are caused by buggy software, which occurs because vendors keep adding more and more features to their programs, which inevitably means more code and thus more bugs.

",7,2018-11-09 18:31:24Z,21
30,5,"

Do I need to implement some encryption algorithm like RSA public key encryption?

If you're considering trying that, you may as well just go the full mile and use HTTPS. Since you don't seem to be an expert, ""rolling your own"" encryption will undoubtedly have misimplementations that render it insecure. 
",6,2018-11-09 16:03:34Z,361
30,6,"
There is actually a way to authenticate a user over an insecure connection: Secure Remote Password (SRP) protocol. SRP is specifically designed to allow a client to authenticate itself to a server without a Man-in-the-Middle attacker being able to capture the client's password, or even replay the authentication to later impersonate the client, whether the authentication succeeded or not. Furthermore, successful authentication with SRP creates a shared secret key that both the client and the server know, but a MitM does not. This secret key could be used for symmetric encryption and/or HMACs.
However, there are a number of limitations of SRP:

The user must have registered in some secure fashion, as the registration process does require transmitting password-equivalent material (though the server does not need to store it).
Although it is safe to attempt an SRP login with an untrusted server (that is, it won't expose your password to that server, and you'll be able to tell that the server didn't have your password in its database), It's not safe to load a login web page from an untrusted server (it could send down a page that captures your every keystroke and sends it off somewhere).
Although successful authentication via SRP generates a secure shared secret key, the code to actually use this key in a web app would need to be loaded from a server, and an attacker could tamper with that code to steal the symmetric key and make changes to the requests and responses.

In other words, while SRP can be useful in situations where you have a trusted client that doesn't need to download its code over the insecure connection and also you have some other secure way to register the user(s), SRP is not suitable for a web application. Web apps always download their code from the server, so if the connection to the server isn't secure, a MitM (or other network attacker, for example somebody spoofing DNS) can inject malicious code into the web app and there's nothing that you, the victim, can do about it. Once that code is there, it can capture any password or other data you enter, steal any key that is generated, and tamper with any requests you send or responses you receive without you even knowing.
",2,2018-11-10 10:42:20Z,362
30,7,"
It is actually quite possible.
Password has to be hashed on the client side, but not with a static function. The following method uses two steps, and is inspired by the Digest access authentication:
NOTE: The server keeps a HMAC of the Password and the corresponding Key (Digest);

The client starts by requesting a nonce to the server, the server returns the Key and the Nonce;
Client calculates: HMAC( HMAC( Password, Key ), Nonce) and sends it to the server;
Server calculates: HMAC( Digest, Nonce ) and compares it to the received value.

This protects you against replay.
The main interest I see is not a protection against eavesdropping, but to be completely sure that the plaintext password will not be store on the server, not even in a system log (see Twitter or GitHub).
Note: HMAC can be replaced by PBKDF2.
",2,2018-11-10 09:38:45Z,364
30,8,"
It is certainly possible to send a password securely using HTTP. There is even a standard for it. It's called HTTP-Digest and it's defined in RFC 7616. It's been a part of the HTTP spec for quite a while. It was originally designed to use only the MD5 message-digest (""hashing"") algorithm, but later revisions of the standard allow the client and server to negotiate which algorithm to use.
Unfortunately, there are many problems with HTTP-digest and the most glaring of them is that the server needs to have the password in cleartext. So while it's possible to communicate passwords securely using HTTP, it's not possible to securely-store passwords on the server while using it. So basically, it's not useful.
As others have said, it would be better to implement TLS because it solves multiple problems all at the same time, and it's fairly forward-compatible.
",1,2018-11-10 22:51:55Z,365
30,9,"
As others have said TLS! TLS! TLS! (and with decent key length too)
But if you have to to use HTTP and any server to implement, is this for programmatic use or human use?
If for human use, are you looking at basic authentication or form-based? And are your uses able to apply a little logic E.g. not for joe public to use.
Does anything else need to be protected, or just the password itself?
If so you may be able to look at:

Use a one-time password pad/generator (RSA fobs ain't cheap, and if you've got that sort of money to spend then you can afford TLS)
Some other 2-factor auth like SMS (not without it's own issues).
A simple challenge/response mechanism to obfuscate the password, for example the  page displays two numbers on it in the text (not blatantly labelled either), user has to start password with a simple calculation such as difference between the two numbers added to each digit of a PIN, maybe with random stuff before and/or and after, eg 98634572dkkgdld. Changing the challenge numbers should stop replay attacks that aren't tried instantly, but if an attacker can capture enough attempts they may be able to work it out. You can't put any obfuscation logic in user-facing script either 'cos that'll be seen and if the scheme is too complex your users will hate you even more.

I'd still so with TLS though, certificates are cheap (even free) these days so convoluted obfuscation schemes really have no reason to exist any more.
",1,2018-11-12 21:55:03Z,366
30,10,"
You need the recipient to send you his key. You can't randomly send someone encrypted info without them first providing you with their personal public key.
",-4,2018-11-11 01:58:52Z,367
31,1,"
Welcome to the internet! This is the normal situation, business as usual.
You don't have to do anything, but to harden your website. Probes like that occurs all the time, on every site, day and night. Some people call that ""voluntary pen testing.""
Depending on your site, there are some tools that you can use to help you keep those kinds of probes out of the site. Wordpress sites have a couple plugins (you can search for Security plugins on the plugins directory), and I believe the other popular platforms out there will have equivalent plugins.
Other tool I usually employ is fail2ban. It can parse your webserver log files, and react accordingly.
",67,2018-11-05 14:08:39Z,219
31,2,"
The first step outside of immediately looking to a solution is to conduct a pentest of your own site and be actually aware of what weaknesses there are in your site. If you don't know what you are protecting, then how will you know to protect it?
First, look at the infrastructure such as CMS. For example, if you are using Wordpress, then there are pentesting tools for Wordpress available both as apps and cmd tools. ie Wordfence , and I've used WPscan also.
Second option is to look at tools like OWASP zaproxy and do an attack scan of your network and gain a list of vulnerabilities. Just a note that some of these could be false positives.
Your findings may mirror what has already been found but I think knowing what the vulnerabilities are in your own site is useful.
The next step is how you are finding out about these probes. If it was a manual check, you can also consider setting up some log collection system like NXLog 
",5,2018-11-05 15:18:27Z,372
31,3,"
Work out what they are looking for, and ban their IP for a month or two if they try it on.  You might also dummy up some PHP to slow them down. 
Do not refer them to other sites for huge downloads, and do not leave malware for them to find. 

90% will be Wordpress, PHPMyAdmin, Telephony. If they are script kiddies the same old values pop up.

Look into Fail2Ban and DenyHosts for ideas.
If you are actually running WP, harden it up with a security solution.
Only allow access to admin tools and any database by exception, and this should almost never be from an Internet address, but something local with it's own Bastion-like protection.
",2,2018-11-06 02:09:45Z,12
31,4,"
If I had a cent for every scan my website gets...
Literally, if you check your logs, you will notice a constant stream of automated probes and attacks. When I consult clients (I work in information security), I call this ""background noise"". It is there and any attempt to do anything about it is more costly than just accepting that it's there. I would even go so far as to filter it out before you pipe the logfiles into your monitoring, alerting, SIEM, etc. systems.
What you must do is keep your systems up-to-date and patched. Almost all of these attacks are using well-known and often quite old exploits. They are fishing for easy targets.
What you should do is spend a little bit of time on hardening your system. Setting up permissions correctly, blocking unused ports, disabling unused software, running stuff under dedicated users, that kind of stuff.
What you can do, especially for a private website with a local audience, is to block out broad IP ranges belonging to China, Russia, Europe and/or the USA, depending on where your audience isn't. The vast majority of attacks originate from these origins, and if you don't have anyone in, say, the USA who reads your webpage because your webpage is about your local dog club in Spain, you can reduce the noise just by blocking them out at the firewall. I write ""can"" because it doesn't make much of a difference, really, but it will reduce the noise in your log (it will also affect your Google ranking, but that's a different subject).
",0,2018-11-06 13:06:14Z,25
31,5,"

Block the whole country
Check ASN and it’s allocated IP range, and block that IP range.
Fingerprint the attacker using a user agent or a JavaScript library and attach a strong captcha when the fingerprint is detected.

Last but not least, secure your site and monitor attacks regularly.
",-5,2018-11-05 14:07:36Z,0
32,1,"
The server time is of little use to an attacker, generally, as long as it is accurate. In fact, what is being revealed is not the current time (after all, barring relativistic physics, time is consistent everywhere) so much as the exact difference between your server's notion of the current time and the actual current time. Note that, even if you do not explicitly reveal the time, there are often numerous ways to get the local server time anyway. For example, TLS embeds the current time in the handshake, web pages may show the last modified dates of dynamic pages, HTTP responses themselves may include the current time and date in response headers, etc.
Knowing the exact clock skew of a server is only a problem in very specific threat models:

Clock skew can be used in attacks to deanonymize Tor hidden services.
Poorly-written applications may use server time to generate secret values.
Environmental conditions of the RTC may be revealed in contrived situations.

",84,2018-06-08 09:22:39Z,76
32,2,"
""Servlet"" sounds like you're already running a complex server there.
There's a lot of ways that protocols leak server time. For example, HTTP has a popular header field for creation/modification time, and dynamically generated data would always have the current system time, if used properly.
For TLS authentication reasons, you can even binary search to find an endpoint's date and time using expiring client certificates.
This is a necessary evil – if you're doing TLS, your server needs to have a notion of time to know what's a valid key / cert and what not. If that's the case, this will very likely be an NTP-coordinated time. So, you really can't tell anything about the server that an attacker wouldn't already know – there's really only one ""correct"" time.
If you're not doing TLS: leaking system time is probably not the first thing you should fix.
Regarding security: Not quite sure you should expose yet another servlet just for devices to figure out world time. 

To explain more, this end-point will be used by mobile apps to enhance their security (to avoid cheating by adjusting the device date).

Red flag. You're depending on your client's unmodifiedness for security. Never do that. You simply can't trust anything happening on your user's devices. These are not your devices. They can simply hook up a debugger and modify notions of time in-memory of your process, no matter whether kept by the phone's OS or by your application. 
",10,2018-06-08 09:24:02Z,383
32,3,"
The only thing that could really expose a security risk is the high-precision timers and performance counters. These could be used to precisely time how long some cryptographic process takes on the server, and from there infer secret data.
Timing data to the milisecond or normal ""tick"" rate is unlikely to expose this.
",2,2018-06-08 15:30:15Z,384
32,4,"
Knowing the server time is of little use for an attacker. So here you should find about possible side effects.

is the protection for that servlet weaker (in any way) that the one of other servlets? What are the possible implications of the no need for authentication in term of DOS/DDOS attacks. It could matters if authentication was processed in other more secured servers. Is there any difference for reverse proxying?
what are the risks for security flaws in that servlet? Has it followed the same quality insurance controls than the other servlets? What about its maintenance cycle?
what about its servlet container?

",1,2018-06-08 12:44:48Z,385
32,5,"
I would like to answer that by pointing out, what would follow from the assumption, that exposing the server time would in fact be a risk.
It would mean that system administrators would have to set random times on their systems, because any server set to the correct time would be vulnerable, since a correct or near correct server time is just to easy to guess.
It would also mean extreme development effort to translate a false setting to a correct time for use in every time related application. Every single file on the system would  have a wrong time stamp.
In my experience you invite much more trouble by setting wrong server times, than you can expect with the correct setting. Some examples are mentioned in other answers, but you also have to consider distributed applications or clusters, where correct time settings are usually essential. Basically any time related information you store would be flawed.
So if exposing your system time would be your biggest risk, you were in luck. But most likely there are many other concerns that you haven't paid enough attention to. See https://www.owasp.org/index.php/Main_Page for security related issues and best practices.
",1,2018-06-08 22:44:07Z,386
32,6,"
Server time is not a secret. On the contrary, you are strongly encouraged to sync your time to an outside objective time source (such as an atomic clock exposed through a time server).
Not being a secret has two consequences:

you don't need to protect or hide it. You can safely assume that the attacker is capable of figuring out what the current time is.
it should not have a function on anything that you do wish to protect or hide. For example, you should not derive any keys or secrets from the time. Any random functions you are using should be seeded not with the current time (still a lazy default in many), but with a better seed source.

",0,2018-07-06 10:06:46Z,25
33,1,"

Is there any standard defense technique against it?

As outlined in the other answers, bit errors when querying domain names may not be a realistic threat to your web application. But assuming they are, then Subresource Integrity (SRI) helps. 
With SRI you're specifying a hash of the resource you're loading in an integrity attribute, like so:
<script src=""http://www.example.org/script.js""
    integrity=""sha256-DEC+zvj7g7TQNHduXs2G7b0IyOcJCTTBhRRzjoGi4Y4=""
    crossorigin=""anonymous"">
</script>

From now on, it doesn't matter whether the script is fetched from a different domain due to a bit error (or modified by a MITM) because your browser will refuse to execute the script if the hash of its content doesn't match the integrity value. So when a bit error, or anything else, made the URL resolve to the attacker-controlled dxample.org instead, the only script they could successfully inject would be one matching the hash (that is, the script you intended to load anyway).
The main use case for SRI is fetching scripts and stylesheets from potentially untrusted CDNs, but it works on any scenario where you want to ensure that the  requested resource is unmodified.
Note that SRI is limited to script and link for now, but support for other  tags may come later:

Note: A future revision of this specification is likely to include integrity support for all possible subresources, i.e., a, audio, embed, iframe, img, link, object, script, source, track, and video elements.

(From the specification)
(Also see this bug ticket)
",131,2018-05-08 12:32:24Z,387
33,2,"
Your concern is very likely unfounded.
First of all, you need to realize just how unlikely these memory malfunctions are. The person who wrote the above article logged requests to 32 clones of some of the most visited domains on the Internet over the course of 7 months. Those 52,317 hits had to be among hundreds of billions of requests. Unless you operate a website on the scale of Facebook, an attacker would have to be extremely lucky to even get just one unlucky victim on their bitsquatting domain.
Then you have to note that memory errors cause several malfunctions. The author writes:

These requests [...] show signs of several bit errors. 

If the system of the victim is so broken that they can't even send a HTTP request without several bit errors, then any malware they download from it will likely not execute without errors either. It's a miracle it even managed to boot up in that condition.
And regarding those cases where bit errors were found in ""web application caches, DNS resolvers, and a proxy server"" and thus affecting multiple users (some of them maybe unlucky enough to get enough of the malware in an executable state): In these situations, the HTTP response would come from a different server than the client requested. So when you use HTTPS-only (which I assume you do, or you would have far more serious attacks to worry about), then their signature won't check out and the browser will not download that resource. 
And besides, HTTPS will also make it much less likely to get a successful connection when there is a system with broken RAM on the route. A single bit-flip in a TLS encrypted message will prevent the hash from checking out, so the receiver will reject it.
tl;dr: stop worrying and set up HTTPS-only.
",65,2018-05-08 13:56:54Z,71
33,3,"
I doubt about the article's dependability. While discussed at DEFCON and published as whitepaper, I have serious concerns about the experimental results.
According to comment by @mootmoot, the author failed to determine deterministic programming errors from random fluctuations of bits.
My concerning statement is

During the logging period [Sept. 2010 / May 2011, ndr] there were a total of 52,317 bitsquat requests from 12,949 unique IP addresses

No, the author only proved his squat domains were contacted, but likely failed to provide additional information

What percentage of original CDN network does that traffic represent (this is verifiable in theory, but I don't have those figures)
Occurrence of source referral domains

The second is very important because it helps isolate deterministic programming failures from random fluctuation of bits.
Consider the following example: if you find any entry to gbcdn.net (facebook squat) with referer https://facebook.com you likely have found a bitsquat.
If on the contrary you find multiple entries from a poorly known webiste which you can inspect to find with a broken like button, then the problem is probably due to a programmer not copying/pasting the code correctly, or even a bit flip occurred in the programmer's IDE. Who knows...
",20,2018-05-08 14:01:05Z,395
33,4,"
Disclaimer: I'm an employee of Emaze Networks S.p.A.
One way to defend against this kind of attacks is to not allow attackers to register similar domain names.
To achieve this you can register many bitsquatted/typosquatted domains together with your main domain name, but this is impossible to do with all bitquatting/typosquatting cases, since they can be billions (consider also unicode etc!).
An alternative is to periodically monitor the domains registered, and if you find a suspicious domain you can check what it does and, if it seems to be a malicious site, you can report it to the registrar and ask them to pass the ownership of said domain to you.
We have a small service, called Precog that does this, by aggregating registrar information from different sources and running various kind of queries to detect bitsquatting/typosquatting/punycode-squatting domains: you can register your brand, put some keywords and we will contact you if a suspicious domain is registered.
Our tool takes into consideration 2nd level domains, obviously, but is also able to detect registration of many 3rd (or more) level domains, so we may be able to detect someone is going to use app1e.account.com to try and steal your apple credentials.

I must add: I believe the biggest use case for attacks of this kind is not to ""get lucky"" to receive a request because somebody mistyped the domain, but to use the domain as a phishing domain. So people will register the site àpple.com and send tons of emails that look like Apple's emails and try to get some people to insert their credentials/credit card information on their page.
",3,2018-05-08 18:40:25Z,277
33,5,"
Among all the answers, I am surprised I haven't seen a reference to Content Security Policy.
It is basically a one-stop solution for white-listing allowed sources of content. A simple fix would be to only allow JavaScript from your current domain (www.example.com) and block everything else.
",1,2018-05-11 15:18:49Z,399
33,6,"
Arminius's answer is great. You can also use a strong Content Security Policy as another line of defense against bit-squatting. 
CSP is an HTTP header that allows you to craft a fine-grained whitelist of URIs that your website can communicate with. For example, you can tell the browser that you'll only allow JavaScript to come from example.org/js/file1.js, only allow images from example.com/imgs, and fonts from example.net. 
For a bit-squatting attack to be successful they'd need to flip a bit in the HTTP header AND in the request. If an attacker can flip only one bit then your website will throw a lot of errors - regardless of which bit is flipped.
",0,2018-05-11 15:16:16Z,400
34,1,"
Technically slightly, yes. But:

It would be security by obscurity, which is a bad idea
It does not boost confidence in your product
It would be very easy to figure out what does what, it would only take a bit of time
Google Translate, you can just use meaningless names, it would still not help much
It would make maintenance harder
It would make audits very hard, as the auditors may not understand the language

All things considered, it is probably never worth it.
",174,2018-04-30 14:03:35Z,404
34,2,"
It would not be appreciably more secure. Reverse engineers are often forced to work with systems that do not have any original names intact (production software often strips symbol names), so they get used to dealing with names that have been generated by a computer. An example, taken from Wikipedia, of a snippet of the kind of decompiled C code that is often seen:
struct T1 *ebx;
struct T1 {
    int v0004;
    int v0008;
    int v000C;
};
ebx->v000C -= ebx->v0004 + ebx->v0008;

People who are used to working with this kind of representation are not fooled by the usage of variables and such that are given irrelevant names. This is not specific to compiled code, and the use of C was just an example. Reverse engineers in general are used to understanding code that is not intuitive. It doesn't matter if you are using JavaScript, or Java, or C. It does not even matter if they are analyzing nothing but the communication with the API itself. Reverse engineers are not going to be fooled by the use of random or irrelevant variable or function names.
",62,2018-05-01 06:05:43Z,76
34,3,"
Not really - all of the built-in functions will still be in English, so it wouldn't take much extra effort to work out what your variables are going to represent. It might slow someone down slightly, but given that people still manage to reverse-engineer code with single character variables all over the place, or which has been run through obfuscators, swapping the language used for variables and functions just means doing a find-replace once you've worked out what one of your variables is used for, then repeating until you have enough understanding.
",35,2018-04-30 14:03:09Z,407
34,4,"
That is security through obscurity and will delay a dedicated attacker all of five minutes.
If you want to confuse an attacker, naming things their opposite or something unrelated would have the same effect. So your ""create user"" function could be named ""BakeCake"". Now you can answer yourself how much security that gives you. Actually, this would be more secure, as it can't be defeated by simply using a dictionary.
Yes, at first it would confuse, but one look at the system in operation and everything becomes crystal clear immediately.
",21,2018-04-30 19:11:16Z,25
34,5,"
Your system MUST be secure by itself. If it relies on user-side javascript passing a parameter with the value ""open sesame"", you are doing it wrong.
You should develop the program in the language that is more convenient for you (eg. based on your coder proficiency, consistency with your code base, or even with the terms that you are using).
Other answers already pointed out how it doesn't really provide security. If you want to make a secure program, rather than concerning about potential hackers easily reading your code and learning a secret hole, you should probably care more about making it readable to the people auditing it.
If there is a function parameter called nonce, and a comment saying how we are ensuring it is unique across the requests, yet it isn't sent on the request, you can be quite sure it is a slip-up. Actually, having that code easily readable will decrease the chances of that parameter being dropped/empty (after all, everything worked without it...), or if that really happens, make easier that another of your developers notices the problem.
(Third parties could hint you about it, too. Probably, there will be more people having a casual look at it than ones actually trying to break it. A random attacker will most likely start by launching an automated tool and hoping it finds anything.)
TL;DR: produce readable code. For the people that should deal with it. In case of doubt, you should prefer the one most people know about.
",9,2018-05-01 21:15:38Z,14
34,6,"
It could even make things worse by making the system harder to maintain. 
Take an extreme example inspired by history, and communicate between front and back ends in Navajo. When (not if) you need to patch the code you need to either:

work with what to you is nonsense (with the extra chance of bugs/typos plus it takes longer to work on non-intuitive code), or 
hire/keep programmers fluent in Navajo (a rare and potantially expensive skill combination, possibly impossible to find a contractor)

",7,2018-05-01 14:08:36Z,411
34,7,"
I would also note that in most instances for javascript on the client side, the script as developed (with meaningful variable names etc) would be 'minified' in order to increase performance on the client (smaller file size to download). 
As part of this, most variable names are reduced to single characters and as much whitespace as possible is stripped out.  This becomes just about as unreadable as anything else you might write.
I would also note that chrome (for example) has methods that take a 'less human readable' file like this and 'pretty print' it, making it a lot easier to figure out what is going on.
In short, the human language that you use to write your client side code really doesn't make a big difference.
",5,2018-05-01 12:56:41Z,412
34,8,"
If the mass media are to be believed, then the majority of hackers are Russian, Chinese, North Korean, so they are already operating under the “handicap” of having to hack Western systems in a non-native language. Therefore unless you choose something incredibly obscure, like in the movie Windtalkers it won’t make any difference to them. But the extra effort for you means less time for you to find and fix bugs, so if anything this strategy would make your security weaker.
",4,2018-05-03 21:36:22Z,413
34,9,"
Several responses have (rightly) pointed out that this is “security through obscurity”, which isn’t actually a form fo security. It is safest to assume that the attacker has fully annotated source code sitting in front of them while busily attacking away.
Software is “secure” when knowing everything which can be known in advance of a request / transaction / session is public knowledge AND this has no impact on the actual security of the product.
Source code must be assumed to be public knowledge, if only because disgruntled employees aren’t taken out back and shot when they are terminated. Or as is often said, “The only way for two people to keep a secret is if one of them is dead.” Thus any “special knowledge” which is being concealed by obfuscation of any sort must be assumed to have become “general knowledge” as soon as it is produced.
Security, at its core, is nothing more than “saying what you do, and doing what you say.” Security analysis — code auditing and formal evaluation schemes, such as are conducted under the Common Criteria — requires that the mechanisms for performing an action are well-documented and that all transitions from one secure state to another secure state are only possible when all the requirements are satisfied. One risk with obfuscation of all sorts is that these requirements are obscured by clever coding — see the example of “create_user()” failing because it is “secretly” the “delete user” or “rename user” or something else method. For a real-world example of how hard such renaming is on the brain, there are on-line tests in which you must read the name of a color which is printed on the screen in a different color. For example, the word “RED” written in a blue font will result in some number of people saying “BLUE” instead of “RED”.
",2,2018-05-03 21:47:01Z,414
34,10,"
It might be irrelevant. Consider the scenario where you're coding in (e.g.) Italian instead of English, but most of your customers are in Italy, naturally Italian-speaking hackers have a higher motivation/payoff from attacking you.
In this scenario, coding in another language has either no effect or it can even make it easier to hack.
",0,2018-05-01 05:15:32Z,416
35,1,"
/test and /Test are both hosted on example.com … so it's just a page redirect not a domain redirect … this is a non-issue.  
People redirect like this all the time, for instance redirecting from HTTP to HTTPS is pretty much industry standard at this point.
",30,2018-04-12 06:16:51Z,418
35,2,"
Implemented correctly, there are no issues with this.
There are two things you should look out for (I assume that test is not static here, but user supplied, so you eg want to upper-case every path):

Open Redirect: If your redirect is implemented incorrectly, it might be possible for an attacker to redirect outside of your domain, which could be used in phishing attacks
CSRF: If your CSRF protection is only a simple referer check (which isn't recommended), and if you have state-changing GET requests (which also is not recommended), those may be possible to exploit, depending on your implementation of the redirect mechanism

",18,2018-04-12 06:55:28Z,332
35,3,"
Redirecting users to different page or domain is a normal practice followed by many developers (even MNC's including FB, fb.com redirects to facebook.com). It's no harm if you try to redirect requests in a secure way.
You might want to check OWASP Cheat Sheet for Unvalidated Redirects and Forwards (Also called Open Redirection). This document provides to secure ways to redirect URL in multiple programming languages.
",8,2018-04-12 06:55:10Z,419
35,4,"
As far as hostname remains same and your user trust it. It should not be a problem.
This sort of redirection are common across internet and help to provide a better user experience.

For instance:
you have a resource at https://testwebsite.com/Test but due to some typo or 
   developer's mistake it is written as https://testwebsite.com/test. The redirection will help user to see an appropriate file instead of seeing a 404 file not found error or Internal server error. 

",1,2018-04-12 09:48:47Z,420
35,5,"

Redirect within the same domain - no risk. (because your domain is trusted)
Example: https://domain.com/login redirect to https://domain.com/dashboard
Redirect to third party website is medium severity risk if you are leaking sensitive data like access_token, secret keys to the third party website.
Example: https://domain.com/redirect_to_fb redirect user to https://fb.com
Attacker controlled redirect leads to:
a. Phishing https://domain.com/login?redirect_to=http://evil.com
b. Cross Site Scripting (XSS) issue: https://domain.com/login?redirect_to=javascript:alert(document.cookie)
c. Leaking tokens Example: https://domain.com/login?redirect_to=https://attackerdomain.com Example: https://r0rshark.github.io/2015/09/15/microsoft/
d. Content Security Policy bypass
e. Referrer check bypass
f. URL whitelist bypass
g. Angular ng-include bypass

If an attacker is able to control the redirect then it's a serious issue.
",-4,2018-04-12 10:07:09Z,422
36,1,"
In the end, it all comes down to trust and risk. How much do you trust this person and how much do you want to give as far as details goes, what can someone do to harm you when they have your data? That you're asking here tells me you're not really sure if you can trust this person. There are risks involved (such as real life threats or maybe a possible scam) but it is not easy to identify risks without knowing the full situation, as you explained it rather vaguely.
To me the whole situation sounds kinda phishy to be honest. Are you sure you didn't fall for a phishing or scam attempt by helping this other person?
Most countries do have PO boxes and other rent-able post solutions such as Poste Restante (as suggested by Molot in the comments) so you don't have to give out your own personal details.
",86,2018-11-20 15:53:30Z,427
36,2,"
Many countries' postal systems have general delivery by which you can receive a package held at a post office for you to pick up, without having to give the sender an address. This might be an option for you.
In order for anyone else to assess how risky the situation is, I think you need to elaborate more on your relationship with the sender and the favor you performed for them. The vague way you've stated it is a big red flag for scams, involvement in money laundering, etc. but it may be that you've just poorly stated the situation out of a wish for privacy. At the very least though you should mention (or at least reflect upon for yourself) whether you had any relationship with the person prior to their asking you for a favor and whether you expected to be compensated in any way for the favor.
",58,2018-11-20 18:49:25Z,56
36,3,"
The whole favour thing--which they initiated--sounds rather like a setup aimed at getting your personal info. So I would recommend being very cautious here, and graciously declining their offer. Ask them to ""pass it on"" or ""pay it forward"" or something. The more they demand to get your info, the more suspicious you have a right to be.
",16,2018-11-20 21:26:46Z,434
36,4,"
Depending on the country you are in, your physical address is enough to do quite a lot of damage, or at least cause a huge amount of nuisance. Common examples:

An antagonist using your address to order delivery of unwanted, pay-on-delivery things (pizzas being the canonical example, but I've also been subject to Internet pranksters sending evangelicals to my home to try to convert me)
Being ""doxed"" and have your private information put online for stalkers/harassers to take advantage of
In the US, Swatting, which has resulted in deaths and significant property damage
Your address can also be used to socially-engineer others into e.g. giving up other identifying information, seizing domain names, or making you an unwitting part of a ""lost delivery"" or ""brushing"" scam 

So you are probably right to limit sharing your mailing address to people you don't know if you can trust with the information.
As other answers have stated, you can minimize your exposure by using a PO box or similar; you may also be able to find mail hold-and-forward facilities that can handle individual pieces of mail or parcels (which usually charge by the piece of mail and whatever additional handling is necessary for the forwarding itself); this can be much more convenient to you than a PO box, as well as less costly overall since you don't need to rent it on a monthly basis.
",14,2018-11-21 02:00:49Z,435
36,5,"
Be careful about personal information. There was a crime in Austria reported some days ago, where people ordered stuff online to some strangers addresses and then redirected it through an app from the delivery service. Of course they never paid anything but the strangers got payment reminders.
I've got a news article about this, but unfortunately only in german. (Try a translater): https://wien.orf.at/news/stories/2948455/
",3,2018-11-21 07:21:32Z,436
36,6,"
Just giving out the address is no problem, as long as you limit it to the address. Don't you have phone directories ""white pages"" or similar where you live?
Think about it, hundreds of people probably have your home address. Of course, any of them could send you something illegal, immoral etc but as long as you act reasonably (E.g., you open an envelope and there is something that looks like drugs in, as long as you immediately call the police and you live in a reasonable country, nothing will happen to you. Rather, you will be greeted as some local hero if some local paper hears of the story.) there won't be any problems.
Same goes for this situation.
",2,2018-11-20 22:49:49Z,437
36,7,"
You could ask them to send it to a local post office or delivery centre, that way you can only give them a very approximate location of where you are, ie city or town. 
In the UK, you can do so as specified here: https://www.royalmail.com/personal/receiving-mail/choose-local-collect/
Although I'm not sure on how specifically to do so in other countries, but it gives you an idea, and I'm sure there will be similar services offered in this way where you are. 
",0,2018-11-22 14:20:29Z,266
36,8,"
In the UK, Belgium and Holland, most corner shops are part of one or more logistics networks, and hence drop-off points for senders; with many also pickup points. Often (via say Amazon.UK or Bol.com etc) you can choose them as destination from webshops. These same networks should fit in the chain that gets the package to you.
It's a very reasonable idea to tell the sender: You'll be out at work (though I use my work address for such, which has a receptionist!) so would hate to miss the gift. So go to the cornershop where they know you by sight or name, ask if you can get it delivered in your name [""A.E. Neumann, C/O The Corner Shop, 12 High Street, Mummerset""] there. This doesn't unload the risk onto them, as its your name and their location so not relevant to any bank account or so.
But consider they might find your real address even then. E.g., in the UK, I was surprised when googling my name for free it clearly hints my longterm partners (several addresses shared over time; precise up to city quarter if not paying for the data; a mixture of Electoral roll data before opting out, and other sources). 
I wouldn't judge this approach risky; in the real world there's thousands of instances where your details have been taken at a higher risk.  I once got one UK parking fine (escalated over months of non-paying) while not owning a car, having no driving license, nor living (nor having lived) in the country; somebody had declared to the parking attendant that it was my car, and passed my details (collected from a B&B guest registration years before, in another country, because very very specifically misspelled)! Of course in ID-card-less UK there's no trace of the original declarant (I bet the then-owner), and I think no legal way for an individual like me to find the car owner's info from their numberplate. To get the bailiffs off eventually took months, hours of paperwork, various registered letters, and a statement from the DVLA (the UK ""DMV"") that I wasn't the owner.
",-1,2018-11-21 13:03:19Z,441
37,1,"
Just got the exact same-very-bad-written-in-French email.
I do have a business (canadian based, but we are mostly working with large companies abroad). It smells very fishy... phishing? 
Found this website (in French of course)
https://www.cnil.fr/fr/professionnels-comment-repondre-une-demande-de-droit-dacces
It says :
« Quels justificatifs demander ?
Pour exercer ses droits, la personne doit justifier de son identité. Par principe, cette justification peut intervenir « par tout moyen ». Ainsi, il n’est pas nécessaire de joindre une photocopie d’un titre d’identité en cas d’exercice d’un droit dès lors que l’identité de la personne est suffisamment établie (par exemple, par la présentation d’un numéro client ou des éléments permettant d’identifier des abonnés à un service).
Par ailleurs, dans un environnement numérique, le fait d’exercer ses droits depuis un espace où la personne s’est authentifiée peut être suffisant, en fonction des données d’identité numériques demandées (par exemple FranceConnect).
Néanmoins, si vous avez un « doute raisonnable » sur l’identité du demandeur, vous pouvez lui demander de joindre tout autre document permettant de prouver son identité, comme par exemple, si cela est nécessaire, une photocopie d’une pièce d’identité. En revanche, vous ne pouvez pas exiger systématiquement de telles pièces justificatives, lorsque le contexte ne le justifie pas.
Il est par exemple disproportionné d’exiger automatiquement une copie de la pièce d’identité si le demandeur effectue sa démarche dans un espace où il est déjà authentifié. Une pièce d’identité peut toutefois être demandée en cas de suspicion d’usurpation d’identité ou de piratage du compte par exemple.
Le niveau des vérifications à effectuer peut varier en fonction de la nature de la demande, de la sensibilité des informations communiquées et du contexte dans lequel la demande est faite. » 
So basically, ask for a Client Number or anything that could identify the person asking for sensitive data. That should solve the problem; if it's phishong, the person will send you a fake Client Number then you can decline its demand or you'll get no reply to your email. Either way, you'll be safe if ever this email was legit.
",1,2018-11-09 01:12:28Z,443
37,2,"
Their website explains this:

Hey, our email looks suspicious?
Don’t worry, we are not doing SPAM toward DPOs. We are going around sites testing the information processes for citizens on “my personal data“. We only have 4 emails for which we request access to information.
If you are not DPO, we apologize for the inconvenience. You can simply inform us using the link at the bottom of the email we sent you and we will not send you email anymore.
If you are DPO and you found the email suspicious, it's probably because we have automated the process: we are trying to ask all websites and it takes quite a lot of time. Our requests are legit. If you find that they are not, you can ask us not to send you mail using the link at the bottom of the email we sent you. We will put you in the refusal category for our study and will respect your choice.
If you are DPO and you are curious to know more about what we do, you can send us your email here and we can discuss it directly. 

So this is not a private EU citizen requesting what data you have of them.
People do have the right to ask a third party to make these kind of requests. From e.g. the UK ICO page about GDPR Individual Rights:

What about requests made on behalf of others?
The GDPR does not prevent an individual making a subject access request via a third party. Often, this will be a solicitor acting on behalf of a client, but it could simply be that an individual feels comfortable allowing someone else to act for them. In these cases, you need to be satisfied that the third party making the request is entitled to act on behalf of the individual, but it is the third party’s responsibility to provide evidence of this entitlement. This might be a written authority to make the request or it might be a more general power of attorney.

It does however not seem that this electronicprivacy club is doing that; they just seem so be spamming random site owners. What for? Well, the text says it's a study.
Ignore them.
Or ask them what study. They probably want to publish a report on the state of GDPR implementation as they see it. Which would be for their own benefit, so you could even consider this spam: even if they were asking you to participate in the study it would be spam.
Or point them to the regulations and ask on who's behalf they are doing the request (as lolablindfold suggested). They will probably not answer.
This thing looks fishy anyway:

Nowhere on their site are they telling more; or who they are.
Yes, you can subscribe to their MailChimp email list; more data collecting from  their side.
They place one or two cookies on your system without asking for consent.

",1,2018-11-09 09:24:56Z,444
37,3,"
According to GDPR's extremely broad phrasing all EU citizens have this right to request their data even from companies that do not have any physical holdings in the EU.  This is a proper request under that regulation.  I would say if you have data pertaining to the email address in question provide that data to that email address.  If the data is of an extremely sensitive nature I would perhaps correspond with the person asking for the data to verify their identity in other ways before sharing. Finally make sure any data you share is done in a secure encrypted way (GDPR recommends setting up a self-serv TLS encrypted portal for this purpose)
",0,2018-11-08 17:47:38Z,445
37,4,"
French citizen here !
The request if I understand correctly seems to be about asking which informations you have on this email.
GDPR request are legitimate and you have to take them into consideration even if your business is not EU oriented. In other word, you have to comply with this user request (if you really have any information on this email) and find a way to answer him, even if you don't speak French. Otherwise, this user could sue your company and you might be fined.
electronicprivacy.eu seems to be a kind of ""patrol"" checking the actual state of GDPR compliant websites. I don't know their final aim but you should consider replying to them as they might be building some kind of GDPR compliant 'ranking' or whatsoever. This being said, it is very very unlikely that it will have any other impact on your business.
If your business is not oriented EU users and you do not want to comply with GDPR, then you have to block them (by blocking their IP address).
",0,2018-11-08 18:14:53Z,388
37,5,"
Effectively GDPR request are legitimate in Europe but don't reply to this mail. It's a spam fishing. See that on @rosebirdpro and https://twitter.com/Security_Hack3r/status/1060294795501989888
Engaged citizen LOL
Bye
",0,2018-11-09 09:06:27Z,446
37,6,"
I received this e-mail too from firstname.lastname@electronicprivacy.eu.()That e-mail address isnt in my database and I never saw it before. And it is not a customer of us and it is not in our mailing list. So it feel like spam from their site and maybe a try to place malware on your computer as far as they ask you to click on their links to answer and not ""just"" ask for answer with reply. 
() to avoid any recognize I am so polite te change his name into firstname.lastname. 
",-1,2018-11-08 21:09:02Z,447
38,1,"
First: almost every single site out there is an ""IP logger"". Every server logs at least this information:

IP address of the client
Browser type and version
Operating system
Which site they came from (the Referer)

So, not only does this site have your IP address, but each site you ever visited has your IP address in their own logs. A few, very few sites won't log any information, but they are a negligible minority.
But you don't need to be paranoid. The IP address alone is not enough to get your name, your home address and the kind of car you drive. It's possible to correlate information and get close to that, but it's not something you will have to be worried about, unless someone is being paid to track you specifically. It's expensive, takes a lot of work and time, and does not always work, so don't expect a full tracking mode to be started just for you because you clicked a link.
Concerning GDPR:

6.1 Processing shall be lawful only if and to the extent that at least one of the following applies:
  f) processing is necessary for the purposes of the legitimate interests pursued by the controller or by a third party

I am not a lawyer, but in sysadmin circles, it seems that protecting your service or a third party from fraud or security violations are legitimate reasons to log an IP address, and thus are legal under GDPR.
",74,2018-09-13 14:41:43Z,219
38,2,"
Any web page you load will have your IP address
In order for your browser to download the content associated with a website, your computer will send requests which include your IP address (this is how the data knows where to be sent). However, your antivirus software may have prevented the connection. Depending on how your AV works, it may have prevented you from making a connection to the suspicious website, and your IP address would not be known to the suspicious website.
It is unlikely someone has your home address from your IP address
The whois protocol could be used to determine a physical address from an IP address. However, in home-user applications, your ISP's information will be returned, not your own. Furthermore, ISPs often dynamically assign IP addresses to their clients, so the IP address you use today may not be the IP address you use tomorrow.
How else could an attacker get your home address from your IP address?
An ISP could store information on the modem such as a customer account identifier which could lead to an attacker determining your home address if they compromised the modem. If an attacker compromised your router, they could sniff traffic to look for your address traversing the network unencrypted or attempt to correlate a Wi-Fi router's MAC address or broadcast name with a Wi-Fi geoloation database such as WiGLE. If an attacker could compromise a computer on the local network, the attacker may be able to find documents which contain the user's home address. Keeping the modem, router and computers well configured and up to date will mitigate the likelihood of this happening.
Assume your IP address is known
You can't operate on the Internet without exposing your IP address, so you should assume that it is known. Additionally, (assuming IPv4) there is a relatively small number of IP addresses available, which means scanners may be trying to connect to your IP address even if you have never ""given"" it to them somehow.
A well managed local network will mitigate the risks of an attacker having your IP address
Because you must assume that your IP address is known or will be guessed, you should set up your network to protect your computer.

Keep your router up to date with the latest firmware, and check its configuration.
Connect your computers to the router and the router to the Internet. This will give each of your computers a private IP address that is not routable from the public Internet. Your router will then forward requests from all clients using the same public IP address.
For each of your computers, set up a firewall to block access that is initiated from the public Internet.

Be wary of all software, including VPNs
You should be wary of any software you download, especially those offering free services. If you are using a free service, it is likely that your data is what is ""paying"" for that service. If you want to maintain privacy on the web consider using Privacy Badger.
",17,2018-09-13 14:47:10Z,455
38,3,"
Unlike on TV, it's not easy to track an IP address to a physical address without getting the ISP involved. 
So don't worry about that.   
VPN's are always a good idea. My personal kit is always on VPN connections but I'm paranoid.
",10,2018-09-13 13:58:18Z,458
38,4,"
You shouldn't worry to much about this.
It's unclear from your description if the antivirus blocked the page before or after it was accessed. So I can't say if they got your IP address. But even if they did, it doesn't matter. You can't get someone's home address from just an IP address, just a very approximate geolocation (like what city you are in). Only your ISP could connect the IP address to you personally.
A VPN hides your real IP address from sites you visit. But it does not help retroactively, so installing one after you clicked the link makes no difference here. Of course, it can be good for the future, though.
",3,2018-09-13 14:11:37Z,9
38,5,"
Every time you connect to another computer via the Internet protocol (IP) the computer at the other end can see and log your IP address. (I am ignoring rare examples where you are sending messages via UDP with a faked source IP address and receive no data back or manage to intercept messages in transit to your faked IP address.) HTTP and HTTPS are TCP protocols, meaning before you connect, you first have a handshake (where you have to observe and send back a random 32-bit ACK), so they can observe your IP address.
That said, if you use a VPN, you only expose your real IP address to your VPN provider, and then expose one of the VPN's IP addresses to every website/computer you connect with.
As for their ability to get your physical address from your IP address:  generally, using public IP address-based geolocation tools only traces an IP address to your city based on the RIR records. That said, someone who can access ISP records should be able to discover the actual address that was assigned that IP address -- so law enforcement or certain ISP employees could access this.  
They also may be able to cross-reference your IP address with a physical address from other data sources -- e.g., if you buy something from an online store and put in your physical address from a given IP address, someone could associate that IP address with your real address (especially if that online store's database is compromised).  Or if someone with GPS enabled (or other rough location tools) on their phone connects to your Wi-Fi network (or observes the profile of nearby APs which seems to be unique and scanned in your area), it would be possible for GPS and data using applications to associate your physical address with your IP address.
",3,2018-09-13 14:43:33Z,460
38,6,"
Although all previous answers are of course entirely correct, they lack an important point: This link was in an email. This is why the antivirus has raised an alert. Exactly for the same reason that Thunderbird or any other email client does not download remote content by default. Of course the malicious website has not your home address, but still it has grabbed some infos about you: (Quoted from Mozilla support)

Remote content is a privacy concern because it allows the message
  sender to know:

each time you view the message
rough details about what application and what platform you are using
your current geographic location (a rough approximation by IP address)
that your email address is actually used (""active"")


",1,2018-09-14 16:18:28Z,461
38,7,"
One point to note is that ""Am I safe?"" depends on your threat model.
If you have powerful or well-funded adversaries, who only know you from your online presence, and are keen to track you down, then your IP address will help them immensely, since they'll be able to get your details with help from your ISP.
If you've got kinda motivated adversaries, then they'll be able to use a Geo IP lookup to figure out which city or region you're in, which might or might not be a threat, depending on what else they know about you.
If you've got no real adversaries, but are worried about opportunists trying to scam you, you're probably fairly safe, although you should always stay vigilant to scams, install regular security updates, etc. Unless you're a particularly high value target, they're not going to drive to your house, even if they do somehow have your address, and they're probably not even going to waste money on a stamp to send you a letter. Scammers like easy targets.
",1,2018-09-14 16:29:40Z,45
38,8,"
The IP address isn't yours - it's your provider's. So no, they do not have your address - they have an IP address from your ISP's or organisation's block.
Yes, these can be used to look up locations. My own work can be found via our outbound address - but the requests going through this will not from address using this gateway will not.
",1,2018-09-13 23:43:56Z,465
38,9,"
You are facing two entirely separate issues.
First, as everyone else has already explained, every web site knows your IP address. They can usually infer your general geographic region, but they cannot locate your home address. Your ISP can map your name to that IP if ordered to do so by a court, however, so you don't have complete anonymity.
The second issue is your antivirus alert. There is nothing inherently dangerous about a web site knowing your IP address, and there is certainly nothing dangerous about displaying it to you.
The antivirus alert probably tripped due to some script on the web page that either invades your privacy or attempts to compromise your computer. If your AV blocked it, you were protected from that threat and don't need to worry.
How to deal with it?
If you don't want to worry about exposing your IP address (because your ISP can link that to your identity with a court order), use a good VPN service.
Don't visit that site again, and be careful where you click. While there is a chance your AV alert was a false positive, there is also a lot of malicious code on the internet.
",0,2018-09-14 14:44:30Z,296
38,10,"
At first I wish to inform you that every server have some application to store the ip details or session details. Some firewall types equipment installed at every server end. They used to save records.
IP address is the physical address of your PC., but not your's.
So nothing to worry. It is not too easy to crack any IPS database. 
They have different firewall types devices for their safety process. It is different issue that hackers used to crack almost all major ISP. But at some level in this recent time ISP are using private IP address series with subnet /31. So it is very difficult to crack.
VPN is safe. But it depends what you are using and through which process.  
",-2,2018-09-15 17:14:15Z,466
39,1,"
Password entropy is calculated by the number of possibilities it could be to the power of the length ie. 8 character password of both upper and lowercase letters = (26*2)8, (26 characters of the alphabet * 2 for upper and lowercase).
If you include the numbers 0-9 as well, then that power becomes 26*2+10, and if you include special characters as well this number can become quite large for number of possibilities. 
So as an example, we'll take a password QweRTy123456, (a horrible password I know). This password is 12 characters long, so the power is the number 12, it uses both uppercase, lowercase and numbers, so we have 6212. Which gives a total number of possibilities of 3.2262668e+21.
Now if we take that same password, but all in lowercase, ie. qwerty123456, the value we have is 3612, giving us a potential of 4.7383813e+18, still an enormous number but much smaller than using uppercase as well. 
Password strength relies on two major factors, length and complexity. As an example I'll show a numeric PIN of 4 vs 8 characters to show the difference. So for a 4 digit pin number, 104 we have 10,000 possible combinations, and if we use an 8 digit pin, ie. 108 we have 100,000,000 possibilities. So by doubling the length of the password, we have increased the potential candidates 10,000-fold.
The second factor of passwords relies on complexity, ie. upper and lowercase, special characters, etc. I gave an example above to show how this increases quickly by using more character sets. 
Just a final note, a password is not as strong as its potential, because a 12 character password could be someones place of birth, a pets name, etc. The contents of a password are also extremely important. The 4 random words model tends to be quite popular and secure, mandatory xkcd. 
ConorMancone's answer gives an excellent explanation and example of the contents of a password vs the entropy, so i'd suggest giving that a read too for more info on this subject.
So in summary, take the number of possibilities from the character set, to the power of the length of the password, and divide by 2 for a reliable method of getting password strength based on brute-forcing techniques.
Hopefully this answers your question, if you have any more leave a comment and I'll update this to reflect your questions.
",12,2018-09-05 09:50:18Z,469
39,2,"
Attempting to add to the other Connor's answer:
Something important to keep in mind is that entropy of course is, in essence, ""the amount of randomness"" in the password.  Therefore, part of why different entropy checkers will disagree is because the entropy is a measure of how the password was generated, not what the password contains.  An extreme example is usually the best way to show what I mean.  Imagine that my password was frphevgl.fgnpxrkpunatr.pbzPbabeZnapbar. 
An entropy checker will probably rate that with a high amount of entropy because it contains no words and is long.  It doesn't contain numbers, but taking a simple calculation (like what Connor outlined in his answer, and what most entropy calculators do), you might guess an entropy of 216 bits of entropy - far more than a typical password needs these days (38 characters with a mix of upper and lower case gives 52^38 ≈ 2^216).
However, looking at that, someone might suspect that my password isn't really random at all and might realize that it is just the rot13 transformation of site name + my name.  Therefore the reality is that there is no entropy in my password at all, and anyone who knows how I generate my passwords will know what my password is to every site I log in as.
This is an extreme example but I hope it gets the point across.  Entropy is determined not by what the password looks like but by how it is generated.  If you use some rules to generate a password for each site then your passwords might not have any entropy at all, which means that anyone who knows your rules knows your passwords.  If you use lots of randomness then you have a high entropy password and it is secure even if someone knows how you make your passwords.
Entropy calculators make assumptions about how the passwords were generated, and therefore they can both disagree with eachother and also be wildly wrong.  Also, XKCD is always applicable when it comes to these sorts of things.
",43,2018-09-05 10:55:28Z,214
39,3,"
Password entropy is calculated by: Knowing (or guessing) the algorithm used to generate the password, and gathering in the number of different branch points used in generating the password you chose.
Let me give some examples:
Password: password
This isn't 26^8 (or 2^38) - because the algorithm wasn't ""choose 8 random lowercase characters"".  The algorithm was: choose a single, very easy to remember word.  How many such words are there?  If you decide, ""there are 200 such words"", then you're looking at about 8 bits of entropy (not 38.)
Password: password6
Similar to the previous entry, this isn't 36^9 (or 2^47) - because the algorithm is choose a single, very easy to remember word, and then decorate it at the end with a single digit number.  The entropy here is around 11 bits (not 47.)
Password: carpet#terraform2
By now you can guess what's going on.  Two relatively uncommon words, with a punctuation character between them and a number digit at the end.  If you estimate that those words were chosen from a dictionary of 10000 words (2^13), you're looking at something like 33 bits of entrophy (13 for the first word + 4 for the punctuation + 13 for the second word + 3 for the final digit.)
So, now, to answer your direct question: why do the various entropy checkers give different values?
Well, let's use that last password: carpet#terraform2.
One entropy-evaluator might say, ""Hey, I have no clue how you generated this.  So it must just be random characters among lowercase, punctuation, and numbers.  Call it 52^17, or 97 bits of entropy (2^97.)""
Another, slightly smarter entropy-evaluator might say, ""Hey, I recognize that first word, but that second string of letters is just random.  So the algorithm is a single uncommon word, a punctuation, nine random letters, and then a number.  So 10000 x 16 x 26^9 x 10, or 63 bits of entropy""
A third and fourth entropy-evaluator might correctly figure out the algorithm used to generate it.  But the third evaluator thinks both words should come from a dictionary of 5000 words, but the fourth evaluator thinks you have to break into a 30,000 word dictionary to find them.  So one comes up with 32 bits on entropy while the other thinks there are 37 bits.
Hopefully it's starting to make sense.  The reason different entropy evaluators are coming up with different numbers is because they're all coming up with different evaluations on how the password was generated.
",11,2018-09-06 13:33:03Z,256
39,4,"
There is no way to judge whether a sequence of numbers or characters happens to be random.  A machine asked to produce a random seven-digit number would produce 8675309 about once every ten million requests.  A Tommy Tutone fan might offer up that number every time.  If a security application happens to need a seven-digit number (perhaps it needs to be accessible via numeric pad), it might judge that 8675309 as having good entropy, unless that application happens to have been written by a Tommy Tutone fan in which case it might regard that number as being just as bad as 1111111 or 1234567.
Basically, the only thing an entropy checker can do is check whether a passcode matches any patterns that are regarded as having low entropy, and observe the lowest-entropy pattern that it matches [every passcode will match some pattern--a passcode of e.g. 23 characters will match patterns like ""any other passcode of 20-25 characters"" if it doesn't match anything else].  If different people are asked to produce a list of all the low-entropy passcode patterns they can think of, they'll almost certainly think of different things.  What really matters, though, is whether a particular passcode would happen to match a pattern that an attacker decides to try.  While there are some sets of passcodes that attackers would be particularly likely to try (e.g. 1111111 and 1234567), the choices of passcode once one gets beyond that are likely to vary significantly by attacker.
",7,2018-09-05 15:32:51Z,316
39,5,"
Broadly speaking, the security of a password depends on how hard it is for your attacker to guess it. This is what those websites try to measure, with different algorithms for guessing. 
A very stupid checker may think that the password ""00000000000000000000"" has high entropy because it's long and the web site's algorithm for guessing is ""try every combination of all characters"", which would indeed take a long time to correctly guess this password. A smarter checker will try numeric passwords first, thus introducing a security penalty to this password (it'll be found faster). A smarter still guesser will try patterns first (like repeated characters or dictionary words) and rightly conclude this password is garbage.
This may sound useless, but note that even a bad guesser will give you a useful upper bound: ""your password can be broken with at most this many guesses"". That is the value these checkers provide. By trying several guessers and taking the lowest number you have a more conservative estimate.
",6,2018-09-05 15:01:49Z,472
39,6,"
Other answers do not use the information theory definition of entropy. The mathematical definition is defined as a function based solely on a probability distribution. It is defined for probability density function p as the sum of p(x) * -log(p(x)) of each possible outcome x.
Units of entropy are logarithmic. Typically two is used for the base of the logarithm, so we say that a stochastic system has n bits of entropy. (Other bases could be used. For base e you would instead measure entropy ""nats"", for natural logarithm, but base two and ""bits"" are more common.)
The term comes from information theory. Entropy is, of course, related to information. (Another reason why we say ""bits"".) But you can also describe entropy as a measure of ""unpredictability"".
Flipping a fair coin once (in the ideal world) produces 1 bit of entropy. Flipping a coin twice produces two bits. And so on. Uniform (discrete) distributions are the simplest distributions to calculate the entropy of because every term summed is identical. You can simplify the equation for entropy of a discrete uniform variable X with n outcomes each with 1/n probability to 
H(X) = n * 1/n * -log(1/n) = log(n)
You should be able to see how one can get the entropy out of system that is one or more coin flips with just knowledge of how many coin flips are to be recorded. 
The entropy of non-uniform (discrete) distributions is also easy to compute. It just requires more steps. I'll use the coin flip example to relate entropy to unpredictability again. What if you instead use a biased coin? Well then the entropy is
H = p(heads) * -log(p(heads)) + p(tails) * -log(p(tails))
If you plot that you get this

See? The less fair (uniform) a coin flip (distribution) is the less entropy it has. Entropy is maximized for the type of coin flip which is most unpredictable: a fair coin flip. Biased coin flips still contain some entropy as long as heads and tails are both still possible. Entropy (unpredictability) decreases when the coin becomes more biased. Anything 100% certain has zero entropy.
It is important to know that it is the idea of flipping coins that has entropy, not the result of the coin flips themselves. You cannot infer from one sample x from distribution X what the probability p(x) of x is. All you know is that it is non-zero. And with just x you have no idea how many other possible outcomes there are or what their individual probabilities are. Therefore you're not able to compute entropy just by looking at one sample.
When you see a string ""HTHTHT"" you don't know if it came from a sequence of six fair coin flips (6 bits of entropy), a biased coin flip sequence (< 6 bits), a randomly generated string from the uniform distribution of all 6 character uppercase letters (6 * log_2(26) or about 28 bits), or if its from a sequence that simply alternates between 'H' and 'T' (0 bits).
For the same reason, you cannot calculate the entropy of just one password. Any tool that tells you the entropy of a password you enter is incorrect or misrepresents what they mean by entropy. (And may be harvesting passwords.) Only systems with a probability distribution can have entropy. You cannot calculate the true entropy of that system with knowledge of exact probabilities and there is no way around that.
One may be able to estimate entropy, however, but it requires still some knowledge (or assumptions) of the distribution. If you assume a k character long password was generated from one of a few uniform distributions with some alphabet A then you can estimate entropy to be log_2 (1/|A|). |A| being the size of the alphabet. A lot of password strength estimators use this (naive) method. If they see you use only lowercase then they assume |A| = 26. If they see a mix of upper and lower case they assume |A| = 52. This is why a supposed password strength calculator might tell you that ""Password1"" is thousands of more times secure than ""password"". It makes assumptions about the statistical distribution of passwords that aren't necessarily justified.
Some password strength checkers don't exhibit this behavior, but that doesn't mean they are accurate estimates. They're just programmed to look for more patterns. We can make more informed guesses of password strength based on observed or imagined human password behaviors, but we can never calculate an entropy value that isn't an estimate. 
And as I said earlier, it's wrong to say that passwords themselves have an entropy associated with them. When people say that a password has ___ entropy then, if the know what they're talking about, they are really using it shorthand for ""this password was generated using a process that has ____ entropy.""
Some people advocate for passwords to be computer generated instead of human generated. This is because humans are bad at coming up with high entropy passwords. Human bias produces non-uniform password distributions. Even long passwords people choose are more predictable than expected. For computer generated passwords we can know the exact entropy of a password-space because a programmer created that password space. We're able to know machine generated password entropy without estimation, assuming we use a secure RNG. Diceware is another password generating method that people advocate for that has the same properties. It doesn't require any computers and instead assume you have a fair six-sided die.
If a password is generated with n bits of entropy we can estimate how many guesses a password cracker needs to make to be 2^(n-1). However, this is a conservative estimate. Password strength and entropy are not synonymous. The entropy-based estimate assumes that the cracker knows your individual password generation process. Entropy based password strength follows Kerckhoffs's principle. It measures password strength in a sense (since entropy measures unpredictability) using the security-through-obscurity is not security mindset. If you consciously keep entropy in mind while generating passwords, then you can pick a password generating method with high entropy and get a lower bound on how secure your password is.
As long as the password is memorable/usable there is nothing wrong with using true entropy calculations as a conservative estimate of password strength; better to under-estimate (and use computer-generated or dice-generated passwords) than to over-estimate password strength (as password strength estimating algorithms do with human generated passwords.)
The answer to your question is No. There is no such thing as a reliable way to check password entropy. It's actually mathematically impossible.
",1,2018-09-06 19:22:11Z,473
39,7,"
There are two crucial things that need to be understood, each of which I will elaborate on

Anything trying to judge the entropy of a password from the password alone is grasping at straws. This is because the strength is a feature of the system that generates the password and not the password itself.
Entropy is not a coherent concept for password strength unless the password is the result of a process that generates passwords uniformly. Entropy is a piss poor metric when the distribution of possible passwords is not uniform.

It's the creation scheme that matters
The strength of a password (whether measured in entropy or not) is a function of the scheme that was used to create it. To quote myself from something I wrote in 2011

I can’t over-emphasize the point that we need to look at the system instead of at a single output of the system. Let me illustrate this with a ridiculous example. The passwords F9GndpVkfB44VdvwfUgTxGH7A8t and rE67AjbDCUotaju9H49sMFgYszA each look like extremely strong passwords. Based on their lengths and the use of upper and lower case and digits, any password strength testing system would say that these are extremely strong passwords. But suppose that the system by which these were generated was the following: Flip a coin. If it comes up heads use F9GndpVkfB44VdvwfUgTxGH7A8t, and if it comes up tails use rE67AjbDCUotaju9H49sMFgYszA.
That system produces only two outcomes. And even though the passwords look strong, passwords generated by that system are extremely weak.

Any password strength meter is going to report those as really strong. To also quote myself describing how 1Password's strength meter works (Disclosure: I work for 1Password), I introduced my 2013 answer with:

The usual way to determine strength of a specific password is to sacrifice a chicken and then read the entrails. But in 1Password, we do better. We compute the horoscope of the chicken before sacrificing it. Given that some of our staff are vegetarians, we are always looking for alternative approaches. 
The sad fact of the matter is that accurate password strength meters are impossible (short of spending years or decades trying to crack the offered password).  The reason is because the strength of a password depends largely on the system by which it was generated. That is not something that can be determined with confidence by inspected a single password.

So it is not at all surprising that different strength meters are going to produce different results. And all of them should be taken with a large grain of salt. As others have noted, the strength meters have to guess what sort of system you used to create the password from the single instance of the password itself. And that guess work in hard.
Suppose for example, the password that you test has mixed case. Now most human created passwords will have the occasional capitalization at the beginning of chunk. That is, humans are far more likely to create a password that looks like 
Password1234 than they are to create paSsword1234? Does your strength system account for that kind of difference? Or does it just spot that there is mixed case and make a computation assuming (incorrectly) that upper and lower case letters are equality likely throughout the password?
Now strength meters have improved over the past few years, but even the best of them are deeply and fundamentally limited because they are trying to figure out what system you used to create the password from the password itself.
Entropy is the wrong notion
Entropy only makes sense if every possible password that the scheme can create is equally likely. In a 2013 PasswordsCon talk, I illustrated this with some extreme examples. (View the PDF of the slides in single page mode to get the overlays).
Imagine a password creation scheme that 9 times out of 10 picks a fixed password, but the other 1 time out of ten picks uniformly from 2512 possibilities. The Shannon entropy of that scheme is around 52 bits (which is very respectable for a password), but clearly it is a terrible password creation scheme. I argued, instead, that when there is a non-uniform distribution we should just use the likelihood of the most likely result (min-entropy) as a strength measure of the scheme.
Although that is a contrived example to make a point, the fact of the matter is that the distribution of human chosen passwords is a fat headed distribution. It doesn't really matter how many different passwords could be generated if lots of people all end up picking from the most common ones.
So I strongly recommend that if you use the word ""entropy"" that you use ""min-entropy"" which is log2 p(xm), where xm is the most likely password and p is its probability of occurring. Note that when the probability distribution is uniform, min-entropy and Shannon entropy work out to be the same. But when the distribution is not uniform, they can be very different.
",1,2018-09-07 02:17:06Z,474
40,1,"
Needing to install things is kind of the point of needing the laptop, so it makes perfect sense that they want to install Office, AV, and certificates. There are no surprises there. To do that, they need admin access, but I would want to revoke that access once they were done.
I would want to know the list of everything they want to install, and if they have central control over the AV (and if they do, why they want that). 
If your worry is that they might install malware, then download a Live CD of an anti-malware program and run it on the laptop after they are done.
If the laptop is only used for school work, then there is really no harm here. If your child will be using it for other things, then there might be some privacy conflicts. 

The onslaught of comments and the split in votes highlights a difference in understanding of the operating model here. This is not a situation where the school wants sudden control of a personal device. This is a situation where the school is asking the parent to purchase a device for the school to control and this answer is meant to be applied in this model. The school needs to be able to control the device as a part of due care (and remember that the child in this case is a minor; 12 or 13). In terms of protecting the child's privacy, my advice to make sure that the device is only used for school work holds. 
The fact that the parent can retain admin control is a great thing for the protection of the child, something that would not be possible if the school owned the device. The parent can inventory, patch, and uninstall.
This operating model means that the school can ensure consistency of software, which would be required for teaching consistency, it lowers the cost to the school (yes, it increases direct costs to the parents, but does offer cost efficient options) and it offers due care controls for the protection of the child. You just have to shift your mindset that just because you bought the device does not mean that you should have 100% control of the device. 
And again, with the new onslaught of comments, I say: consider the idea of a ""burner"" device. You own it, but it is meant to be, at least in part, out of your control and properly classified for certain activities. 
If the operating model was that the school wanted sudden control of a personal device, my answer would be very different (more like AviD's).
",138,2018-08-28 20:46:50Z,123
40,2,"
It might just be because I am already ""that parent"", but it would be a strong NO from me - and the school administration would get a strong talking to about this. I would push to have that policy changed (though without much hope), for everyone and not just my own child. 
There are privacy issues. Security issues. Potentially legal issues - is the software licensed? Cracked? Are they logging all traffic from the laptop, or more? Do they want to install custom software?
And, why are they even asking for all this? What justification could they have. 

They want to ensure the children are safe online.
Fine, require some form of parental control. It's your laptop, it's your child - it's your responsibility. (E.g. my son's school requires content filtering on smartphones brought into school. They ""demand"" some dodgy app by a local fellow. I declined and installed a proper app.) 
They want to ensure children's laptops are secure. 
Great, first lesson ""how to stay safe online"". Require Windows Defender (or some other AV/AM) is active and updated, etc. Though this really shouldn't matter to their network...  
They want to ensure children are not accessing illegal / inappropriate sites.
First of all, it only concerns them inside/during school. None of their business at home... And they can easily set up a locked down proxy for the school network.
And again, at home they should still have the parental control / filtering software anyway. 
They want to educate your children.
Oh do they? Because this sounds like the opposite of that. This is an educational opportunity, a veritable goldmine for several topics, and they are going the other way. 

You might need to discuss with the teachers, the principal, the school board... You might need to reeducate them about this. And you might even lose, but fighting this is the right thing to do - as @Mike and some of the commenters mentioned, best chance to teach your kids about safeguarding your own privacy and preventing onerous demands from misguided authority. :-) 
",347,2018-08-28 22:56:55Z,201
40,3,"
I wouldn't.
You have no real way to tell exactly what they've changed. Some schools are excessively nosy or controlling.
And even if the district is being respectful of your privacy, they could have a rogue admin in their ranks.
Others have been bitten.
There have been lawsuits because of blatant misconduct before. They have alternatives, so administrative access should not be necessary.
How should they do it?
Cloud-based software requires no installation. As long as you have a modern OS and web browser, you're ready to go. While I dislike cloud apps in a number of scenarios, it's perfect for bring-your-own-device (BYOD) scenarios. Obviously, they did not choose this if they're asking for admin rights. You might suggest it to them.
With volume-licensed software, they should able to provide a product key or setup a license server on their network. (The stuff that requires license servers is more common for university-level applications, but I've heard of it in technically-oriented college prep schools, too.)
What would I do?
I would install the applications myself. It doesn't take a lot of time, and typically they don't change over the course of the year.
Certificates can be installed very easily on Windows, but I'd have to see them first before I could say whether or not that's a good idea.
",135,2018-08-28 23:01:45Z,296
40,4,"
Others have already stated why this is a bad idea and I fully agree, don't let them install those stuff (certificates??, no way), now, you don't have to be that parent if you present some options:

Multi-booting: this way your kid can have a school OS and a home OS, he just need to let them install all the stuff on the school OS and remember not to do any private stuff while using it (and always boot that one while on class). Add encryption on each OS for better results.
VirtualBox: almost the same as in the previous scenario, but the school may have some concerns regarding network access
GoogleDocs: if that is all they need this is a good option, unless some advanced docs features are used that may not work properly (or if it's more work for the teacher to teach the same in many platforms)

Good luck and let us know how you handled it.
",64,2018-08-29 00:23:05Z,480
40,5,"
Under these circumstances, the ideal case is simple.
Get a ""burner"" laptop for schoolwork only.
Use standard tech and low specs suitable for the work at hand (contact their IT dept to find out what they feel is suitable) and let the school do whatever they want with it.
The burner should cost at most a few hundred dollars and save a lot of hassle.
If your kid has a personal laptop already, this isn't a work device and will only be a source of distraction at school anyway.
This side-steps and compartmentalises the issues of personal-data and Info-security by keeping the personal and work lives separate in the first place.
It's clear enough to me and evidently to yourself that a personal device's security measures should never leave your personal control.
With any due respect to your school's IT dept, schools don't have a good reputation for information-security.
With hundreds of kids of all ages exploring what can and can't be done, they're generally overtaxed and loopholes are found in school IT systems all the time.
Never mind that School IT Departments are rarely information-security specialists, their chief job is maintenance of the huge network across the school and monitoring to make sure the system isn't being misused.
If their Info-sec game was strong enough for me to trust them with free access to my data, they'd probably not be working in a school.
",40,2018-08-29 15:02:30Z,225
40,6,"

Now the school IT department wants to install some software on the laptop and is asking for administrative access.

The school does it because it's easy for them. Lots of parents are computer illiterate and asking every parents to review and install software every time they needed to and keeping all of them up to date is very laborious.

I feel that on principle this is not right, as it's not the school's device, so school staff shouldn't have access.

I totally agree with you on this. It's your device and it's your child, it should be your right to draw the line on what is and is not acceptable.
On the flip side, the school also has responsibility to other students and parents. If other students come to know that your children's device is not set up with the same security software as the rest and they use your child's device to access or do illegal things or if your child's device is the entry point of a virus infection on the school's network or other security breaches, then you may become partially liable for that and the school may not be able to shirk the entire responsibility to you either as they have a duty of care.

I'm setting myself up for a few years of headache as any time the school wants to add new software, I'll have to do it myself. 

You can't have your cake and eat it too. There is no rights without responsibility. If you don't want the school to have full administrative access to the device, you have to be prepared take the responsibility yourself.

What would you do? 

You should discuss the issue with the school. If the school has a BYOD policy, you likely won't be the first nor will you be the last person to have such concerns. The school may have a policy to allow you to self administer, you'll have to negotiate what you would or would not accept. In the end, you have to be prepared to either switch to another school or to permit some or full administrative access to the device if the school's BYOD policy does not allow you to self administer.
In such case where you decide to permit the school administrative access to the device, you may want to take some steps to protect your child and yourself from the device. You may want to treat the device like a guest or untrusted device when in your network by putting it in a separate virtual network. You should not forget to wipe the device clean once your children is no longer in that school. You may want to talk with your children on what they should not do with the school laptop, and perhaps to use another device for all their personal needs (e.g. entertainment, personal emails).
If you decide not to allow the school full administrative access, be prepared for the possible consequences. The school may have a policy to not allow devices that does not comply with policy from accessing their internal network and some school resources may only be available from said internal network. You may have to settle with finding another way for your child to access said resources.
How far you want to take this is really up to you. Be flexible and be prepared to compromise. but also have a clear idea on what the line that you won't cross would be.
",35,2018-08-29 04:25:30Z,310
40,7,"
From a sysadmins point of view:

They want to install Office, Outlook, an AV and some site certificates. 

If you already have an AV installed, (which you should), then another AV will conflict with yours and be a larger threat to your child's computer.  Do a Google search for: ""multiple antivirus installed"" and you'll see why it's bad.
As for the certificates, I interpret this as a way to spy on your child.  A universal certificate can be used to decrypt ALL of the traffic to/from that computer.  Lenovo security incident.  They should have a basic filter to block bad websites, and if they don't: Smoothwall can help them out.
Tell them to give you the keys to Office, (if they grip about it, there are alternatives out there, such as OpenOffice and LibreOffice), and ignore their requests to install certificates.  You don't know anything about their IT department; and they sound/usually are incompetent.  They could possibly be perverted as well, (they could install other things which might give them access to the cameras).
Tell your child to never give them their laptop.  There are ways to extract passwords and if push comes to shove, you can simply: backup the password file, wipe the passwords, install whatever, and then restore the password file, (it's called the SAM file).
",31,2018-08-29 06:58:06Z,487
40,8,"
Let's break this down:
Your concerns as a parent

Privacy: You don't want school staff being able to view what sites your kid is visiting, what files they have on their laptop, and other things that would come with admin access.
Security: You don't really trust the school having the ability to install software; you're worried about viruses getting onto the laptop because of the school's sloppy security.

Their concerns as the school

Accountability: The school wants to monitor what the students are doing online, I assume, while in the building (I assume that's what the installed certificates are for). This could also have legal repercussions on the school if students are doing illegal things from their IP addresses or via software for which the school is paying the licencing fees (Office, Outlook, etc). This could also have repercussions on your kid if they're caught doing something very illegal.
Security: The school doesn't trust teenager-owned laptops to be virus-free (probably a good call). They want to enforce a minimum security standard before letting devices onto their network.


Good for you for being concerned and raising the question. On the whole, it seems like they are taking reasonable security precautions; though there is the risk that your kid's internet browsing history, downloaded files, etc are visible to the school and they get in trouble for it. 
There may be a silver-lining here of talking to your kid about internet privacy and being aware of what they do on that machine (a very real-world problem that any adult with a work computer has to learn to navigate!)
",16,2018-08-28 20:51:07Z,489
40,9,"
I don't think anyone else has discussed the certificate issue:
In my experience, a lot of schools use a MITM firewall to intercept HTTP traffic for their filtering policies such as to look at the content of the page. This is a problem for HTTPS because they have to replace the certificate with their own - which is probably what they want to install. 
See this vendor for example:
http://www.rm.com/products/online-safety-tools/rm-safetynet/ssl-interception#downloads
I am assuming that is what the certificates are for. There is no reason to give the IT department admin access when you can just install it yourself, it's easy.
Installing will allow you to browse HTTPS but obviously bear in mind that they will be able to intercept and read communications, over any network that is controlled by the owners of that certificate.
To avoid the interception you could probably run an encrypted VPN on one of their open ports 80/443/53 etc. and tunnel all your HTTP traffic through that. Just don't tell them because it is probably against their policy.
",14,2018-08-29 14:43:57Z,491
40,10,"
I'm going to provide a situation that I have experience with, and then draw parallels. I am a Software Engineer, and have worked at several shops with a BYOD (bring your own device) mentality. Each of these shops had their own security practices and software requirements that devices were expected to follow, and it was understood that IT would periodically want to verify that your device was compliant. However, this was either done remotely (by the network upon connecting) or in the presence of the developer. Had IT asked for administrative access, that would have been a major red flag because it is NOT their system. 
As the owner, user, and maintainer of the system, it's care and upkeep fall on one person: you. In the event that the device is compromised, infected, etc.. as a result of their practices, you are the responsible party. YOU will be the one who spends time and money cleaning or replacing that system, not the school. You already recognize this by coming here and asking if this is a reasonable request on your machine. If they were maintaining and caring for the machine, this would have been a non-issue, they would have just installed the software while they were doing setup or maintenance.
What would be reasonable (in my opinion) is to have the required software listed so that you can install it yourself. If you are expected to purchase and maintain your own device, you should be trusted to make sure it is compliant with their standards (both hardware and software).
Personal opinion: This is a missed teaching opportunity. Your child will be the primary user of the device, and teaching them how to care for and keep it protected (even with your guidance) goes a long way toward learning and using better safety practices online themselves.
",13,2018-08-29 12:34:30Z,492
40,11,"
Both a burner laptop and a virtual machine are respectable options.
Multiboot is not, as any time the hostile os is running it can modify the clean os, with beyond-admin privileges.
I feel like virtual machine might be superior in more regards than simple cost:

The child might benefit from being able to use a better laptop at school. I am talking about both practical benefits such as better quality keyboard and trackpad, bigger and higher resolution/better finish screen, and psychological benefits (no reason to force them to use a bargain bin laptop in front of their peers, this might give the teacher an in to incite mockery of the child from the ""paranoid"" family)
The base OS will be clean, so if the laptop is accidentally/necessarily booted in the home/office, the network isn't exposed to the school's dirty installation. It's possible to boot is without network by default.
The child gets to have their clean personal laptop on their person, which they can use offline/on a mobile connection/wherever, which means they are much less likely to get forced into a situation where they'll do something unfortunate like accessing their personal email account from the school's OS.
It's easy to inspect it, since you can monitor its network usage externally, or take snapshots of the drive and compare them.

However, I wouldn't trust the IT not to mess with the base image. Yet I assume that it wouldn't be ergonomically possible to prevent them from booting the machine unsupervised, on their own time. As such, this is the protocol I would use:

Install linux or other base OS. Don't give the child any administrative access or BIOS/UEFI access, so that they can't boot from an external device.
Install virtualization software, and install the required version of windows inside that.
Create the administrator account on the windows vm in accordance with the requirements.
Ensure that all updates are installed on the vm.
Backup the whole linux os drive, and the vm.
Give the laptop to the school.
When you get it back, don't boot it, copy the drive again, and treat it as hostile.
Overwrite the host drive with your backup.
Extract the vm drive volume (drive volume only, not vm settings) from the hostile image.
Replace the vm volume on the now trusted laptop with the hostile image's vm volume. Be careful, make sure the volume file doesn't allow them to access any host drive partitions, some formats can include symlinks or full access to host disks/partitions.

Now you are basically safe, from the technical standpoint.
For extra credit, you can diff the base disk image and the VM volume image with the old one, to see what the school has been up to.
The second part is explaining this setup to your child. You can't possibly overdo this. While it's good for them to have confidence in your setup, they must understand just how dangerous the school's VM is. Explain that the threat is comprised of both the school staff, who both see them in person and can have a significant effect on their future, and third parties that can compromise the VM or MITM any network activity from the VM. Explain that neither of these parties mean well or are even neutral, as such ""power"" corrupts people and is always abused very quickly. Give some graphic examples, such as as all of their private chats being distributed to every teacher, parent, and student, or their webcam and microphone being accessed by strange women and men, including to scout out your home for a robbery, invasion, or kidnapping.
Don't forget to include that these consequences can trivially result not only from doing their personal computing inside the VM, but also from not practicing hygiene with the VM, such as running executables from a USB drive that was exposed to the VM.
Finally, you need to consider the needs and desires of the child. If they want to use software such as creative software by Adobe, Ableton, or play games, they may need their own Windows VM or boot option, otherwise they will be tempted to use the school VM.
Also, this assumes that they only want access to the machine once. If they want repeated physical access this complicates things.
",13,2018-08-30 13:03:21Z,494
40,12,"
Big No!
While most everything has been covered, there is still the issue of child safety.  Every year there are multiple lawsuits about schools spying on kids through their webcams.  While you might think that they won't be able to do that with what they plan on installing, there is a good chance that the AV will allow them to.
Take this case for instance: https://www.cbsnews.com/news/610k-settlement-in-school-webcam-spy-case/
In this case the school thought that the kid might be selling drugs and essentially spied on him through his webcam.  What if the computer was in your child's room while they changed and an administrator was watching it?
Additionally, you have no idea how safe their system is.  Even if they aren't going to, how do you know that they won't be hacked, or are already hacked?  Do not let the school install anything on your kids computer.
The best way to prevent that would be to either give them a chromebook for school, and lock it down with parental controls, or prevent their account from installing program and not give them a password for an account that can.
",9,2018-08-29 20:08:45Z,495
40,13,"
To add to the others: Have a look at the list: 

They want to install Office, Outlook, an AV and some site certificates.

Why?
Installing Office means teaching a dependency on a big vendor early. The teachers themself should teach in a way, that it works in libreoffice as well or even other office programs. Most things done in schools do not use the advanced features of a special office suite anyway. A media compentent teacher should focus on teaching techniques, not programs, which can be applied to different similar programs. It is way more useful to teach how to find out which button makes the text bold than learning by heart how the button looks in a specific MS office version.
Why do they need Outlook? There are several good e-mail programs, some even free*. I would guess Outlook is not the program of choice for most pupils and in my experience most typical users do not use a e-mail program at all but web mail.
AV is disputed anyway, read the lengthy discussions about how AV can be ""snake oil"", exploits in AV programs and the general concept of making a system more insecure by running a high privileged complex program.
While AVs were often recommended in the past, many experts today recommend to use only what the system brings with it (i.e. Windows Defender for Windows).
Even when you want a possibly better AV, you should decide which one, not the IT person at the school. Especially since some solutions are subscription based and try to sell their subscription after a free trial period and switch themself off if you do not buy it.
""some site certificates"" sounds like ""Man in the middle tools"". What might be okay on their network, is a real security risk when the personal laptop is used somewhere else, because you do not know who may have the keys. For example some MITM security appliances use intermediate CAs, which are not limited to the single appliance, but to all appliances sold by the company. This means with the laptop prepared by the school, the traffic may be sniffed in other networks as well.
Giving admin access is a bad idea anyway (do you know what else they might be installing or if their media are infected?) and teaches the children to give persons access rights just because they insist on it.
The next time the password inspector calls them, they will give admin access as well, because you do so, don't you? 
So either they should hand out school laptops or work with what the children have, but do not demand to be admin and install stuff.
* both as in beer and as in freedom
",9,2018-08-29 12:22:48Z,236
40,14,"
To minimize your hassle, I suggest you inquire about minimum specs and buy your child a ""work"" laptop to be used only for school. Then you just let them do whatever they want. Then the school is at fault for any problems and you have no further work with it.
If you are budget strapped, then the next best solution which still gives you complete separation and tamper-security of the home os at the cost of some additional work is this:

Backup
Inquire/think about minimum hard disk space for a school OS.
Make a partition that large/shrink the rest.
Install Windows on it.
Encrypt the home os (Veracrypt can encrypt Windows without reinstalling, some Windows versions may have that ability built in and many Linux distributions offer encryption during installation)
Backup again.
Let the school do whatever they want. Maybe put a sticky note on the computer explicitly telling them to leave the partition scheme alone and that the encrypted partition is private data (no need to elaborate - an os is data too). Maybe set up the computer to not display the boot manager but just boot the school os immediately.
If they messed up the encrypted install, use the backup to fix. Enable boot menu again if you disabled it.

I'm not quite sure if you need a second license of Windows for that.
Otherwise you will need to compromise and will probably have much more work in the long term.
",7,2018-08-29 14:02:06Z,493
40,15,"

Should I let my child's school have access to my kid's personal laptop?

No.

My kid is starting 6th grade and the school requires him to get a laptop and bring it to school.

No.
The school can bulk buy books, stationary, tools and computers at a discount and tax free. How is it cheaper or better for each parent to know: what to buy, where to go, how to set it up ...
The school can provide a Lenovo 100e (Windows 10 - S-Mode) or a Chromebook (Chrome OS) for under $200. Perhaps a refundable deposit is appropriate but the cost will be less than having each parent spend the time and money to buy whatever independently.
Much like each student receives the same books and opportunities each should be provided with an equal computer. You don't want some students to be way ahead while others are way behind.
If there is homework the computer can be safely secured in the school's locker and the child can access the same material on their home computer, otherwise much like their books they can bring the computer home with them.
If the parent supplies the computer and it breaks down what does the child do, return home and grab another one?
If the school supplies the computer and there is any problem (including breakage or forgetting it at home) a new computer can be provided and the child can be back to work in a minute (with all their files the same as on the other computer).

Now the school IT department wants to install some software on the laptop and is asking for administrative access.

It is understandable. The child needs to get the same software as everyone and the school has a bulk license. They must also secure Internet and mail access - the school is responsible for your child's education and safety.
That is why the school should provide the equipment and stay out of your child's personal effects unless there's a legitimate safety concern - implementing an unfair and poorly thought out plan with unfair invasions of privacy justifies nothing. On the child's personal computer (and phone) is private information and anything else permitted by the parent, that's the parents responsibility. The school needs to stay on their own side of the firewall.

I feel that on principle this is not right, as it's not the school's device, so school staff shouldn't have access. 

True, it's a warrantless search.

Additionally, I don't have any sense of how good the school's security practices are. What if they inadvertently install malware? 

True. You also don't know if one particular software has bugs with whichever random hardware you supply. It's an unnecessary multiplication of work with diminished value.
When the school supplies the computer they can just line them up and hand them out. When each computer is parent-supplied they might have a readymade install disk, but you don't know what that will do to existing software or parent installed protections.
If the child doesn't have to supply the key to their home why the key to their computer, their privacy.

However, if I refuse then I risk being ""that parent"" and I'm setting myself up for a few years of headaches as any time the school wants to add new software, I'll have to do it myself.

Be that parent, the one whom spoke up for their child.
If the school supplies the computer they just hand the old one in and take the new one with the updated software, alternatively it can be installed automatically over the school's WiFi. Handing over the child's personal computer everytime there's an update means hours a month without it. Hardly a better solution.

What would you do?

Speak up. Pay less. Get a better solution that makes sense. Protect your privacy.
You teach your children to say no to adults imposing the wrong thing - you can say no too.
",7,2018-08-31 18:21:34Z,233
40,16,"
If I were a parent, I would firmly say no to this. This is mainly because the laptop is paid by the parent. You should have control over what you buy, and I believe that it is already pushing it to require every parent to purchase a laptop for their child and have them bring it to school (especially if you believe that technology doesn't benefit learning).
I understand that they would want certain applications installed on the computer, productivity applications such as Office are reasonable (although, you could install it yourself). With Windows Defender and a good network firewall, another AV shouldn't be needed.
A site certificate is one of the main problems I see in this, since it is by far the most invasive. All usernames, passwords and other personal details submitted through the school network are visible to the school. If a security breach occurs, it is possible that this information could be part of it. Do research on what firewall system the school uses and their past security track record.
If you agree to this, at the very least, ask for a list of software being installed and ask to be kept updated when new software is installed on the system. Do research on the software and their credibility. Regularly check on the Windows programs list. 
I am a student, I have seen the way that my school board has dealt with their network security. The network is filled with security problems: accessibility of development servers on the public internet, directory traversal attacks, privilege escalation on certain web services, etc. There is a reason for this: they buy from the lowest bidder. Given this, make sure that all software that they install are from reputable companies.
Another comment: Some courses have a specific requirement to use Microsoft Office, so installing other office software may not be an option.
",6,2018-08-30 00:32:45Z,498
40,17,"
While many answers here outline potential dangers arising from giving someone admin access, it should also be noted that it's also a reasonable tool for the job the school IT is about to do. That's what I would request from parents if I were to do it, since explaining how to properly configure their own system would be a dead end. 80% of them wouldn't even know what a certificate is.
It's true that admin rights can be easily abused, but assuming you trust your school, I wouldn't worry about it too much. Think of it this way: you're asking yourself whether those guys will abuse their access to a laptop, but you have no problem leaving you kid with them for the whole day, every day. Are you sure the laptop is the thing you need to worry about here?
Incidentally, even if you choose to educate school IT about better practices,  their Outlook licenses won't transform into Linux seminars for teachers. I agree that it's a noble cause to fight for, but in your situation it's far too late to actually change anything.
",4,2018-08-30 16:33:36Z,499
40,18,"
The school ""needs to install certificates""? That definitely is a red flag.
As for installing Office, I would say install a copy on the computer for him rather than let the school do it. While some schools do offer this service (the community college does offer free access to Microsoft Office 365 which can be accessed online although using the apps would be better), again that is a red flag.  Install or subscribe to Office 365 yourself (which there is a student discount) or go open source and use Libre Office.
The fact that your school wants to do this seems less of a service to you the parent and more of an invasion of privacy upon the students, something I would seriously not consent to especially if there are ulterior motives which it sounds like there are.
The most access your school should have to get into your child's computer should just be the Wifi Password.  Beyond that, get something in writing explaining exactly what they want to put on there.
If they go over the top (which given the paranoia of a lot of school districts) and try to strong arm you or your kids into doing it by getting the student resource officer involved (who is generally employed from the local police department), assert you legally have the right to say no.  Tell them to get a warrant.
If an arrest is made to force an illegal search of the device, that is illegal. 
Because you have consented to let the school have access to your child's computer, you have given the school or school district the right to let the police access that device via a third party that now has control over the device. (Those certificates can be used by the school to get into the computer.)
In the future, NEVER consent to letting strangers (even if it is your kid's school) to access your computer without thoroughly reading the fine print.
Know your rights so that the school doesn't use local law enforcement or vice versa, against you or your children.
",3,2018-08-30 17:55:46Z,500
40,19,"
Your trading control for convenience. 
You should use the laptop only for his school use and nothing else as you have no idea what they will put on it. As there will be nothing personal on it now or in the future, I think over reacting to this situation would be bad. Treat it like a burner phone. It might feel wrong, but would it be any better if they gave out school laptops already loaded and your child used it? No. You wouldn't even blink.
It's like letting someone else change the oil in your car. You could do it or you could let someone do it maybe right, wrong, or screw something up with your car. You really don't even know if they changed the oil unless you check or what oil they put in it.
",2,2018-08-29 14:08:20Z,501
40,20,"
If anyone wants administrative access to my personal computer, I always require that I acknowledge what they wants to do with that privilege. The degree of detailedness depends on who the person is, and for the IT department of a school, I will demand that I acknowledge the following:

What exact softwares, including versions, they're going to install; What such softwares are intended to be used for; Whether they can be cleanly removed afterwards
What certificates they're going to install; What the purpose of the installation of such certificates is (What they're meant to be); Whether the certificates are signed by a trusted party (VeriSign, etc) or are handcrafted

IMO, some level of monitoring from parents would be good for a kid/child, but digital surveillance from a party that may or may not be directly responsible for the child's activities could be a nightmare.
From my experience, school IT departments usually suck at what they're supposed to master in. I have even hacked into the online management system of my middle school twice (and dropped their database). So as a general advise, it's better to reject request for administrative access to any device you own, by the IT department of a school, or get a ""burner device"" as suggested in other answers.
If it's not, or hardly, possible to reject the request or get a ""burner device"", I would consider the applicability of the following options:

Demand that the software be provided for me to perform evaluation and installation on my own 
Create a full system backup (also restore points for Windows) for later restoration
Install additional software to monitor / limit / isolate those ""school softwares""
Watch the whole setup process

In addition, these factors must be taken into account before proceeding

""Man in the middle"" attack made possible by untrusted certificates
AV software conflict
Privacy leak (what software uploads what to where? you don't know)

Summing up, it's important to separate one's ""work (or study) environment"" and ""personal area"", and set different trust levels to different governing parties.
",2,2018-09-03 06:29:37Z,222
40,21,"
I would add that the child should be told to only use that computer for school purposes (Don't buy things from it, play games, download stuff, or even really use it with other accounts). After they graduate or if it becomes redundant or replaced, reinstall the original OS (with Windows, there's a wipe-disk option on the install media -- use it) to ensure that you do not leave trackers or spyware on the device (though this results in total data loss unless your files are backed up. Be careful if doing such a backup since you'd only want to transfer files that you know are yours. 
Aside from the certificates being a huge tip that there's probably MITM going on (snooping on allegedly-encrypted web traffic that SHOULD be protected -- like logins or shopping) or weird/unnecessary ""security"" features, it is apparently quite possible to install software that cannot be detected or uninstalled (registry hacks can do this, or rootkits can be used, although the latter might be noticeable to another antivirus).
An archive binge through something like TuxedoJack's Reddit posts about tech support will also tell you some of the things that they can do with your devices -- and not just on school grounds. This can include tracking, remote reading of your files, keylogging, and even remote BSOD capability, especially if it also uses a VPN (note also that even independent antivirus scans from alternate boot material may not find commercial versions of such software, for security reasons or because the software may not be aware of all such tools!). There are numerous stories of people who have tracked a stolen computer that was not ""nuked"" with a fresh install. (And, for that matter, some software can run from the BIOS/UEFI instead, but there's not a good way to prevent that. It is better to hope that they don't have that sort of capability than to try to detect it.)
",1,2018-09-01 00:38:42Z,502
40,22,"
As already stated in other answers, creating a virtual machine for the school environment may be the optimal answer. It costs nothing in additional hardware, keeps the school environment isolated from the personal environment, allows the child full-time access to the best hardware you can budget, and allows the school to do as they wish with the VM, over which you/your child can still maintain supervisory control. The host O/S doesn't need to be Linux; it can be whatever you/your child prefers; there are VM solutions (like VMware) that can host Linux or Windows on any other popular O/S (Apple, Windows, Linux). Windows has its own VM solution, although there may be licensing/price considerations - you need at least a Pro edition to be a Hyper-V host. If the guest O/S is Windows, there may again be a licensing consideration. Even with those considerations, it may be your least cost/most expedient solution that satisfies everyone's concerns/requirements.
",1,2018-09-02 21:36:58Z,503
40,23,"
The easiest solution (outside of getting a burner laptop) is to do as following:

Re-format the machine
Let the school do whatever they want with the laptop, for however long they want
Re-format the machine again and this time only install the software that you actually need for school work

This lets you have your cake and eat it too: the school's IT department thinks your laptop runs their software while in reality it only runs software approved by yourself. 
",0,2018-09-04 18:49:46Z,504
40,24,"
I am a security professional. There is a very simple answer: If someone else has administrative access to your device, it is not your device anymore.
A relatively easy solution would be to install a ""for school"" VM and give the school IT team admin access inside that VM.

Some longer explanations:
No, checking afterwards isn't enough against a malicious user. If you trust the school enough that you assume they are not malicious, you can be ok with checking their work. A malicious actor has dozens of ways of hiding rootkits or malware in ways that nothing short of a full forensics will find.
The school request is reasonable as they want to ensure the same environment exists for every kid. They should not have any reason to be opposed to having a seperate (VM) environment.
",0,2018-09-05 11:24:52Z,25
40,25,"
It really boils down to a hard drive.
You can swap out or wipe the HDD/SSD at any point and undo all changes they made.  You can probably back up the state of the laptop and restore it within a VM or external drive, then have the kid launch/boot that while doing school work.
I like to use Clonezilla for copying and backing up my drives.
",-2,2018-08-30 16:35:28Z,506
41,1,"
It depends. Could this number linked to a single person? (e.g. it is your cell number and if I know this number is in a database and know it's yours, then I know you are in that DB)
Then yes.
If this is not possible (e.g. it is the central call in number of a big corporation that is connected to any available agent) then no.
If you only have phone numbers without further information about them, you have to assume it is PII, because you don't know if a number belongs to an individual or not.
",42,2018-07-03 08:49:18Z,508
41,2,"
I don't use WA, so don't know specifically what practice the question refers to, but let's assume that the question refers to a practice where an app on a user's phone gets access to contacts and text history on the phone and uploads all phone numbers to the server without also uploading the names associated with those numbers (despite having access to them.) 
Are these uploaded bare numbers, without names, considered PII? 
Yes, absolutely. 
The server isn't collecting number sequences at random, using them to populate a model of some kind, and then discarding them. 
It is building a durable data structure that has entities that are intended ultimately to map to humans, and storing those numbers as metadata with those entities. (Key test- when a contact on the phone has 2 numbers, are those 2 numbers somehow associated with the same durable entity server-side?)
The fact that at a particular zoomed in point in the overall architecture at a particular point in time there isn't a name directly stored with the number in a relational db row is irrelevant.
In the big picture architecture, comprising both client apps and the server, and both the data flows currently in place as well as the data flows that could reasonably easily be put in place without user action- eg an app update that collected names as well could be trivially rolled out without user knowledge or additional permission- big picture, this is an architectural graph that has PII. 
PII is a forest-level concern, not a tree-level concern.
",15,2018-07-03 11:47:05Z,510
41,3,"
PII is anything that directly or indirectly can be associated to a person based on Regulation 2016/679 of the European Parliament.

‘personal data’ means any information relating to an identified or identifiable natural person (‘data subject’); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;

From the definitions section of the aforementioned regulation
Because you can correlate a phone number with a person (owner of the contract) and that number might be unique to that person you should consider it as PII information.
I am saying this as precaution as you never know if it is a public phone number, shared phone or any other use case. 
In many cases even if shared number it is still possible to correlate it to a person. 
The only one that is not possible is the public phones. 
Because we do not know this we cannot take the risk of relaxed security in all other phone numbers that might be uniquely associated to a specific person.
IP address can be also be PII, because we do not know if is a proxy or a router at some home we should treat them as equal.  
A more simplistic explanation can be found in the bellow link from the European parliament with examples. 
https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en

Personal data is any information that relates to an identified or identifiable living individual. Different pieces of information, which collected together can lead to the identification of a particular person, also constitute personal data.Personal data that has been de-identified, encrypted or pseudonymised but can be used to re-identify a person remains personal data and falls within the scope of the law. Personal data that has been rendered anonymous in such a way that the individual is not or no longer identifiable is no longer considered personal data. For data to be truly anonymised, the anonymisation must be irreversible.The law protects personal data regardless of the technology used for processing that data – it’s technology neutral and applies to both automated and manual processing, provided the data is organised in accordance with pre-defined criteria (for example alphabetical order). It also doesn’t matter how the data is stored – in an IT system, through video surveillance, or on paper; in all cases, personal data is subject to the protection requirements set out in the GDPR.

",10,2018-07-03 09:20:59Z,512
41,4,"
Yes
Clearly and fundamentally, without any doubt; and with especially strong implications when used in a Social Media context like WhatsApp.
The issue is not whether WhatsApp links this phone number to some name when sending it to their server. The issue is that the information itself (the phone number) is linked to you, a person. PII is not so much about usage, and more about the static information content of some data.
Sure,  there are degrees in this. For example, birth dates. If I take a sample of just a random birth date (without any attached information) from one person I meet on the busiest street of a city of 10 million inhabitants, then this birth date probably does not help me identify the person later. But if I do the same in a small classroom, it is highly likely that I can do so. (Birthday paradox nonwithstanding.)
In the case of the phone number, or a social identification number etc., the case is crystal clear - I can immediately identify the single person on the world related to that number (maybe from a phone book, maybe just by calling and getting their name if they don't watch out).
Potentially ominous scenarios for using such bare phonenumbers are plenty; e.g. profiling (by matching with other records that contain the number); discovering groups of people (by grabbing a set of numbers that are somehow grouped inside the application) ; etc.
",8,2018-07-03 19:52:10Z,513
41,5,"
Yes
A base comparison I've been using is with a client's name, does a piece of information allow me to pick out an individual better or worse than a name? You may have hundreds of 'John Smith's but you'll only ever have one individual per phone number (or, at most, a family) and, therefore, it makes it a more identifying piece of information than the person's name.
",1,2018-07-04 10:08:15Z,514
41,6,"
The majority of phone-numbers are linked to unique persons (e.g. their mobile, desk or home), making a phone-number a unique identifier number pointing to a single person. 
You could also argue that some phone-numbers are obviously not coupled to a person, but since the percentage of numbers that do largely outnumber the numbers that don't. (Since 2005 fixed phone-lines are on the decline and mobile-phone usage is on the rise, with currently 67% of the world population having a mobile-phone number.)
Phone numbers can clearly be used to re-identify a person in a later stage of data processing. If you enrich it with enough other data (e.g. subscriptions, website visits, geo-locations) you could possibly even identify the natural person. Some companies link phone-numbers to online advertisement-cookie-ids leading to very rich profiles. Online services (for example movie ticket buying service) are known sell this data as well.
Therefor I think in most cases a phone-number should be considered personal data. So always handling phone numbers as personal data seems the wise thing to do, unless you are 100% sure that you are only processing phone-numbers of public organizations for example.
",1,2018-07-04 10:01:41Z,515
42,1,"
The data being ""collected"" or ""requested"" is not the cookies, nor is it likely to be stored directly in the cookies.
The data your privacy policy needs to talk about is data you have requested explicitly, like name and e-mail address; and data you have collected because browsers send it by default, like referrers, IP addresses, and user-agent identifiers. 
The role of cookies is to tie together this information across multiple requests - to know that the same user who told you their name was Bob is now accessing the home page; or that the same user who was connecting from China yesterday now appears to be connecting from Russia. But it is not the cookie that has taught you that their name is Bob, nor that their IP address is allocated to China.
Your privacy policy should first and foremost talk about the data you are collecting. If it must talk about cookies, it should talk about them as a technology used to ""connect"", ""tie together"", or ""associate your browsing with"" that data.
",5,2018-06-19 10:04:03Z,67
42,2,"
Cookies neither collect nor request data. Cookies are just boxes you can store information in the client side that may be later retrieved when the client enters the site after setting the cookie.
",17,2018-06-18 20:18:56Z,521
42,3,"
HTTP is a stateless protocol. From the server pespective, every connection is a new one. Does not matter if you never ever connected to that server, or it's the 100th image you download from it in the last second. Your connection and every single other connection are just the same, faceless anonymous unknown connection. To put an identifier on each one, the cookie was created.
Think of a cookie as being a badge. If you talk to a server without a badge, it will send you one: I don't know you, so your badge is ID:ABC123.
Every time your browser talk back to the server, it sends the badge together:
""I am ID:ABC123 and need logo.jpg""
If the server have anything to add, it will write a new badge and send to your browser:
""You logged in, and are a valuable user. You are ID:ABC123,TYPE:1""
When you ask the next thing, the badge goes back:
""I am ID:ABC123,TYPE:1 and need custom_logo2.bmp""
It's a passive variable. It does not collect anything, it is just arbitrary data the server uses to identify you. Log in to any service, delete the cookies, refresh the page and it will ask who are you. The cookie is what identifies your browser and session.
This is a simplistic example. Usually cookies are encrypted and don't really hold values, they usually only point to a record on the server side where the real values are stored. Otherwise anyone could put the TYPE:1 on their cookies and be a very special guest on the example service.
What is the privacy thing about the cookies? They can track you around. If I have a service hosting images, and you link a image from my server on your page, my server will receive a request from your client. The request will have a special field named Referer, and this tells me you are coming from, say, your-own-site.org, and I send him a cookie identifying him (ID:ABC999). Not only this, but I put on my database a record telling ABC999 acessed your-own-site.org. Later your client requests another image, but coming from slashdot.org, and the request gives me the ID ABC999. From my server I know the client accessed your site and slashdot, so I can start building a picture of what kind of sites he access, and what is his profile. Does not look like much, but if you think about Google, Facebook, and almost every Content Delivery Network, they track almost every single site you access. That's why almost every browser have a Block third party cookies option somewhere: this way the client will only store and send cookies for the domain he is accessing, not every CDN, image storage, telemetry or analytics site around the world.
",14,2018-06-18 22:01:24Z,219
42,4,"
A cookie is just a piece of text. It doesn't ""collect"" or ""request"" anything, it isn't an active component, and so it all depends on how you use it. 
It can be used to collect and store some data about the user, like language preferences for example. The cookie will contain something like lang=es, and that means the user should be shown the content in Spanish. Or it can be used to identify the user, so for example it could contain id=482477359937882940034 and that can be used to serve content specifically to one user (like their email, private messages, etc.) or track them by linking several different requests to the same ID.
The server usually sets and modifies the cookie, and send it to the browser. The browser then sends it back to the server with every request. It's basically data that is continually passed back and forth. 
So your privacy policy should say that ""your website uses cookies to..."", and then it depends what you use them for. I don't know what exactly you use them for, so I can't give you any specific advice, but I'm pretty sure that a sentence like ""We use cookies to request info..."" is very likely to make no sense at all. To be honest, nobody cares about cookies, and the cookie law (EU law, I'm assuming you are in the EU) was just a stupid law. The GDPR law (EU law) is a much more complete law, and focuses on personal data in general. What matters is what you do with those cookies, what data you collect, for what purpose, and if it's a legitimate purpose according to the GDPR. It's a pretty complex law, but I can only tell you that if you are trying to avoid asking for the user's explicit consent by assuming they can always tweak their browser settings, you are on the wrong track. 
",1,2018-06-19 10:50:18Z,450
42,5,"
Servers don't have to do anything. Client browsers send cookies to the servers together with the request willingly - because they expect it to be useful for the server to provide the client with some service based on the cookie they store, so to speak.
I am not sure what either of those two (collect vs request) in this context mean but the server is in a passive relationship with client browser here. You can as well disable storage (and sending) of cookies in your browser if that's what you wish.
",0,2018-06-18 21:41:54Z,522
42,6,"
The cookie STORES data, the server will collect/store data within it, and 'request' data back out of it later.
",-1,2018-06-19 08:06:37Z,524
43,1,"
That article is wrong and that website in general seems like a very unreliable source for anything.
With the netstat tool, among other stuff, you can see established TCP connections.
When you use Facebook messenger (or any other chat), at least one server is between you and the person on the other side, it's not a peer-to-peer connection. Hence the IP you see using netstat is an IP of a chat server (or whatever other obscure infrastructure, not know to us simple users) that you established connection with.
",90,2017-10-19 14:23:44Z,527
43,2,"
In addition to @skooog's answer which states that the IP address that you are detecting is not that of the user:
IP address geolocation is, at least for IPv4, doomed to fail except in specific edge-cases. Many ISPs dynamically allocate IPv4 addresses, meaning that increased precision gained from a user who enables GPS location on their browser or posts geo-tagged images to (e.g.) Facebook is lost once the IP address is re-allocated. This is also complicated by IP-sharing, which happens when addresses are scarce.
However, a side-effect of most ISPs' dynamic allocation systems is that they split both the world and their subnet into regions, which they then assign in a mostly one-to-one relationship. This allows approximations of the user from their IP address, but unlike telephone numbers it is not possible to get a precise location from this.
This is why not even Google can accurately identify the specific location of a user before they have signed into their Google account. The country and region can be attained, but a more precise location requires other techniques (e.g. GPS, SSID-sniffing) which aren't passive and require collaboration with the user's device.
So: even if you did have the user's IP address, it would not tell you ""the exact location of the person"" - it would tell you their ISP and the centre of their IP range.
",17,2017-10-19 16:43:48Z,19
43,3,"
That article isn't just making unjustified assumptions about the nature of the chat protocol and the accuracy of IP address geolocation. There's also this helpful tip about netstat:

Here, you will have to look out for your friend’s IP address among the hundred other IPs, well, that is simple: The last one in the foreign address column is your friend’s IP since it was the last data packet came in into your web browser.

netstat doesn't order its output that way. And if you did have a way of sorting it to find the most recently active socket... you'd still be assuming that nothing else is active in the background while you're doing this.
It's baloney all the way down.
",4,2017-10-20 18:40:51Z,528
43,4,"
No, even Facebook is one of the worst privacy invasion services, it doesn't make the blunder by making Peer to Peer chat: Facebook record the chat contents as well, there is no chance that they will let the opportunities slip away from their hand then giving you a P2P private chat services. Those IP gather definitely belongs to facebook.  Using a linux ""dig"" upon those IP will reveal who is the netblock owner/ISP.
The site is probably a content farm that publishes fake contents, some sort of Blackhat SEO mechanism to boost its website traffic and display tons of junks ads to earn some money.
",2,2017-10-19 15:03:00Z,223
43,5,"

So this is just fake right?

Most certainly.


Specifically, by looking at the IP addresses that appear in the ""Foreign Address"" column of Netstat. To find the geo-location one has to copy the last added IP address from the list into a IP address lookup tool like http://whatismyipaddress.com/. Then copy the coordinates and use google maps to find the exact location of the person.

The IP address you see is the IP address of most probably some Facebook CDN. The coordinates, if they're correct or ever were correct, are/were likely that of a datacentre.
As IP addresses are virtual resources (not tied to any physical location) which might be transferred from location to location, the information you obtain via this process comes from a database which could be out-of-date or inaccurate about the state or even country, and is almost certainly inaccurate about the city that the IP address is being used by.
To confirm, I found ip2c.info, which is another such database you can query (this one supporting IPV6) and they explain this on the front page:

Well it appears IP to country is as good as it gets, most of the time RIR don't even have the right information in their databases, so anyone claiming that they do provide IP to city is probably giving you false information half of the time. My IP location is registered to my ISP and if I perform an IP to city look up on it, I receive the location of the ISP, which is on the other side of the country.

... so whatever you mean by exact location, you and I will likely disagree about. I guess you could say ""he's located on precisely the same planet as that Facebook CDN datacentre""...
",2,2017-10-19 19:38:16Z,0
44,1,"
Loading that page loads
https://www.googleadservices.com/pagead/conversion.js
https://www.googletagmanager.com/gtm.js?id=GTM-WPPRGM
https://stats.g.doubleclick.net/dc.js

The reason Google can track you is that the website shares details of your visit with them - in this case via loading Google JavaScript code for their ads service.
*To expand on this -
The Google ad code will use a cookie to track you. But even if it didn't there are browser fingerprinting mechanisms which in most cases can correctly identify a user's machine even after a full browser cache / history clear.
When you visit a site with ads a request is made to the ad providers server. This sends the ID associated with you to say ""an ad on [x website] for [user y] is available. The ad providers nowadays often then real-time auction off the slot in 1/100th of a second - where potential advertisers computers can bid for the advert space.
The site you visited is djoser. Since djoser knows you looked at products on their site yesterday they know there is a reasonable chance you are considering buying something from them. So when you visit another site somewhere else, the ad slot on that other site is more valuable to djoser, and they bid higher than anyone else - hence why you keep seeing them.
",193,2017-10-11 11:28:13Z,531
44,2,"
While Hector's answer correctly explains how Google got to know the page you visited, the real answer to the OP:

How did Google know I looked something up?

Is 
They Don't
In fact, no one is telling Google what your search was: only the destination page is sharing information about your visit, which does not include your search query (because DuckDuckGo hides that).
Of course, Google's advertising services still target you with content that is relevant to the website you visited (to avoid that, follow Hector's good advices), but they don't know what you looked up on DuckDuckGo. 
Also, please know that Google do get informed of the fact that DuckDuckGo sent you there (while, sorry for the repetition, they don't get to know what you searched for) 
",17,2017-10-12 07:55:39Z,532
44,3,"
It is most likely due to the website using Google tools.
Many website uses Google analytics or advertisement service, with whom they can get information on who's visiting their website etc. 
Hence, Google is informed of your visit of this website.
",15,2017-10-11 11:28:52Z,536
44,4,"
HOW GOOGLE KNOWS?
@Hector already mentioned the reasons how it is showing related pages of your searched for last time. I would like add few more points. Well this is part of targeted marketing and their are many companies which are adding these google tags or other DMP tags like AdeX, KRUX, Salesforce, etc to the websites. So not only google is targeting you but also all these trackers from other DSPs(Demand Side Platform) or DMPs(Data Management Platform).
HOW TO BLOCK THESE TRACKERS?
If you want to check on a website what trackers are present and if you want to manage them then you can use chrome extension called Ghostery or alternate softwares like Ghostery.
You can block a specific tracker if you don't want to share your information with them. This is going to greatly reduce the advertisements related to your search.

",6,2017-10-13 10:17:27Z,537
44,5,"
This sounds like a remarketing cookie to me.  Google, Facebook, and many other sites use ""retargeting"" (which Google calls remarketing) which gives advertisers the ability to target you specifically, on other unrelated sites that also serve ads in the same network.
So you visit site A about travel, and you read a few pages.  Even if that site displays not a single ad, they can hit you with a retargeting/remarketing cookie.  Now you go to site B, which is utterly unrelated to travel - say it's a TV show site - and they advertise using Google's Adsense program.  If Google's algorithms determine that it is likely to be more profitable for them to show you ads for the travel site you visited previously, than to show you other ads for other things, then they will show you the travel ads.
",2,2017-10-12 19:57:06Z,540
44,6,"
Another possibility:  https://superuser.com/questions/1250944/how-can-this-website-reidentify-me-even-after-deleting-all-of-my-browsers-histo
There is a little known area where sites can store informations in Firefox: the IndexedDB
To paraphrase the top answer from @arjan : They can store information inside IndexedDB (which is NOT cleared when you clear cookies, history, etc). To clear it you need to do something specific: either go to: about:permissions, or if it does not work (ex: Firefox 55), going into Tools, Page Info, Permissions to get the button ""Clear Storage""
",-1,2017-10-12 13:18:08Z,541
44,7,"
If your travel site sets a third-party cookie on behalf of an ad network the ad network can follow you around the Internet with knowledge of your previous behavior. Disabling third-party cookies can protect your privacy and prevent ad networks from collecting broad information about your behavior. With third-party cookies blocked, sites cannot set or read cookies on behalf of others and only the site that sets the cookie will have access to it.
You can this in Firefox under: Preferences - > Privacy and Security -> History -> Use custom settings for history -> Accept third-party cookies -> Never
Anyone who cares about online privacy should make this change in all their browsers on all their devices.
",-1,2017-10-12 18:57:45Z,542
45,1,"
Paid subscribes to virustotal can download files uploaded by others. If you consider this still safe for your users depends on what you consider safe. 
See also their Privacy Policy which clearly says:

Information we share
  When you submit a file to VirusTotal for scanning, we may store it and
  share it with the anti-malware and security industry ... Files, URLs, comments and any other content submitted to or shared within VirusTotal may also be included in premium private services offered by VirusTotal  to the anti malware and ICT security industry

Still, I think that your idea of offering a simpler access to a useful service directly from the mail client makes sense. I would though recommend that you add an easy to understand but not easy to ignore warning about the privacy implications before the user uploads a file. And it might be less invasive to first check if the hash already exists at VT before uploading a file (and not upload if hash is known to VT).
Ideally you also make it easy for users to remove an accidentally shared file (thanks to @Mirsad for this suggestion in a comment).
",71,2017-08-18 05:08:28Z,66
45,2,"
I wouldn't recommend uploading files containing any sensitive information. Passwords, personal notes or other forms of data that can identify you as a person or expose your privacy. As Steffen mentioned in his answer, the files can be downloaded by premium users, meaning that the files and its contents will be available to other individuals. Usually, reading the privacy policy of the website helps you grasp the general concept of what they are going to do with the data.
",18,2017-08-18 07:58:18Z,22
45,3,"
Yes, the files do get exposed to people outside of VT administrators. 
Virustotal Premium allows downloading files and ""hunting"" - which involves writing YARA rules to match the files from everything that has been uploaded to VT (e.g. I can search for files that have a string ""private"", get alerted every time such file is uploaded to VT and download them myself). Having the Premium service is very common for security teams and companies.
Also as already mentioned, the information is shared with other communities. So if there's a risk that private documents could be uploaded, I wouldn't implement this feature.
",15,2017-08-18 08:11:15Z,527
45,4,"
First, read this article thoroughly: Security Firm Accused of Exposing Terabytes of Customer Data, it tells you why you should not make the same mistake.
The rule of thumb of using VirusTotal to protect own file privacy is to send a sha256 hash to the database. Also, one should subscribe to prominent antivirus ""business edition"" that will scan the file using localized virus scanning engine. 
",7,2017-08-18 10:05:51Z,223
45,5,"

""how safe is it to upload personal files, could they got exposed to someone beside owner of VT?"".

Everyone on the route can see it. If it's not encrypted they can read it. If it is encrypted they might be able to decrypt it. If VT has a rogue employee or they get hacked then your Data can be exposed.
Never let private information go out if you want to keep it to yourself. Similarly don't put anything on your Phone, take it somewhere expecting not to lose it, and then lose it and all your Passwords, Photos, Banking Info, etc.
Same for anywhere else, not VT specifically.
",2,2017-08-18 17:12:16Z,233
46,1,"
Whether this is visible to the outside world depends on what kind of firewall your university network uses.
You could try using the tools at www.whatismyip.com to see some of what's being exposed to the public Internet. The basic info at the home page will show you what public IP you have; you can compare this with the configuration of your PC to see if they're the same (the PC may have a private IP, and the public IP is is the address of the university's firewall).
If your internal IP is being exposed, the university may still be using split DNS to provide different name resolution internally and publicly. Use the Reverse DNS Lookup tool to see what name is being exposed to the public. I expect it will be a generic name derived automatically from the IP, not the one with your name in it.
Also, it's possible that the university uses dynamic address assignment that doesn't associate a specific IP with your PC permanently. Each time you connect to the network you may get a different IP. So if some outside site has your IP saved, and tries to look up the name later, they may get someone else's name because the assignment has changed. The ""Last login"" message you see in the login greeting on your server isn't affected by this because it saves the name when you login, not the IP.
",20,2017-08-16 20:06:25Z,396
46,2,"
What's your IP address?  If it's an RFC 1918 address like 10.*, then you're being NATted to the Internet, and as a comorbid condition* your reverse DNS won't be visible to external sites.  It will still allow the University to track you better internally, which is perhaps why they do it.
If it's not an RFC 1918 address, your DNS name likely does show up externally; you can check with a site like WhatIsMyIP (Reverse DNS Lookup).

*Having an RFC 1918 address implies that your reverse DNS won't be publicized on the Internet, because an RFC 1918 address must be translated (NAT, or Network Address Translation) before going out to the Internet.  In almost all cases, NAT is one-to-many (one public address, many private addresses) or pool-to-many (some public addresses, many private addresses) so there is no one-to-one mapping between the internal RFC 1918 address and the external address the Internet sees traffic from.  If you have hundreds or thousands of students being NATted to one or more public IP addresses, having that smaller number of public IP addresses be mapped to hundreds or thousands of names becomes meaningless.  All of which I glibly summarized as ""NAT means no external reverse DNS"" above.
",24,2017-08-16 15:17:55Z,35
46,3,"
Your hostname may be included in the Received headers added to any emails sent from that host (and not passing through NAT to the first MTA to handle them).  You can find out if that's currently happening by sending a test email to an 'echo' reflector, but that doesn't tell you what server changes may affect that in the future.
",2,2017-08-17 12:36:12Z,53
46,4,"
No it is not really a security or privacy issue. It just seems to be an internal DNS name assigned to you on your university network. Networks usually have split DNS i.e. internal and external. If your network is properly setup with NAT, internal host names do not resolve from outside. 
",-1,2017-08-16 21:34:00Z,551
46,5,"
If you really want your name to remain private, you'll need a lot more than hiding the DNS entry of your computer. Most universities will give you an e-mails formed from your first / last name, and many also have a public directory of their staff. Make a request to have a non-default e-mail address and be excluded from any directories and address books. For the latter, you're usually given an agreement to sign when you start working, and you usually can opt-out.
Needless to say, if you're planning to participate in research, make sure you pick a pseudonym for your publications, otherwise your name will become irrevocably public and tied to the university. And even if you use a pseudonym, that won't completely hide your identity, since you will still need to use your real name in many places which may be publicly visible, like conference participant lists or grant applications.
",-1,2017-08-18 08:00:27Z,499
47,1,"
No, Docker containers are not more secure than a VM.
Quoting Daniel Shapira:

In 2017 alone, 434 linux kernel exploits where found, and as you have seen in this post, kernel exploits can be devastating for containerized environments. This is because containers share the same kernel as the host, thus trusting the built-in protection mechanisms alone isn’t sufficient.

1. Kernel exploits from a container
If someone exploits a kernel bug inside a container, it exploited it on the host OS. If this exploit allows for code execution, it will be executed on the host OS, not inside the container.
If this exploit allows for arbitrary memory access, the attacker can change or read any data for any other container.
On a VM, the process is longer: the attacker would have to exploit both the VM kernel, the hypervisor, and the host kernel (and this may not be the same as the VM kernel).
2. Resource starvation
As all the containers share the same kernel, and the same resources, if the access to some resource is not constrained, one container can use it all up and starve the host OS and the other containers.
On a VM, the resources are defined by the hypervisor, so no VM  can deny the host OS from any resource, as the hypervisor itself can be configured to make restricted use of resources.
3. Container breakout
If any user inside a container is able to escape the container using some exploit or misconfiguration, it will have access to all containers running on the host. That happens because the same user running the docker engine is the user running the containers. If any exploit executes code on the host, it will execute under the privileges of the docker engine, so it can access any container.
4. Data separation
On a docker container, there's some resources that are not namespaced:

SELinux
Cgroups
file systems under /sys, /proc/sys,
/proc/sysrq-trigger, /proc/irq, /proc/bus
/dev/mem, /dev/sd* file system 
Kernel Modules

If any attacker can exploit any of those elements, it will own the host OS.
A VM OS will not have direct access to any of those elements. It will talk to the hypervisor, and the hypervisor will make the appropriate system calls to the host OS. It will filter out invalid calls, adding a layer of security.
5. Raw Sockets
The default docker Unix socket (/var/run/docker.sock) can be mounted by any container if not properly secured. If some container mounts this socket, it can shutdown, start or create new images.

If properly configured and secured, you can achieve a high level of security with a docker container, but it will be less than a properly configured VM. No matter how much hardening tools are employed, a VM will always be more secure. Bare metal isolation is even more secure than a VM. Some bare metal implementations (IBM PR/SM for example) can guarantee that the partitions are as separated as if they were on separate hardware. As far as I know, there's no way to escape a PR/SM virtualization.
",420,2017-09-18 02:13:48Z,219
47,2,"
Saying either a VM or Docker is more secure than the other is a massive over simplification. 
VM provides hardware virtualization; the hypervisor emulates hardware so that the guest kernel thinks it is running on its own machine. This type of virtualization is easier to isolate from one another. If your primary concern for virtualization is isolation (you don't really need the virtual machines to interact with each other), then VM is going to be significantly simpler to secure.
Docker provides kernel/operating system virtualization, and Docker uses the kernel namespaces to virtualize the kernel and the operating system so that the guest thinks it is running on its own instance of the operating system. Operating system virtualization provides significantly more flexibility on how you can secure the interconnection between your containers. If your primary concern for virtualization requires you to interconnect containers, then Docker provides the ability to define these sharing rules in ways that are impossible or too cumbersome with virtual machines.
With great flexibility comes great risks; it's easier to misconfigure docker than VM, and the flexibility of docker resource sharing also creates more opportunities for both implementation and configuration bugs. However, if you manage to configure the sharing permissions properly and assuming there are no implementation bugs in Docker or the kernel, then Docker provides much more fine-grained sharing than hardware virtualization and may give you overall better security.
For example, socket sharing. With Docker, you can create a shared named socket between two containers easily by sharing the socket file, and you can define security permissions (using any existing security modules: traditional Unix permission, capabilities, SELinux, AppArmor, seccomp, etc.) between the socket endpoints so that the docker/kernel enforces which and how applications can access the socket endpoints. With a virtual machine, you can share a socket a bit more cumbersome by setting up a chain of sockets to pipe data going to the socket via TCP. However, the hypervisor has very limited visibility and no way to control access to the socket endpoints, because permissions to these socket end points are applied by the guest kernels.
Another example is folder sharing. With containers, you can share a folder by setting up a shared mount, and since the Docker/kernel enforces file permissions that is used by containers, the guest system can't bypass that restrictions. With a VM, if you want to share folder you have to let one machine run a network file server or Samba server or FTP server, and the hypervisor have little visibility into the share and can't enforce sharing permissions. The additional moving parts here (the file server), may also have its own vulnerabilities and misconfiguration issues to consider.
TL;DR: Use VM for isolation and containers for controlled sharing.
",72,2017-09-18 01:39:35Z,310
47,3,"
As you correctly stated, Docker uses ""Operating-system-level virtualization"". You can think of this (if you are a *nix fan) as a fancy form of chroot.
By harnessing features and functionality built into the OS, Docker acts as a director for containers. The software's view of the OS is dictated by Docker.
The same kernel is used across all containers. So for instance, if I was able to cause a kernel panic in one container (think ""Blue Screen of Death""), all other containers are affected.
Configuration seems to be much more critical than with hardware based solutions. Everything is in the same shared space effectively. Imagine putting a wild predator beside it's natural food source. If you didn't put a strong enough enclosure surrounding the predator, or forgot to shut the gate every time you left it's enclosure, you can likely imagine what would happen.
While certainly a lightweight solution, I certainly wouldn't run any unknown code along side that of trusted code.
Malicious code would have to determine a way to escalate it's privileges to a root/Administrator level in order to escape the container in any meaningful way.
In a virtual machine, the hypervisor would be attacked, not the kernel. This may prove more secure as there is a higher level of isolation between ""containers"" but introduces higher overhead for management.
From my understanding, there's nothing that makes Docker more secure than ""bare metal"" or hardware based solutions. I would be inclined to say that Docker is less secure. In terms of 1 container per a piece of software, that can prove a different story.
If you are unsure of real world examples, I would take a look at OpenVZ. It uses OS level virtualization in a similar style just as Docker does, but with a modified kernel.
",23,2017-09-18 00:49:03Z,560
47,4,"
Docker Containers are Not Inherently “More Secure” But the Ability to Quickly Spin Up—and Destroy—Duplicates in a Cluster Is Very Useful from a Security Standpoint.
Okay, lots of other answers here but people tend to forget that sometimes the best tool one can use to secure a web server/application is the ability to quickly redeploy clean code after already installed code has been compromised.
Nothing in the world 100% safe or secure. Especially in the world of exposed web applications. But Docker allows for better practices if people actually understand their value and engage in those practices.

Do regular backups of assets and databases.
Always have a solid, portable configuration.
Manage code in a code repository.
Use a deployment process that allows for the redeployment of code in a few keystrokes.
And use a system config tool to ensure systems/servers can be quickly be recreated with minimal effort.
If possible, deploy your code in some kind of load-balanced cluster so if one “thing” running code gets compromised, you can kill it off without completely bringing down the app.

Docker fits into the last point. So do VMs, but Docker even mores because once you have made the decision to use Docker, you are inherently using a tool whose mindset/mentality is: “I will not last forever. I need to be recreated.”
And in the cases of an infected Docker container, you can take it offline—as far as the outside world is concerned—to do some forensics to see what happened and see what can be done to redeploy code safely to other existing and new codebase installs inside of Docker containers.
VMs might be able to be used in such a way, but in my practice and experience only developers really think that way about VMs. Most systems administrators—and the teams they are a part of—see VMs as a way to more easily squeeze out more use and utility out of a bare metal server rather than see them as quickly disposable machines that can be recreated at whim.
With Docker you really have to go out of your way to make a Docker container a non-disposable monolith. Docker is an application developer’s tool built for an era where virtualization is quick, cheap and easy. And when deployed in some sort of load-balanced cluster, you get the added stability of little to no downtime.
",11,2017-09-22 13:55:47Z,283
47,5,"
I agree with ThoriumBR's answer if we're just comparing a blank VM with a blank Docker container. It should be noted, however, that properly configuring your system such as in Red Hat's Atomic Host mitigates many of those factors and even eliminates some.
Also, since Docker started, you can count on one hand the number of the sorts of vulnerabilities mentioned in his answer, all of which could be mitigated by further layers such as SELinux. We are also starting to see hypervisor-based OCI-compatible runtimes, that you can use instead of runc if you're really paranoid and willing to take a performance hit.
I will additionally point out that the vast majority of vulnerabilities in software are not in the kernel/driver space where VMs have the security advantage, but in the application layer, where Docker containers have the advantage, because they make it easier to create single-process attack surfaces. You can build a usable Docker container with a single statically-linked executable that runs as a non-root user, with limited resources and capabilities. You can't make a VM with an attack surface that small.
The bottom line is you have to look at the entire security picture. Docker containers can be very secure, but you have to look at the entire software supply chain, and make sure you configure docker and the host properly.  VMs have their own set of strengths and weaknesses. You can't just compare the two ""out of the box"" and make a decision. You need a process in place to harden either solution.
",10,2017-09-19 13:13:58Z,563
47,6,"
In the application layer, yes
The question is too broad to be answered by a simple ""yes"" or ""no"".
There are very clear and open attack surfaces for Docker containers: 

If the attacker is the one who can start containers (i.e., has access to the Docker API), then he immediately, without further action, has full root access to the host. This is well known for years, has been proven, and is not under debate by anyone (Google or SE will immediately give you simple command lines which not even neet a particular container to work).
If the attacker manages to get root inside the container, then you're in trouble. In effect, he can then do pretty arbitrary kernel calls and try to affect the host kernel. Unfortunately, many docker images seem to run their stuff as root and skip the USER in the Dockerfile - this is not a Docker problem but a user problem.

I see one scenario which indeed would make a system based on Docker images safer (possibly...):

If you religiously make your images as small as possible, and only one per concern.
And all of them run unprivileged (i.e., without --privileged and not as root).
And networking is as tight as possible.

Then this would likely be more secure than a VM solution with the following parameters:

Installed base as small as possible.
Many concerns installed in the same VM.
Networking as tight as possible.

The reason being that if, e.g., a HTTP server is broken into, and it is running inside a minimal container (and I do mean ""minimal"" here - i.e., alpine with nothing except the bare minimum, including no outbound networking, no rw volumes etc.), then there is less chance that the attacker would be able to do anything than if it was a VM with other services running in it. 
Obviously, this scenario assumes that the VMs are actually more fat than the containers. If you make the VMs the same as the containers, then it's a moot point. But then, a well designed Docker scenario would be designed like that. And a usual VM setup would maybe, at least over time, migrate to more and more stuff being installed.
",6,2017-09-21 11:47:34Z,513
47,7,"
There are already some great answers, but to have a full illustration of the difference between docker and VM I will add a picture:

Source
From that perspective it is easier to understand why is VM more secure than Docker container.
",2,2017-09-29 04:32:54Z,544
47,8,"
The main selling point of docker is not to be more secure, but to be easier. This includes:

Useful defaults, including some security options.
No work with configuring LXC, cgroups and so on yourself.
Ready made images which can be downloaded with one line.
Reproducible VMs. No more ""works on my machine"" arguments.

Docker is as secure as the techniques it is using, which are mostly LXC (linux namespaces), selinux and apparmor.
The usage of docker is often horribly insecure. People are using one line to download an image made by somebody, they never even read the name of before running his operation system container. Even when you build the image yourself from a own baseimage (easy to create with debootstrap like when you're building a chroot) and a Dockerfile, it often includes the curl $URL|bash anti-pattern to install software in the container.
The other thing is, that ""the docker way"" is not to upgrade images, but to rebuild them. This means stopping (while people often assume you have a failover running with the new image), rebuilding and starting again.
This comes from the way the snapshots are created, where a usual apt-get dist-upgrade introduces layers which are semantically noise from the docker point of view, where the history should look like ""baseimage, added apache, added php, installed roundcube"" without ""daily apt-get upgrade"" between the steps.
If you're maintaining an own image-repo in your LAN, docker can be very useful, as you can redeploy updated images fastly.
The snapshot feature is another thing which can be a good security improvement, when you can quickly roll back or fork an image to try something new in a test environment.
The bottom line is, that docker is a very useful tool for developers, which want to test the deployment of their code reproducible and without changing their installed operating system, but there are better solutions for productions. So you can run docker in production securely, but it does not make things more secure than a good LXC setup without docker.
As far as I know they are not restricted to LXC as their VM or will not be restricted soon, especially when they target windows as well. Choosing another backend has security implications, which are the same as i.e. LXC vs. KVM vs. VirtualBox. The other points will probably stay the same.
",1,2017-09-27 14:11:16Z,236
47,9,"
First I would like to point out that the more you re-use/share the same resources harder is to keep it safe.
Said that, Docker try to run each process on a sandbox (Container) and this is the only reason why docker may be considered more secure. From the multiple process your server runs, you will theoretically have access to your own process and expose a shared folder or a socket to the other process. Configuration files, credentials, secrets other debugging and maintenance ports/sockets will be unaccessible even from the same ""machine"".
Docker it feels more secure as the way has been designed to work: it sandbox each running process. Where Virtual Machines and Baremetal you sandbox a group of process and applications and between them it is your responsibility to setup permissions accordingly.
",0,2017-09-29 09:10:14Z,564
48,1,"
By using the same VM for browsing, word documents, and email, you are exposing all of your data to the same level of risk. 
Instead of doing all of this activity in the VM, consider doing your browsing and email in the VM, but the contract work and bookkeeping stuff on the host OS. That way if you get phished, the attack is limited to the VM, and can't do really nasty stuff like encrypt your docs and hold them for ransom.
",65,2017-06-21 04:53:48Z,48
48,2,"
In context of a Windows setup, a hypervisor such as VirtualBox, VMware helps isolate your guest from the host (the main installation of your OS).
This is a considerate move in terms of Security, if you're dealing with activities or files/software that pose a risk to compromise your data. One example would be for Security Analysts, who is analyzing malware on a daily basis.
So, if this is your business computer: It might make sense to encapsulate an installation inside a virtual machine if you ever would lend your computer for someone to use it, download/install unsigned software or torrents, etc. Breaking out of a virtual machine takes some very sophisticated attacks, and is a highly regarded exploit on the 0-day market.
It's the guest you want to isolate from the host and not vice versa.
There is some operating systems such as Qubes (https://www.qubes-os.org/), who does app virtualization - which means that every single application run in it's own isolated instance.
Judging by your description of daily use you are a fairly low risk user.

Only open files, e-mails from trusted entities.
Have unique passwords, and change then regularly. Monitor leaks from hacked sites you might have registered yourself on through haveibeenpwned.com
Always use the latest version of your web browser, Adobe PDF, Java and Flash if you need them at all.
Be conservative with browser extensions.
Don't install unsigned, outdated or pirated software.
Enable 2-factor authentication for your crucial accounts such as GMail, Facebook, etc.
Encrypt your backups, if having any (pull).
Be considerate with how you choose to split up your virtual machines.
Configure your Virtual Machines with none or limited access to folders etc, on your host machine.
There is a myriad of software for hardening in Windows such as EMET (The Enhanced Mitigation Experience Toolkit), Microsoft Security Essentials and Windows Defender.
If you run a Linux based installation in your virtual machime, I'd suggest you to install a supported release and enable unattended Security upgrades, reboot the server whenever there is a new kernel installed.

These steps will improve the average users information security significantly.
NOTE: Don't forget that the virtual machine, separation only mitigates certain threats and attack vectors. There is a lot more to take into consideration to have all the Security best-practices in place.
",18,2017-06-21 05:01:47Z,571
48,3,"
Usings VMs as a lone security measure is not enough. However, for a security conscious person who's already practicing other security hygiene, using VMs for additional security works well. 
Here is how I use VirtualBox for routine daily use. You should consider your use and tailor it for your purpose.

Separate VMs for separate purposes. e.g., Documents received from / going to external sources that I must download and use go into one VM. Online purchases and banking into a separate temporary VM. 
Keeping VMs updated with latest patches / updates is a chore. To keep it to a minimum I use VM templates - VMs that I don't use directly, other than to run updates. I clone these VMs when needed and use the clones as temporary VMs. Delete after use (typically 1hr to 1wk, never longer). 
As much as possible, leave attachments and documents online (e.g., use Chrome's in-browser PDF viewer / document viewer). This works when you're not worried about document leakage but do want to avoid malware infections from external documents.
Use uBlock Origin and uMatrix browser plugins even within the VMs to minimize contaminating the VMs through the browser route.
One VM where most ""semi-parmanently logged in social media accounts"" are maintained. Recycle this VM less frequently than others - about once a month. Be extra alert with this one because of extra risk.
Setup Shared Folders to make sure the necessary files are available in each VM, but keep the access to a minimum (only the specific directory that's needed). Over a period, this falls into a pattern and becomes manageable. At first it's a pain.
Absolutely no work at all on the host machine. The idea is to keep the host with the lowest risk of all. If that's gone, then everything is gone.

Hope this helps.
",5,2017-06-21 05:49:08Z,572
48,4,"
A short answer here : if you're doing everything inside a virtual machine then the risk is exactly the same as if you were doing everything outside of it. If your virtual machine get compromised, then you lost everything but your computer. 
As others stated, you should only use a VM for some stuff, especially for work requiring access to risky websites / ressources / the internet in general. In this way, you limit some risks on what you have on your computer outside of the virtual machine. 
However, keep in mind things like downloading a ressource in your virtual machine before running it on your own computer (like a PDF with potential malicious macro) are not 100% secure. Some malware were designed to evade sandboxing solutions, so that they only trigger when they are outside of a virtual machine / test environment. 
",3,2017-06-21 07:54:07Z,573
48,5,"

what do you want to secure yourself from? - schroeder

If your answer to this question would be something similar to:

I want to protect my business from people that may be determined enough to cause damage or steal my contractual and research documents that I keep in my computer.

Then using qubes, as mentioned by William in a previous answer, virtualizes the apps that are used to access those same documents, thus reducing most attack vectors that may be caused just by opening them.
Many people use Windows with the default Admin account and/or have UAC settings on a low level. If this is your case, there is perhaps a tiny possibility of one of your ""well intended"" and more determined ""clients"" to send you an unconspicuous word/excel document, produced by a ""friend who can do wonders with documents"", that when opened on your Windows 10 executes macros, causing a silent and ""lethal"" strike.
If your computer does not have the minimum requirements for qubes or you just feel it's too much trouble to go for it, then I would perhaps reverse John's suggestion, in the sense that you should only keep, access and modify your documents in an encrypted VM with a frequently updated linux OS (limiting the possibility of these attacks) while shredding the documents from your Windows OS.
Backup this VM image frequently and if you want to go a bit further then use another VM just for accessing your mail and work websites (as SaS3 suggested) and transfer your documents through a shared flash drive or folder whose files you shred after your daily work.
",0,2017-06-22 20:34:46Z,574
48,6,"
If you have a business were the uptime is critical I recommend an hypervisor such as ESXi or XenServer where you can can have separate VMs for different purposes and manage networks inside of the hypervisor. For the VMs inside the hypervisor to communicate with the outside world (rest of the Internet) I recommend a Firewall, for example PFsense, that works as a Firewall/Router for the internal networks you may have in the hypervisor.
If you want to go really big and provide maximum uptime as possible, I suggest 2 physical machines with a bridge connection between them, have a hypervisor on each computer (from the same company for compatibilities purposes) and with a live migration system in place in case something happens, for example not enough memory or a computer has lost power but the other is alive. What the live migration does is the hypervisor migrates (transfers) the VM still on to the other computer and keeps it on at all times. It's really cool.
",-1,2017-06-21 22:18:32Z,576
49,1,"
First and foremost I would say that if you don't know how the malware works, running it in a VM is ""a very bad idea™"". The people who do this professionally are experienced malware analysts and reverse engineers who have quite a lot of knowledge and capability with the various virtualization and segmentation concepts that are required to actively run malicious code without letting it escape the sandbox.
There is a plethora of information and analysis available online.

https://www.us-cert.gov/ncas/alerts/TA17-132A
https://www.trustwave.com/Resources/SpiderLabs-Blog/WannaCry--We-Want-to-Cry/
https://securingtomorrow.mcafee.com/mcafee-labs/analysis-wannacry-ransomware/

These resources should give you a basic understanding of how the malware appears to execute and spread.
If you were to build a VM and manage to introduce (infect) the malware into the guest system, it is entirely possible your host system could be compromised if you do not protect against the attack vectors and distribution mechanisms.
",5,2017-05-16 19:00:09Z,577
49,2,"
These are malicious weapons, by definition you cannot trust them to react in any way.
Once a malware becomes quite old and its code has been thoroughly studied in every direction and all lead to very well defined scope and behavior, then yes it may be safe.
But appart from that, either because the malware is too recent and not very well known, or because it is too advanced and complex to be fully reverse engineered, then you should prepare yourself (and your environment) to any possibility.
",1,2017-05-16 19:00:17Z,579
49,3,"
MEMZ:
If you run a 'regular' VM, then no, but if you run a VM sharing the host computer's files, then yes.
WannaCry:
Also safe as long as you use a 'regular' VM. You don't need to worry about the internet connection anymore as no OS has the security issue allowing it to control your system any longer! 
",1,2018-01-16 15:38:40Z,580
49,4,"
No, but if you have internet connection enabled, it could spread. I'm talking about WannaCrypt. Make sure you turn off internet connection otherwise it could spread.
",1,2018-06-17 14:11:24Z,581
49,5,"
It is safe to run MEMZ on a Virtual Machine , but if you we're to run WannaCry please just please for the sake of the internet, disconnect the network card from your VM so it does not get to another pc and then another, etc. 
Except that yeah its safe!
",-1,2017-05-22 01:15:38Z,583
50,1,"
Separate development and production
It is usual practice to give developers local admin / root rights on their workstation. However, developers should only have access to development environments and never have access to live data. Sys-admins - who do have access to production - should have much more controlled workstations. Ideally, sys-admin workstations should have no Internet access, although that is rare in practice.
A variation I have seen is that developers have a locked-down corporate build, but can do whatever they want within virtual machines. However, this can be annoying for developers, as VMs have reduced performance, and the security benefits are not that great. So it's more common to see developers simply having full access to their own workstation.
",88,2016-08-30 16:06:15Z,585
50,2,"
So this answer is from the point of a developer. Keep that in mind. 
First, not having ""local admin"" rights on my own machine is a sign that I should look for a job elsewhere. It's nearly impossible to write code, fiddle with stuff, and maintain a toolchain if you have to ask permission every-time you need to update (or test out) a new dependency or tool. So, here are the permission levels I require. Keep  in mind I am usually pretty high up on the ladder so to speak. 

Total and complete Admin over my local machine
Total and complete Admin over all development and testing hardware
Some level of admin access to the production servers (this gets tricky I don't need or want everything, but I need enough to diagnose and fix problems that occur on production, and enough to actually deploy code (assuming that I'm the one that has to oversee code deployment). Usually this level of access evolves over time, but starts with log files.

Less then that, and you can go find a new developer. 
That said, there is a lot of risk involved with that level of access. So what I normally recommend is a separate network.  Put all the dev ""stuff"" in its own network. Its own AD, its own file hosting, its own everything, and never let it talk to the production network. (But do let it get out to the internet)
Yes this means duplicate hardware (or VPSs) but you need that anyway for testing. Yes it means a little bit more overhead when upgrading or administrating, but again, it's needed for testing. You also need a place to see ""What happens to X software if I upgrade the AD server?"" Look at that you have an entire network of test machines ready for that kind of test. 
What I have successfully implemented (with the help of a good IT team) is a separate VLAN for all dev ""stuff"" and a single VPS host that dev has full access to, to do with what ever it wants. On that host is an AD server that is setup by IT to look like a smaller version of the companies AD server. Then a set of documents and guidelines for what a, for example, webserver should run. What a DNS server should run, what a xyz server should run. Then, part of ""development"" is to install and configure those VPSs for our use. Everything on that VLAN is totally isolated from production, and considered external to the company. Finally a set of ""punch throughs"" are created for assets that we did need access to (like email). Normally this was handled as if we were external, and the list of these tools were very small.
",40,2016-08-31 15:32:15Z,589
50,3,"
From a developer's point of view:
Your job is to prevent change (known bugs and vulnerabilities are better than unknown, right?), but mine is to change things.  This puts us at an impasse.  My job is to create/change things.  If your policy prevents that, then, like any other obstacle, a part of my job is finding a way around that.
Which do you think is more of a danger, a developer that you've granted access to the things he needs to do his job, or one who has obtained that access by learning how to circumvent all of your defensive measures?  And why is your company paying you and your developers to fight this war against each other when you should be working together?
The simple answer is, give developers access to what they need to do their jobs.  And talk with them.  There may be rare conditions (clean room reverse-engineering with major legal consequences, or handling top-secret classified government data) where you need a more complicated policy.  But for the most part, it's not that big of a deal.  If you start a war and make your developers enemies, they will either leave or become much more dangerous than if you work with them.
Sensible measures include not allowing production database dumps on dev laptops (only testing databases with bogus data).  That way, if the laptop gets stolen, no confidential data lost.  But if the dev needs to debug things, they still need access to copies of the production database somewhere in a controlled environment.
Restricting internet access is ridiculous.  You might as well require all of your developers to write their code on paper with a feather quill and ink.
Talk to your developers, find a way to give them what they need to do their jobs while maintaining the security that you need to keep the important data secure.  The details will depend on your company and what data you're dealing with.  But it isn't likely to need draconian measures.
",17,2016-09-01 06:07:22Z,132
50,4,"
A security engineer doesn't maintain computers, that's what the service desk does. In your case you will require him to install three tools:

a hypervisor
docker
database software

From there he can add and remove machines for development as much as he wants (shouldn't require a sec engineer to intervene). With regard to your ""rogue container"". In general you don't deploy containers to another server, you deploy docker files which pull code from a code repository or download a signed binary of compiled code (which is even safer). 
In terms of rogue I can only imagine an attacker gaining access and adding more code. This is why you need to have security embedded at every step of the SDLC to ensure that all code is at least reviewed or gone through by another dev before pushing it up the tree. Furthermore you can also integrate code scanners to automatically scan for suspicious or vulnerable code stubs. 
On your third point it actually depends. Companies like Riot Games are doing exactly that. They found out that limiting intelligent individuals will lead those individuals to circumventing controls. So they decided to use simple rules and effective awareness training to make sure that they keep security in the back of their mind and gave full administrative privileges. They handed out little cards which stated what they should take care of and be careful of. 
",15,2016-08-30 15:31:01Z,116
50,5,"
We give our developers admin-access on their computers and let them know what the rules are. Most run Linux but we have a few devs on Windows as well. They all know to store their documents, designs, etc. on our fileshares (that have more specific permissions) and push their source code to our Git server.
They also know that I won't spend much time to fix a problem with their OS. If their computer malfunctions we will often just wipe it and reinstall the OS. The common applications and settings are automatically installed via Puppet and they'll have to do the rest themselves. Most of them have a Git-repo with dotfiles (the settings and preferences for their environment).
The time that they lose in such a case is enough motivation for most of them. They like to get work done, instead of fiddle with fixing their OS. If they lose important work because it was stored only locally they'll be frowned upon by their colleagues and boss.
We don't block any website or applications (except for a DNS-based anti-malware filter) but have some company policy rules about things like illegal software. We rely on peer pressure for most things. People who spend their time on Facebook aren't productive and don't last long. Much of our policies are based on an honor system and that appears to work well.
",8,2016-09-01 21:21:52Z,594
50,6,"
This fundamentally depends on context. Consider the following two setups, both of which I've encountered in the wild:

Developers work on machines that they either own, or have complete access to, even including installing their own operating systems. There are no restrictions on how they write the application. They have SSH access to production systems whenever they're connected to the network, including VPN.
All development is done in an air-gapped development lab, that developers are not allowed to bring electronic devices into. All computers have all permitted software preinstalled. Deployable artifacts are securely delivered on physical media to another team, who are responsible for deployment.

Both of these setups were appropriate in context - taking into account the threats the organisation faced, and the needs of the project.
There is a fundamental tradeoff here. Any move away from the first possibility, towards the second, reduces productivity. Anyone who has any control over the application is in a trusted position. Anything that you can't trust them to do, is something that you'll have to either do without, or create a handover process for someone else to do. You can't revoke trust without reducing productivity.
",6,2016-09-01 13:03:01Z,45
50,7,"
It depends on the kind of software you're developing and the size of your organization.
For a single rockstar developer's workstation in a small company, I would use an executive level security exception.  The risks (including the annoyance of IT, executives and fellow developers) would have to be discussed, but ultimately, if the rockstar doesn't get his way, he's probably going to move to another company.   These guys do not tolerate friction, if they encounter it, they can easily find more interesting places to work.
A more typical scenario than an uber-workstation is to have a development cluster where operations manages the life and death of the VMs, the environment is monitored with IDS/IPS and Internet access (on the development cluster) is limited but opened as-needed.  e.g., for Documentation... nothing wrong with whitelisting every technology source related to your development effort.   Developers can't pull in code willy-nilly anyway.   They need to document their sources and verify weird licenses.
If you can get the ear of the rockstar, he can help push the requirements to ops and executives and architect the cluster and processes, and educate the development teams on the need.
If there's a budget and the developer is reluctant... then IMHO, you're not dealing with a rockstar, but a diva and their whining should be carried right up to that executive risk signoff.
The hard part becomes managing machine lifespans and making sure developer afterthoughts don't become ""operations-like"" developer-production systems.   But that's much easier than VMs on developer workstations.
",5,2016-08-30 17:08:58Z,596
50,8,"
This question raises a number of interesting questions and challenges. As
someone who worked as a developer for many years and then moved into security, I
can appreciate many of the arguments and points of view expressed in the
responses and comments. Unfortunately, there isn't a definitive answer because
the correct solution depends on context, risk exposure and the risk appetite of
the organisation.
Security is not something you can measure in absolutes. Any time you come across
a security person who constantly talks in absolutes and insists on enforcing
specific policies regardless of anything else is either inexperienced or
incompetent. The role of the security engineer is to facilitate business
processes, not impede them and to ensure business decisions are made which are
informed by the relevant security risks. A good security engineer will know that
any policy which prevents staff from doing their job will inevitably fail as
staff will find ways to work around the policy. More often than not, these
work-arounds will be even less secure. It is the responsibility of the security
manager to understand the various roles within the organisation and ensure that
policies are structured in such a way that they not only support what the role
requires, but encourage good practices and discourage the bad ones. This can be
vary difficult, especially in large organisations. At the same time, developers
and others within the organisation also need to recognise that they may not have
all the relevant information to understand why certain policies are
required. Too often, developers and others see the security engineer as an
obstacle or interfering bureaucrat who doesn't understand what they need to get
their job done.
More often than not, the way to address these issues is through communication. I
have frequently had developers come to me frustrated because some policy which
they see as pointless is getting in their way. Once we sit down and discuss the
issue, a number of things typically happen;

The developer has either misinterpreted the policy or has read into it
assumptions which are incorrect and given the impression the policy prevents
something which it doesn't. This will often result in a review of the policy
to improve clarity
The security engineer becomes aware of a legitimate business requirement which
needs to be satisfied in a way which maintains adequate security
controls. This will likely result in a review of the policy. 
The developer becomes aware of some other requirement or risk which were not
obvious to them initially. This often results in the developer identifying
alternative solutions to satisfy their requirements
An issue is identified which cannot be resolved to the satisfaction of either
party in a manner which is within the accepted risk appetite of the
organisation (i.e. accepted levels of risk). This situation will typically
result in the issue being escalated to the executive level for a decision. The
difficulty of doing this will depend on the size and structure of the
organisation. Once the decision is made, both the developer and the security
engineer need to work within the parameters set by the executive to find the
best solution they can.

There has been a number of responses who are critical of policies which result
in an adverse impact to their level of productivity. While any developer should
raise such concerns, at the end of the day, they either must accept whatever
decision the executive makes or look for an alternative employer. To assume you
know better or that you have some special right to ignore the policy is arrogant
and dangerous for both you and your employer. If you are convinced you are
right, then you should be able to convince management. If you can't, either your
not as good as you think, lack adequate communication skills to present a
convincing argument, don't possess all the information or are working for an
incompetent employer. After 35 years in the industry and despite what Dilbert
may lead you to think, the latter is not as common as you may expect. The most
common source of conflict in this area is due to poor communications and lack of
information.
A common failure amongst developers (and one which I have been guilty of) is to
focus so much on your specific task or objective that you miss the bigger
picture which team leaders, managers and the executive need to also
manage. Environments where developers have been given a lot of freedom and
trust which has resulted in high levels of productivity, but it has
resulted in other problems - the situation where a
key developer has left or is off sick for an extended period of time and nobody
else can pick up their work because it is all on their uniquely configured and
managed desktop or trying to debug a difficult issue which cannot be easily
reproduced due to lack of standard setups/configurations, or dealing with a
possible security breach due to a developer accidentally leaving some service
running which was under development and lacked standard security controls. Being
a developer focused on a specific domain or technology does not also guarantee
expertise in everything else. Some of the best developers I've ever worked with
have been some of the worst when it comes to security and/or management of their
environment. This is probably partially due to the focus on a specific problem
and more likely simply due to capacity. None of us have the capacity to be
across everything and we need to recognise that sometimes it is important to
defer to those who specialise in areas we don't.  
The Changing Environment
One of the main reasons for policies which restrict what can be done on the
desktop is due to an underlying assumption that desktops within the local
network have a higher level of trust than desktops outside the
network. Traditionally, they are inside the firewall and other perimeter
defences and have move access to information resources. This means they pose a
higher risk and therefore need more security controls. Other reasons for
restrictive policies/practices include standardisation of environments, which
can reduce support costs and increase consistency (which was especially true
when there were more applications where were platform/OS dependent - remember
all those horrible applications which needed AtiveX or a specific version of
IE).
The growth in virtual machines and containers, cloud services and commodity IT
services and increased network capacity is resulting in a number of changes
which will likely make many of the issues raised in this thread irrelevant. In
particular, the move towards zero-trust networks will likely see significant
changes. In a zero-trust network, devices inside the network are not seen as
having any special additional level of trust compared to devices outside the
network. You are not provided with the ability to access resources simply
because you have the right IP address. Instead, trust is based more on a
combination of user and device information. The policy which determines what you
can access is determined by your credentials and the device you are connecting
from. Where that device is located is irrelevant. This is also a model which
fits far better with the growth in BYOD and the increased mobility of the
workforce and growing demands to employ staff based on sills/ability and not
geographical location.
Once you remove the level of 'trust' associated with the device, you don't
require controls over what you can apply to the device. You may still require
devices to support specific profiles - for example, you may refuse allowing
anyone to access your resources if their device is running Windows XP
etc. However, you are less concerned about the device. Likewise, you won't be
doing as much work directly on the device. To some extent, the device will be
more like a thin client. You will use VM's and containers hosted remotely. This
won't in itself solve all the problems and may be seen as just moving some of
them from the local desktop to the remote VM or container. However, once you
combine this with various DevOps style orchestrations, many of the reasons
developers may need enhanced privileges are removed. For example, you may not
require any access to the production systems. Promotion of new code will be
handled through an orchestrated continuous integration system and when you need
to debug an issue, you will be provided with a VM or container which is an exact
clone of the production system.
None of these changes will magically solve anything, but they will provide a
wider range of more flexible tools and solutions with potentially less
complexity. Lowering complexity will make security management much
easier. Easier security management will lead to less unintentional or
unnecessary restrictions or impediments to performing work duties. However, at
the end of the day, the key requirements are

Recognition by all that one size does not fit all. Everything needs to be
evaluated within the context of the organisation.
Willingness to put the needs of the organisation first. 
Good bi-directional communication.
Willingness for all parties to work towards solutions which are mutually
acceptable and
Willingness for all parties to compromise
Willingness to work within the system and adjust your workflow or preferred
way of doing things to fit with the requirements of the organisation

",2,2016-09-02 03:44:29Z,60
50,9,"
As a consultant I have worked for many different companies, and only 2 of them did not grant developers admin access to their machines; one was a small company, the other was a very large company.
At the small company I requested admin access, explained why for about 60 seconds, and was given it right away.
At the large company I requested admin access, and was told that even though they agree I should have it, company policy forbid it and they could not change it. So one of the IT guys came over and created a local administrator account on my machine and had me set the password for it. From then on anytime I needed to be an admin I could login to the machine as the local admin, or simply use runas to startup Visual Studio or whichever service/application I needed to run with an admin user (IIS, for example). This isn't much worse than choosing the built in ""Run as Administrator""; it's just slightly slower, though fortunately doesn't come up too often.
",1,2016-08-31 17:39:04Z,597
50,10,"
I am also a developer and here is my opinion:
The problem is worse than you think
There is no spoon
Development is not about software. Development is about making or improving products, services and processes. Software is an important gear but is not the only one. Good developers will define the processes in the wider sense to know which software components to create as well which human, logistical, risk-management process to propose. Does not make any sense to develop a software system that depends on human, logistical and paper processes that are not implemented.  
There are no rules to the development because the development is defining the rules. That is what makes development the worst environment to secure. 
But that does not mean some controls should not be established. On the contrary, many controls should be set up by the development team itself.
Engineering process
There are companies that advocate separation between business and technology 
in a top-down process. This is actually very popular because suggests that business people with no technical knowledge should be on top. Lazy people love that. But in engineering a top-down design simply does not work. Feyman (1985) made a nice article about that in the presidential commission that analyzed the Challenger explosion. Basically engineering top-down processes eventually break the company. And my market experience reinforce this understanding.
The Challenger explosion is an great example. Nasa managers testify on camera on a inquiry commission that they developed a rubber that can remain flexible under 60 degrees below freezing. A thing that was contradicted by an simple high-school physical experiment made by one of the commissioners (put the rubber component in ice water pressed by a clamp). This was a great example because this rubber component needed to be flexible for the booster not to explode; since it needed summer temperatures to do that, the booster only worked in summer. A characteristic of a single component defines a visible characteristic of the entire product that is very limiting. 
Engineering should happen bottom-up because you need to know the limitations and weaknesses of each component to elaborate processes to mitigate them. More often than not, the mitigation processes are not software and will affect the cost of the product. Meaning that the characteristics and limitations of the individual components will define the characteristics of the products, services and processes. 
Top-down processes are fundamentally broken. Many companies that adopt this philosophy on paper still have some market success.  But when you go down and investigate their big and most successful projects, you learn that they were conducted outside the normal company rules. The biggest successes are attained when one person that have deep engineering knowledge and market-wise vision is informally empowered. Since this happens informally, the management thinks the top-down process works. They brand that all other teams are incompetent. Turning a blind eye for the fact that the initial project outline when it left the ""top phase"" was completely ignored and does not describes the products, services and processes built. 
Your manager can define that you will engineer a teleport device by the end of the month because he concluded that this will allow high profit in the travel business... but that will not make it happen. Top-down projects are like that, set expectations that are not technologically sound. 
Do not get me wrong, it is good to see the problem for many angles. Bottom-up, Top-down, SWOT and more are healthy for the analysis, but the genuine engineering effort is bottom-up. There is no genuine goal without technical viability. 
Development Security
We got to remember that software developers will change the company software on a regular basis, and, that way, they can: change what appears on the screen of anyone, send automated e-mails to anyone including themselves, or open back-doors to do what they want. Meaning that a criminal hired as a developer can do significant damages to the company. 
Worst than that, there are many companies that do not enforce the a source code repository provenience, and then the hired developer can deliver a binary that is different from the source given. This allows criminal developers to hijack the company systems, if they are not hired, the things will stop working soon. 
To me development security should focus in:

Source code version control: to ensure that the source code and third part components needed are stored in a secure location.
Strategic division of labor: junior and temporary developers must have limited access to the source code and data. They only need access to the components they are changing to avoid a junior developer being able to understand the inner workings of all systems and be able to exploit that. 
Trust the core developers: Senior/Core developers will have to know everything, have access to everything, to plan and distribute the tasks and diagnose severe problems. This core must have access to the whole thing, both in the development and production. They are your partners in the development of the security policies. We must accept that the core developers sort-of own the company. My old boss used to say: ""we are lucky Lucas is on our side, on the other side he would destroy us"". Core developers can do a lot of damage if they want and there is no firewall and production control that can prevent that. 
Separate the environment through firewalls: separate your development network, from your test network, from your production network. On a company I defined the network 10.1. as development, 10.2. as testing and 10.3. as production. The 10.2 and 10.3 networks only receive code through the corporate CVS and build them automatically upon the admin command. Although it was a small startup and I was in the production and in the development teams, I made some bureaucracies to avoid my own mistakes (developers can be your best allies). I also did change the terminal colors by network: when connecting in a production server the terminal background was red, testing was yellow and development green. Since all my servers used the same configuration it was easy to confuse them if the prompt was open. To my experience most problems come from badly tested software and new software installations. To make clear: where you are is a powerful security feature in my opinion. It has nothing to do with access, but it is security. 
Hire an skilled test developer: The key aspect in testing is to have large amounts of good simulated data that is meaningful to the problems that the company face. Monte-Carlo simulations are good to generate large datasets that has meaning to other developers and can lead to stronger and resilient software and processes. To me there are no ""production"" failures, the developer is always to blame. The maintenance tasks and contingencies have to be written. Software has to be resilient. 
Source code review: have people to review the source code before accepting the modification. All projects should be branched on the source code control and the merge should be reviewed. The source code review should only bother with malware detection, access escalation, access profiles and a good explanation of what the source code means and should do. The quality of the code will be assured by the testing, not the source code review. You can see this in action in most open source projects.
Test policies tests are much more a corporate culture than a framework. Some companies adopt market frameworks, do some testing, but the quality of their code is bad. That happens because you need people capable of engineering meaningful tests. Actually the development must become test driven. I know no other secure way of development. And a curious thing is that humans, purchases, and consulting all have to be tested also. Vendors often claim their products perform flawlessly, but I did not found a flawless product yet. 
Policies are meaningless if not monitored. One company I know have a bureaucracy that every database table should have a description on the attribute level. 95% of the attributes are described as ""the ${attribute name} of ${table name}"". It does not explain what the attribute really is, what values may hold and stuff. 
Appropriated compensation and work environment. To have good developers, both in skill and personality you have to have good compensation policies. Money is important, of course, but is not enough. You also need to have perspective/stability, true recognition and a good work environment. For example, instead on a development office in New York where people live in small apartments, you can choose a smaller city where the same compensation allows for a bigger house and more proximity to the work. Bigger computers, good laboratories are also a plus for technology enthusiasts. 
Data security many activities require sensitive production data, and should be accessed in a special lab. Unless you information is public or not sensitive, maybe the best policy is to put labs in good neighborhoods with controlled physical access. Allow only some simulated data to be put on personal laptops and components that are not sensitive. That is possible. For example, I developed an 4.5 billion records heavily accessed data archive for a company. I did in my home and used absolutely no company data to that end. When I submitted the code it worked as expected in the first attempt. Other than hardware failure and migration of production environments we have 100% of availability in 10 years. The risk of the developer take the source code with him is not relevant. This particular system took me 3 months to develop, a great deal of this time was to understand the performance limitations of the components. This is now knowledge inside my head. Even without the source code I can re-develop this solution in about a week now. 
Strong logs are important to know everyone that did something. The best here is for the logs to be generated by a framework, logging for short time detailed screens, for longer times access and activities and even longer the corporate logs. My critical systems logs for every time a screen is accessed (including the design of the screen). Some of the critical resources should be logged by a trigger on the database itself and the critical tables or resources should be flagged for source code auditing.
-Log screening is difficult to do by hand. Learn how to make filters on the logs to see critical things. One very useful filter is to cross complaint reports with user access and activities. If you have good enough logs you will see coincidences. For instance: before a problem user1 always logins.

About not accessing production
The rules that require the developers not access production systems as users are there to avoid developers from submittin code to show to his/hers own user privileged information. I think this is a very, very weak security measure and easy to detect in source code auditing. There is several easy ways to circumvent that: 

a developer plus one low paid employee; 
send himself an e-mail;
open a back-door in the system.

Source code auditing looking for backdoors, access escalation and malware seems more productive. It allows to identify the bad developers when they are testing their exploits and fire them. Of course a skilled developer can hide an exploit in plain sight, therefore it is important to use languages and variable names plain and clear. Only resort to weird solutions in documented points of the applications that need special performance or obfuscation. For example, 1 << 4 is the same as 1 * 16. This only would made sense if the language does not make this optimization by itself and the point where it happens is a performance bottleneck. Too symbolic languages are bad for this very same reason. Source code should be readable by any geek.
The problem is worst than you think
The easiest and worst damages a developer can cause are not related with tool installation. Even if the development environment is managed, will make little difference if the company does not have strong firewalls, source code repositories, builds based exclusively on the source code repositories and code review. 
",1,2016-09-21 16:42:51Z,598
50,11,"
Do remember the bigger security issue here is the exfiltration of IP, not malware. You hopefully have IDS/IPS in place to deal with the latter which should make it a non-issue.
You can get away with giving admin access to dev equipment, provided you have the following in place:

Anti-eDLP (network, USB, etc)
IDS/IPS
MITM NTLM-authenticated corporate proxy through which you force all connections and monitor for DLP violations
Deny all virtualization software on desktops. If you need a VM, provision one from a managed cloud whereby all the VMs run the same outdated, standardized image with the above tools in place.
Block access to external software repositories at the network level (this cripples pip, git, maven, apt, yast, zypper and everything else)
Block access to github, codeplex and all 150 other code-sharing sites at the network level
Block access to all 9000+ file-sharing sites at the network level
Block access to P2P...etc...

My employer does all of this and more. Quite a few colleagues end up developing on their own personal equipment after-hours at home anyway and throwing it back over the fence through side channels just to get around the crippled infrastructure. Personally I'd rather spend my evenings drinking and dreaming of overdosing on dog tranquilizers than waste an additional undocumented 20 hours doing something that would get me fired anyway.
We consider computer science (and by extension, software engineering) a science. Scientific experiments are conducted in sterile conditions to keep variables under control. It is not scientific to develop in a crippled environment where things do not perform as expected due to environmental factors. I can tell you from experience that you don't get well-engineered software out of it...everything developed in-house is broken, which leads to more spending on third-party solutions.
Developers absolutely must have a sterile environment to develop in. I can't tell you how many times various security controls have introduced gremlins into dev and production architecture-- thus a managed, sterile, cloud-based dev environment is really the only way to go. ""It worked in the lab, the problem must be something external."" 
You just need to make sure the VMs you let them provision aren't anemic and you need to automate the provisioning process a la OpenStack or AWS so devs are not waiting for days to satisfy scaling needs. Having them all centrally managed gives you a great deal of auditing control, and frankly gives the devs better performance than they'd get on their own equipment anyway.
",0,2016-08-31 16:56:53Z,599
50,12,"
I am a fan of option #2: have separate computers.
Allowing company machines with proprietary information to be internet connected opens the door to hackers. It also makes it much easier for employees to exfiltrate company data for illegitimate purposes.
If the employee has to use a special machine to access internet resources, it creates a controlled path for infiltration and exfiltration of data that is much secure. Also, it discourages employees from idly browsing the web or wasting time on internet forums, like I am doing now.
",0,2016-09-02 18:52:05Z,600
50,13,"
As an admin, this problem is resolved completely and utterly through a SaaS approach. Virtualize all your different environments, put them on a server rack appropriate for the number of clients, and configure some remote access (RDP, Terminal Services, or other...). Now everything is on the server, and you only have one of each environments to maintain for everyone.
",0,2016-09-05 04:44:54Z,428
51,1,"
Strictly speaking, both are insecure compared to USB boot etc. However, every layer of security drops the probability of generic attacks succeeding. Using a VM with incoming network connections disabled should be immune to the vast majority of malware on the host. You could disconnect the USB keyboard from the host and attach it exclusively to the VM to avoid keyloggers on the host.
However there is always the possibility of sophisticated malware or a directed attack that can compromise the VM.
Either way I presume a VM with the above precautions is more secure than any sandboxing / safe environment trick on the host where the attacker has root access. Even with root access, the probability of an automated attack on the VM is low - it would need to use the specific virtualization software's APIs to infect the VM since regular channels would be blocked.
",3,2016-01-12 06:20:13Z,602
51,2,"
If you want to do home banking on a public network I would always recommend using a VPN. This should protect you against MITM attacks and other funny things that can happen on a public network.
There is nothing wrong with using a Windows tablet to connect to your bank. Be sure to install a decent antivirus/malware and install your Windows security updates.
And like Natanael pointed out ""a virtual machine can't protect the guest OS from the host"". You should reverse that logic: if you want to visit some kind of website you're uncertain about if it doesn't contain any malware, do this kind of things in a virtualized/sandboxed mode. Also if your kids use the tablet to play games, make sure they do it in a sandboxed mode so your tablet doesn't get infected if they end up installing a malicious app/game.
",5,2016-01-11 22:25:03Z,604
51,3,"
A virtual machine can't protect the guest OS from the host (without specialized hardware features to support it). You will not gain any security from running Linux in a VM if Windows gets infected, and if your VM gets infected then that's still the environment you intended to use for security sensitive things that's new insecure. 
Have you tried live USB solutions? Unetbootin?
",4,2016-01-11 22:01:01Z,605
51,4,"
If you have very sensitive data, you should keep it somewhere, where there's no direct internet access.  In that case, Linux on a virtual machine will do the job(but still, it's not 100% safe in this case). If you need an internet access, I suggest installing plugins like HTTPS Everywhere and uBlock/uMatrix/NoScript in your web browser, to keep the connection safe and block any unwanted scripts from execution.
",2,2016-01-11 22:02:05Z,606
51,5,"
Setting up a Linux VM dedicated for Internet Banking is a good idea, but if the VM is used for other purposes have a browser dedicated for Internet Banking only where everything that is not required is turned off (i.e. scripts, flash, etc...). As mentioned by Jonathan a VPN is a good idea too.
",1,2016-01-11 23:44:00Z,607
52,1,"
Technically slightly, yes. But:

It would be security by obscurity, which is a bad idea
It does not boost confidence in your product
It would be very easy to figure out what does what, it would only take a bit of time
Google Translate, you can just use meaningless names, it would still not help much
It would make maintenance harder
It would make audits very hard, as the auditors may not understand the language

All things considered, it is probably never worth it.
",174,2018-04-30 14:03:35Z,404
52,2,"
It would not be appreciably more secure. Reverse engineers are often forced to work with systems that do not have any original names intact (production software often strips symbol names), so they get used to dealing with names that have been generated by a computer. An example, taken from Wikipedia, of a snippet of the kind of decompiled C code that is often seen:
struct T1 *ebx;
struct T1 {
    int v0004;
    int v0008;
    int v000C;
};
ebx->v000C -= ebx->v0004 + ebx->v0008;

People who are used to working with this kind of representation are not fooled by the usage of variables and such that are given irrelevant names. This is not specific to compiled code, and the use of C was just an example. Reverse engineers in general are used to understanding code that is not intuitive. It doesn't matter if you are using JavaScript, or Java, or C. It does not even matter if they are analyzing nothing but the communication with the API itself. Reverse engineers are not going to be fooled by the use of random or irrelevant variable or function names.
",62,2018-05-01 06:05:43Z,76
52,3,"
Not really - all of the built-in functions will still be in English, so it wouldn't take much extra effort to work out what your variables are going to represent. It might slow someone down slightly, but given that people still manage to reverse-engineer code with single character variables all over the place, or which has been run through obfuscators, swapping the language used for variables and functions just means doing a find-replace once you've worked out what one of your variables is used for, then repeating until you have enough understanding.
",35,2018-04-30 14:03:09Z,407
52,4,"
That is security through obscurity and will delay a dedicated attacker all of five minutes.
If you want to confuse an attacker, naming things their opposite or something unrelated would have the same effect. So your ""create user"" function could be named ""BakeCake"". Now you can answer yourself how much security that gives you. Actually, this would be more secure, as it can't be defeated by simply using a dictionary.
Yes, at first it would confuse, but one look at the system in operation and everything becomes crystal clear immediately.
",21,2018-04-30 19:11:16Z,25
52,5,"
Your system MUST be secure by itself. If it relies on user-side javascript passing a parameter with the value ""open sesame"", you are doing it wrong.
You should develop the program in the language that is more convenient for you (eg. based on your coder proficiency, consistency with your code base, or even with the terms that you are using).
Other answers already pointed out how it doesn't really provide security. If you want to make a secure program, rather than concerning about potential hackers easily reading your code and learning a secret hole, you should probably care more about making it readable to the people auditing it.
If there is a function parameter called nonce, and a comment saying how we are ensuring it is unique across the requests, yet it isn't sent on the request, you can be quite sure it is a slip-up. Actually, having that code easily readable will decrease the chances of that parameter being dropped/empty (after all, everything worked without it...), or if that really happens, make easier that another of your developers notices the problem.
(Third parties could hint you about it, too. Probably, there will be more people having a casual look at it than ones actually trying to break it. A random attacker will most likely start by launching an automated tool and hoping it finds anything.)
TL;DR: produce readable code. For the people that should deal with it. In case of doubt, you should prefer the one most people know about.
",9,2018-05-01 21:15:38Z,14
52,6,"
It could even make things worse by making the system harder to maintain. 
Take an extreme example inspired by history, and communicate between front and back ends in Navajo. When (not if) you need to patch the code you need to either:

work with what to you is nonsense (with the extra chance of bugs/typos plus it takes longer to work on non-intuitive code), or 
hire/keep programmers fluent in Navajo (a rare and potantially expensive skill combination, possibly impossible to find a contractor)

",7,2018-05-01 14:08:36Z,411
52,7,"
I would also note that in most instances for javascript on the client side, the script as developed (with meaningful variable names etc) would be 'minified' in order to increase performance on the client (smaller file size to download). 
As part of this, most variable names are reduced to single characters and as much whitespace as possible is stripped out.  This becomes just about as unreadable as anything else you might write.
I would also note that chrome (for example) has methods that take a 'less human readable' file like this and 'pretty print' it, making it a lot easier to figure out what is going on.
In short, the human language that you use to write your client side code really doesn't make a big difference.
",5,2018-05-01 12:56:41Z,412
52,8,"
If the mass media are to be believed, then the majority of hackers are Russian, Chinese, North Korean, so they are already operating under the “handicap” of having to hack Western systems in a non-native language. Therefore unless you choose something incredibly obscure, like in the movie Windtalkers it won’t make any difference to them. But the extra effort for you means less time for you to find and fix bugs, so if anything this strategy would make your security weaker.
",4,2018-05-03 21:36:22Z,413
52,9,"
Several responses have (rightly) pointed out that this is “security through obscurity”, which isn’t actually a form fo security. It is safest to assume that the attacker has fully annotated source code sitting in front of them while busily attacking away.
Software is “secure” when knowing everything which can be known in advance of a request / transaction / session is public knowledge AND this has no impact on the actual security of the product.
Source code must be assumed to be public knowledge, if only because disgruntled employees aren’t taken out back and shot when they are terminated. Or as is often said, “The only way for two people to keep a secret is if one of them is dead.” Thus any “special knowledge” which is being concealed by obfuscation of any sort must be assumed to have become “general knowledge” as soon as it is produced.
Security, at its core, is nothing more than “saying what you do, and doing what you say.” Security analysis — code auditing and formal evaluation schemes, such as are conducted under the Common Criteria — requires that the mechanisms for performing an action are well-documented and that all transitions from one secure state to another secure state are only possible when all the requirements are satisfied. One risk with obfuscation of all sorts is that these requirements are obscured by clever coding — see the example of “create_user()” failing because it is “secretly” the “delete user” or “rename user” or something else method. For a real-world example of how hard such renaming is on the brain, there are on-line tests in which you must read the name of a color which is printed on the screen in a different color. For example, the word “RED” written in a blue font will result in some number of people saying “BLUE” instead of “RED”.
",2,2018-05-03 21:47:01Z,414
52,10,"
It might be irrelevant. Consider the scenario where you're coding in (e.g.) Italian instead of English, but most of your customers are in Italy, naturally Italian-speaking hackers have a higher motivation/payoff from attacking you.
In this scenario, coding in another language has either no effect or it can even make it easier to hack.
",0,2018-05-01 05:15:32Z,416
53,1,"

My question is what prevents users from intercepting their regular post form the app (getting the token) and then possibly sending bunch of POST requests (using something like postman or fiddler) to create a large number of fake posts or articles or whatever else the app does.

Nothing

Does the fact that the traffic to the service will eventually go via TLS make this a non-issue?

This makes no difference at all.

What are some possible ways from protecting from this?

The most common one is rate limiting. I.e. if someone posts at a much higher level than anticipated reject the post. There are several approaches to this - when did they last post, rolling average over N minutes etc. If you don't want false positives resulting in users losing post content then make them re-authenticate to continue.
Another approach is captchas. I.e trying to make the user prove they are human.
Another is attempting to detect automatically generated content using spam filters or AI.
",30,2017-11-13 15:56:30Z,531
53,2,"

My question is what prevents users from intercepting their regular post form the app

Nothing.

Does the fact that the traffic to the service will eventually go via TLS make this a non-issue?

If you make it for an mobile platform (Android/iOS), that makes it much harder (but not impossible).
If you make it for the browser, this doesn't add much protection.

What are some possible ways from protecting from this?

It is hard to protect against automatic requests, but one thing you could do is rate limit.
",12,2017-11-13 15:54:10Z,617
53,3,"

My question is what prevents users from intercepting their regular post from the app (getting the token) and then possibly sending bunch of POST requests (using something like postman or fiddler) to create a large number of fake posts or articles or whatever else the app does.
What are some possible ways from protecting from this?

You don't.  That is, you don't protect against this - from the perspective of authentication and authorization, there's no attack happening here, just perfectly legitimate traffic.
The problem instead is ""How do I prevent users from spamming my service?"" (or similar), and that's completely orthogonal to the question of authentication tokens.  A user could similarly spam things manually through the app.
Rate limiting by user account, rate limiting by IP address, using cookies or device identifiers to tie multiple accounts together to rate limit by device, terms of blacklists, heuristics for spam, etc. are all common methods to deal with spam.  But whatever your actual thing is that you're trying to prevent, that's what you should be looking into, not preventing users from modifying things client-side (which they will always be able to do).
",6,2017-11-14 21:33:48Z,619
53,4,"
The token you give to the client should contain a signed expiration time that would be verified server-side (e.g., limited to a typical user session time you'd expect for your app). This won't prevent the re-posting, but will limit the period within which it could be done after the authentication. On expiration the user will have to re-authenticate. This is commonly implemented using JSON Web Token.
However, you're talking about malicious mis-use by a legitimate user (unless an attacker already compromised the legitimate user's device and can intercept the traffic in clear) - such mis-use is very difficult to prevent without making the app almost unusable as others noted (e.g., by making users authenticate on every request). Storing credentials on the device and silently re-sending them is a BIG NO-NO.
",4,2017-11-13 18:41:00Z,622
53,5,"
Correct, that a session token alone does not insure that a hacker can't intercept a packet and reuse the token for their own purposes... at least as long as the token is valid. Most session tokens have a time limit, although that is dependent upon the authorization method used to validate the token on the server end. 
Timing is one way to guard against this, although it isn't foolproof. Since you're writing the app, you should have a reasonable idea of how quickly the app can be operated, and an expected rate of service calls. If your app, in typical use, can't submit a service call more than once a second, and the service receives 100 requests in one second, that's clearly a hacker at work. 
However, this assumes a hacker will bombard your service with requests. A hacker could figure that out after a few failures, and lower their rate of requests. As soon as they see the service rejecting what they believe to be a valid session token, they'll start looking at the obvious, like timing. 
Another way to guard against this is to require SSL to access the service. That will make the packets, and authorization token, difficult to extract. They'll need a lot more than a packet sniffer. A particularly knowledgable hacker might try to probe into the app's binary, but that's a lot of work, especially on mobile platforms.  You would have to purchase a SSL certificate for the server, but that's cheap insurance. 
A method I've been experimenting with is to append a sequence number to the session token, hashed so it doesn't look like a sequence number. The authorization service maintains a count that gets incremented every time a token is validated. It strips off the sequence bytes before validating the token, and then checks the sequence number. 
The client is expected to start at zero when it initially receives the session token, and increment the appended count by one every time a call is made. Thus, when the server last received sequence 250, and another one comes in that's sequence 135... ignore the request, maybe lock out the source IP, and send the admins a notice that a hack attempt may be in progress. 
However, that adds some complexity to the client app, and also could run afoul of dropped packets or dropped return packets. Just something I've been experimenting with. 
And, yes, a hacker might eventually be able to figure that out, but only after a lot of false starts... giving the admins some warning that an intrusion attempt is under way. 
",2,2017-11-14 05:59:32Z,623
53,6,"
Plenty of other people have said ""nothing"" in response to the first question on ""what prevents.."" and it's true; ultimately nothing really defeats someone absolutely determined.
You also asked for strategies that can counter this; tj1000 touched on it, and I thought I'd toss a similar idea into the fray, based on work I used to do with credit card terminals.
Way back when I was a junior dev, I was handed a task that was deemed too hard to be worth solving by the pro devs (I guess it kinda shows what I was paid); we had thousands of credit card terminals that called in over an old pre-isdn link, did some auth, recorded a transaction, got an approve or decline from the server and onto the next transaction. The cute part is there was never another message followup from the terminal if the transaction was voided after we'd approved it (this was in the days of signatures, before user identity was pre-authed by a chip and pin), but there didn't need to be
These transactions were protected and confirmed by what was termed a MAC - message authentication code. Built into the hardware of the terminal was a hash key, unique per terminal, by the manufacturer. The manufacturer would share with us what the hashing key was, and when the terminal appeared presenting its unqiue ID we could look up the hash key. The message bytes the terminal formed would be hashed by the terminal, with half of the previous or initial hash being appended to the message. The other half would be used to update the hash key used for teh next message. At the server side, we'd carry out the same hashing to know if the message had been tampered with, and come to the same hash result, we'd also know to roll the hash key on with the same half residue we had, but we'd keep track of the previous hashkey too. The next time a message came in, one of two things was the case. If the previous transaction had succeeded and was to be accumulated into the daily totals, the terminal would use its new rolled hash key to hash the latest message. If the previous transaction was rolled back (user canceled, bad signature etc) the terminal would re0used the previous hash key. By hashing the message with the latest rolled key and finding no match, but hashing with the previous key and finding a match, we knew the fate of the previous transaction was fail, and we'd remove it from the daily totals.
Hash keys would occasionally go out of sync; when this happened, neither of our stored keys would produce a matching hash for the message. There's one more key to try - the initial key (the supervisor users could reset the key to initial, and some users seemed to do this upon any problem, believing it to be something like a reboot - seldom the case and caused more problems than it solved but..).
If the initial key worked out, we couldn't say for sure what happened to the previous transaction but we usually accumulated them (charged people's accounts) based on the theory that people would complain if they weren't refunded when due but not if they were refunded something they'd bought..
if the initial key didn't work out, then the terminal effectively became useless, as the key rolling sync loss means no more messages can work. We didn't have the authority to tell the terminal to reset its key itself, but we could put a message on the display imploring the user to do so
Long story short, you don't have to use the same token, if you're concerned that tokens will be captured and replayed as a stored password alternative. Others have pointed to options of making tokens expire after a time; this method is essentially token expiry after every request (similar to another mention about appending a sequential number to a token), with a known, internal way of calculating a new token on each side that has to be performed in step. 
If you're interested in the boring details of the way the credit card world does it in the UK, look up APACS 70 Standard Book 2 and Book 5. These aren't freely available, alas - you have to be a member to receive a copy of new publications, but you might find the content of old versions of them floating around the web
",1,2017-11-14 10:53:04Z,624
53,7,"
So... there's nothing you can do to secure the client machine to keep the token secure on their end, but there are some best practices to JWT security:

Implement refresh tokens and issue short lived access tokens - this adds complexity to any attack + the way I've seen refresh token implemented is with a SQL table... which you can manage (ex. instant boot).  
Delete the token from the client on logout (this one's obvious).
You can encrypt the token, but the encryption's reversible, again makes it harder.
Use a secret key to sign your tokens and rotate it as needed (ex. per release).  Changing the secret key invalidates all prior tokens.

Also, read:
https://auth0.com/blog/refresh-tokens-what-are-they-and-when-to-use-them/
If you need further security, you need more mechanisms to secure the client specifically: whitelisting user IPs, ensuring users have AV and anti-phishing training, etc... 
",1,2017-11-14 23:39:45Z,625
53,8,"
TL;DR a skilled user satisfied with a reasonable spam rate will still get through - if by no other means, by entering their spam by hand.

Have the app authenticate each request independently. It's definitely not foolproof, and it makes for increased traffic, but it's doable.
One way: a message post is no longer a single post but a GET followed by the POST. The GET supplies a nonce (it could be the current timestamp), and the POST must supply the nonce together with the e.g. MD5 of the nonce, salted with an app secret. Of course you need to store the issued nonces to avoid replay attacks.
You can also supply a session nonce at login and use that for the whole session (of course the user can intercept it and copy it into a second spamming app, once he's broken the app secret out of the app). This isn't a significant improvement from the authentication cookie though.
Or you can silently add a validation to all app requests, in the form of the current timestamp plus hash of said timestamp salted with the app secret. Uniqueness can then be guaranteed by preventing two posts in the same second client side.
The server then verifies that the timestamp isn't too far from now(), and that the hash is matched. You can even omit the timestamp if it is acceptable for the server to bruteforce it (for timestamp = now()-60 to now()+60; if hash(secret+timestamp)...).
Now, the user has to hack the secret out of your app, which is harder than just intercepting the network traffic. If strings/data are easily recognizable in the app, you can add a little security through obscurity by sending the timestamp, and the hash of the secret plus the timestamp of seven seconds before1. This requires the user to reverse engineer the app altogether.

(1) I once broke a salting scheme by testing all sequences from 1 to 32 bytes in a program, affixed and suffixed to the timestamp in hex, binary and decimal, with a set of separators including the no-separator, and verifying whether the result was the response to the challenge. This allowed me not to debug the binary at all, which I wouldn't have been able to do, and it took twenty minutes to set up, and two to run. If the timestamp had been obfuscated by adding a known constant, I wouldn't have succeeded. If the constant was large, it wouldn't be practical even knowing the trick.
",1,2017-11-15 08:08:06Z,627
53,9,"
As others have touched on, there is basically nothing you can do once the code is running on a user's device. If it's running in the browser, reverse engineering the token process is trivial. If it's running in an app, you can make it much more difficult to reverse-engineer, but it's still possible.
For mobile apps, the attacker has two options:
1) Reverse engineer the protocol. The simplest approach is sniffing network traffic. There are obfuscations you can put in place to make that harder (cert pinning, MAC + secret key + rotation) but any determined attacker will eventually break through them. However, if you use cert pinning and/or the secret key approach, the attacker will not be able to simply sniff packets to reverse engineer the protocol. He will need to decompile the binary in order to disable cert pinning and/or locate the secret key in memory.
2) Treat the application as a blackbox and automate interaction with it. This could be done with a farm of physical devices (much easier if jailbroken iOS / rooted Android), or with a farm of emulators. From an attacker's perspective, this could be a more reliable approach as it would be resilient to any updates you push. 
To guard against #1 (binary decompilation) you have many options, but they all come down to increasing the difficulty of reverse-engineering it, not preventing it altogether. The low-hanging fruit is binary obfuscation and debugger detection. For obfuscation, you can obfuscate the symbol tables, and/or hide the encryption logic in mundane looking functions (or write it directly in assembly). For debugger detection, there are different techniques for determining if a debugger is running; if you catch one, you can crash the app or play games with the attacker.
Guarding against #2 (emulator/device farm) is a bit harder, but again, you can make the attacker's job more difficult. One option is to check if the device is jailbroken (this would also defend against #1) and crash the app if it is. Another option, for Android at least, is Google's attestation service. That should prevent the emulator farm scenario, but not a physical farm. For iOS, Apple released a similar deviceCheck API in iOS11, although it's much more limited in scope.
If you want examples of this in the wild, checkout Snapchat.app, which has implemented many of these features.
",1,2017-11-15 13:16:18Z,626
53,10,"
If an app is designed to authenticate (username/password) once and then employ a token system for REST services authorization then that app has effectively created its own back door access.
I am sure that this solution is going to get me flogged but you basically need to authenticate into every single RESTful endpoint.
You can choose to pester your users and ask for a password upon every single REST request but this might make your app prone to becoming a failure.
The other option is to store the user's credentials in the app's memory upon log-in and send the credentials silently to the REST endpoints.
",0,2017-11-13 17:44:36Z,628
54,1,"
This would be impossible. It is fundamental for your app to contain all the  instructions necessary to use your API. Anyone with enough skill and time will be able to extract these secrets and create their own client.
",40,2017-03-22 00:41:59Z,630
54,2,"
It’s pretty easy and straightforward to create one’s own client regardless of whether REST or SOAP is used, as long as your Existing Client is available for everyone in the Play Store. Just capture the HTTP traffic from an Android device using Fiddler, and engineer your own client based on the captured traffic.
Even HTTPS traffic can be easily decrypted using Fiddler. The HTTP Methods, URLs, Headers, Cookies, Body and your Key are all visible. I don’t think this is secure at all. (There are other reverse proxies out there that can do the same as Fiddler.)
",24,2017-03-21 23:44:03Z,632
54,3,"
From a security perspective, no, there's no way to do this. No matter how much obfuscation you put on the code and protocols, the fact is that the code to access the API and the network traffic produced when the API is accessed is in the hands of your users, and they can use whatever reverse-engineering tools they want on it.
From a business perspective, you need to evaluate the cost to you if someone produces an alternative client against the extra cost to implement measures to prevent that from happening. For example, you could use a heavily-obfuscated proprietary API (avoiding REST, SOAP, and other standardised protocols) entirely, but this will be more difficult for you to implement and therefore cost more. On the other hand, it would make it considerably more difficult for someone to reverse-engineer. You need to work out if such measures (or other appropriate measures) are worth it to your business.
From a legal perspective, a lot of places have terms of service that prohibit the production or use of third-party clients. It's up to you to enforce that by monitoring traffic to your server to determine if one is likely to be using a third-party client, or by watching for the appearance of third-party clients on app stores. It's also up to you to decide if you have the resources to take legal action, and if it's worthwhile for you to take such action.
From a user-relations perspective, you may want to reconsider the idea of prohibiting all third-party clients as a blanket rule. Obviously nobody wants a developer to produce a horrible client with their own advertising, but a lot of services these days allow third-party developers to register their applications and receive an API key. The process of registering can be as simple or as complex as you want, ranging from a simple terms of use (e.g. ""don't put your own adverts in the app"") to an evaluation of their interface or a full examination of their source code (of course, the simpler it is, the more willing developers are going to be to sign up). Allowing third-party developers to use your API isn't necessarily a bad thing either - they might want to produce an app that uses your service as a data source but isn't related to it in any way, and in that case they could actually bring you more business if they put proper accreditation in their app, or they may want to fill a market that your business has no intention of going into, or they may truly have a better idea for a client app for your service, which is something you can learn from to improve your own app.
",17,2017-03-22 08:41:33Z,621
54,4,"
You can't make unauthorized access to the API impossible in the security sense, but you can minimize it. It's not really a security question but the concept of the threat model is pretty appropriate: Who do you want to prevent from accessing your API, and what kind of damage do you want to avoid? 
Many large websites forbid alternative means of access to their service, usually so they can continue to serve ads. They do so through a combination of measures at many levels: 

checking the request headers and user agent (which stops great numbers of casual ordinary users)
complicating the API by adding dynamic state, obfuscation and other measures (which stops casual reverse-engineering, but not the determined)
forbidding it in their Terms of Service (which stops other legitimate businesses, and a few conscientious individuals)
enforcing the ToS through lawyering (which may be necessary against unethical businesses, but will do more harm than good if deployed against small-scale individual users.)

None of these are 100% effective, but they don't need to be. The important thing is that they cut down on loss. Think about what you want to accomplish, and choose your countermeasures accordingly.
",4,2017-03-22 09:56:43Z,634
54,5,"

Say I have a RESTful web service and a commercial Android app on the front end which is used to interact with it. I may use SSL so that the endpoints are not visible, but someone could still do some reverse engineering to find them.

SSL does not conceal the target IP address, so they don't even need to reverse engineer anything. Everything else can be obtained via Fiddler + MITM certificate.

I could also use SOAP instead, so that the call to the web service is a bit more complicated. But, I still don't know if this gives me any real advantage over RESTful based service.

Well, the more effort you put in, the more effort a hacker will need to put in, I suppose.
That being said, hackers have a lot more free time than you, and there are a lot of them, and only one of them has to post the solution on a bulletin board and it's game over.
So, probably not worth the effort.

I was thinking about hardcoding the key into my client app, so that only my client app could use the service. Also, maybe some code obfuscation may help. But, how much does this really help?

Hackers now have deobfuscation tools at their fingertips, so it doesn't help as much as it used to.
Probably not worth the effort.

Fiddler could be used to decrypt https and see the full request. However, if I use Android app only, this may be solved by hardcoding server certificate in Android client app. And also, there is SOAP WS-Security, but I guess a tool can be made to function in similar way as fidler to circumvent that.

The hacker could reverse engineer all that much faster than you could build it.
So, probably not worth the effort.

You can't stop a hacker from reverse engineering any code that runs on the client.  So whatever safeguards you put in, the hacker can imitate them.

Exactly. Don't spend time on this.  
Instead, try to design your application so sensitive features are executed on the server, where you can protect it.
Spend the rest of your time improving the features of your app so customers won't want anything else.
",0,2017-03-22 16:18:30Z,635
54,6,"
You want to :

prevent someone from making his own client app for my webservice

You want your webservice to identify your app on user devices before further interactions, this seems impossible as you have no guarantee on the integrity of the app which could be altered with reverse engineering.
You may want to consider reviewing the architecture of your solution.
",-3,2017-03-22 08:20:37Z,636
54,7,"
Yes, this can be done, though of course there are no perfect solution.
You would need to add encryption and message authentication to both client and server. Both negotiate a session key based on a shared secret, possibly with the addition of a unique client ID. There's plenty of documentation around on how to do that (keywords: session key, key exchange, API key). The key to this system is that the communication is encrypted and signed with the session keys, not with the actual key (aka shared secret). This way, the real key never travels the wire and cannot be intercepted.
An attacker could still reverse-engineer your client to extract the shared secret and the mechanism for deriving the session key from it. But this requires higher skills then simply listening to some REST chatter.
",-5,2017-03-22 08:47:07Z,25
54,8,"
The only guaranteed why to stop unwanted usage is to perform some form of user authentication. Such as requiring the user of your app providing a username and password each time they start the app and passing this to the server. If the server validates the user it would return a session token that has to be used in all subsequent API calls. However, it might be that your application is not suited to making users login. 
Without the login it will always be possible for someone with access to your API because they can reverse engineer your code and/or snoop the data to pass back and forth. So your aim is to make it as difficult as possible for someone to reverse engineer your code and API. Make sure you do not provide a fixed value from the client to the server. That is simple to find and then use in alternate client. 
Instead you want to generate a new code each time you login to your server so that every time someone snoops the traffic they see a different value and they cannot simply replay that value. This forces them to reverse engineer your code to discover the algorithm used to generate the key. Put some effort into making that code hard to understand and so replicate. 
",-6,2017-03-22 06:18:28Z,641
55,1,"

Has anyone ever thought about doing this?

Yes, there was actually a talk about exactly this at defcon 21 (video, slides).
Their conclusion was that working with response codes as offensive security can sometimes result in severely slowing down automatic scanners, non-working scanners, and a massive amount of false-positives or false-negatives (it will obviously do little to nothing for manual scans).
While security by obscurity should never be your only defense, it can be beneficial as defense in depth (another example: it is recommended to not broadcast version numbers of all used components). 
On the other hand, a REST API should be as clean as possible, and replying with purposely wrong HTTP codes may be confusing for developers and legitimate clients (this is a bit less of a problem for browsers, where users don't actually see the codes). Because of this I wouldn't recommend it in your case, but it is still an interesting idea.
",83,2017-02-08 18:47:00Z,332
55,2,"
It won't actually slow down an attacker any appreciable amount, but will cause any future developers who work on your platform to be really annoyed at you.  It may also cause certain nice features of your HTTP request libraries to not be so nice, as they're operating off of incorrect information.
This is a very weak form of security through obscurity.  When designing a system like this, you should be thinking about slowing down an attacker by hundreds of years, not tens of minutes - otherwise you're still going to lose.
",58,2017-02-08 18:34:54Z,619
55,3,"

Rather than return a ""401 Unauthorized,"" why not return e.g. ""305 Use Proxy,"" i.e. purposely being confusing. 

Yes it will confuse an attacker. But for a trained one, it might not be for more than two seconds flat. And status codes are not all that useful, mainly just when brute-forcing file names.
Say i have a valid key, and i can observe you returning 200-range codes for my authentication. If i change a bit in my key, and for every request you either return 305s, i will immediately think ""Hmm. Seems like the dev might've messed up"". If you return random codes, i'll know it was on purpose and i just ignore them.

the difficulty in reverse-engineering an iOS app will hopefully deter all but the most determined hackers

Yes it will, but since it only takes one to publish it, it's again just slowing it down..
",8,2017-02-08 18:45:45Z,644
55,4,"
This is security through obscurity, which is to say it does not provide much security at all.  The solution you are suggesting will only slow down an attacker, not prevent them from using their own client. 
In fact, your method of encrypting the requests, depending on your implementation, may actually make your application less secure by opening up attacks on other parts of the crypto system.  Your effort would better be spent trying to secure the api functions themselves (i.e. pen-test them and secure them against attacks like Sql injection), rather than attempting to prevent unauthorized clients from accessing them. 
",6,2017-02-08 18:33:59Z,647
55,5,"
But the user IS already using your protocol.
Your problem is that your server’s interface of what the user can do is not secure!
You decide what data your server sends out to whom!
(Hello, dear online newspapers. Yes, I’m looking at you!)
Design it, assuming that the user is the client. Not your code. Because he is. It should not matter what client is used.
Your app runs on a CPU that is under hardware control of the user. Your code is just a list of commmands/data, and the user and his CPU can process it however they please. Including not processing it.
He decides what his CPU does. Don’t mistake his grace of accepting your app code as-is for a right to blind execution. You’re the one who’s trusted here, and that trust is very fleeting.
Especially with sleazy tactics like this.
In any case: You hand the user the encryption key and everything, and expect him to not use it himself, because you put it somewhere in your basket of code. … Just like DRM, that’s snake oil and can never work.
It takes only one person to find where you put the key. (That would be me, for example.) Everyone else just has to google for it.
But I’m surprised that you only think about encrypting the protocol against the user, instead of for his protection from man-in-the-middle attacks.
Assuming the reason this is usually done (Yes, I’m talking to you “content industry” again.): If your user is your enemy, maybe you should look for a business model that is based on fairness and a win-win, instead of ripping the user off and having to deal with backlash.
P.S.: Ignore all the “security through obscurity” answers. This is a fallacy that results in correct behavior but is still based on invalid assumptions. Using it as an argument, is, at best, amateurish and not really competent.
In reality, all security is through obscurity. Some is just more obscure (= better disguised). The actual reason this is bad, is because what we call real security is a bazillion times more obscure, giving it an actual (statistical) trustworthily high obscurity, as opposed to very simple obscurity that is just waay too likely for someone to come up with from nothing.
",4,2017-02-09 09:58:04Z,649
55,6,"
As others already explained, security by obscurity slows down an attacker at best.
In your specific case, I would say it will have no appreciable effect. To get to this point, your attacker already had to reverse engineer your App and extract the private key. That means your attacker is not an idiot and knows what he's doing. A little bit of obscurity will cost him less time than it takes you to implement it.
",3,2017-02-09 09:46:02Z,25
55,7,"
As others have already mentioned, you're proposing to use Security by Obscurity. While this technique does have its purpose, consider the following before choosing to take this approach:

Providing tech support for your API. Using deceptive HTTP response codes make it difficult for anyone other than you to provide support. They have to consider whether the particular situation is actually sending the proper response code or if it is sending an obscure one. Should you decide to remain the sole contact for any support requests, this shouldn't be an issue.
What is a ""Malicious User""? Use caution when categorizing a request as malicious for it can have adverse effects. Suppose an IP is determined to be sending malicious traffic and countermeasures are used. Now suppose that same IP is actually a proxy with hundreds or thousands of users behind it. You've now applied your countermeasure to all of them. This same principal can be applied to identifying malicious activity in headers and/or body.
Application code is slowest. The request has to traverse the entire stack to finally get to the ""security"" logic. This architecture doesn't scale well and is slow. Bad requests should be stopped as early as possible which is the premise for using Web Application Firewalls (WAF).
Extending access. Should your API become accessible to more clients then it was originally designed for, those new clients will need to be aware of the possible use of deceptive HTTP response codes.
Persistent malicious users. As others have mentioned, this technique is only a speed-bump.

Better Approach
Focus your time on white-listing all known good requests. This is so much easier then trying to identify all potential bad requests. Anything not appearing on the white-list should immediately get an appropriate response code such as HTTP 400 or HTTP 405 if you're filtering on HTTP verbs (as examples). Preferably this happens before the request hits your application code. 
Along with white-listing allowed requests, ensure your application is secured based on OWASP Guidelines. You'll get far better results spending your time with OWASP then you will trying to determine what a malicious user is and returning an obscure HTTP response code.
",1,2017-02-09 18:56:46Z,650
55,8,"
Fundamentally, you CANNOT prevent unauthorized clients from sending requests. The nearest thing possible would be to have some kind of cryptographic check done in the client, but as Sherlock Holmes said, ""what one can invent, another can discover"". (I know this for certain, because I have cracked people's client-side security on a number of occasions.)
Instead, build your API such that anyone is allowed to use it using custom clients. Make it friendly. What do you lose by that? Attackers will attack no matter what you do, and if you make your API easy to use, someone will come up with something you never thought of, and make your server even greater than you ever imagined it could be. What could be done by a client that talks to both your API and some other? What myriad possibilities are there, and will it really hurt you to allow them?
",1,2017-02-10 08:28:14Z,651
55,9,"
I wouldn't do that. It generally means you are creating your own standard, which causes:

your API to be predictive after making a map of your http responses to their actual meaning. Changing HTTP responses might work on some ""hackers"" who would give up after a few attempts, but it won't on others, more determined ones. Do you want to protect your API from former type only, i.e. 11-year-olds? I don't think so.
quite some confusion for developers and probably testers, especially the ones who are used to operate according to global standards.
Pick some other way. There definitely are a few. I've been struggling with the same headscratcher myself for a few weeks lately and came up with at least 2 more or less reliable ways to achieve desired restrictions [cannot post them here though].

There definitely are better and MUCH more effective ways to get what you want. Try to look at your API from a different angle... If you were a hacker and your life depended on hacking that API -- what would you do to succeed? What should happen for you to be frustrated in your attempts to break it?
",1,2017-02-13 07:28:24Z,652
55,10,"
A good reason for not doing this, especially for a mobile app, is that it's very likely that your app is talking to your server via multiple proxies, for example in the phone company, already. Most large enterprises use proxies on their network to try and protect their systems from malicious content, and many phone companies reduce the quality of video or images in transit. It'll also make using the various system libraries for HTTP on your platform useless to you. One approach that's commonly used is to derive some key or token from information the user is unlikely to want to share such as hashing their name address and credit card details. This way even if your app is hacked, people are going to be wary of giving the required information to a random program they downloaded limiting the share ability of any proven attack.
",0,2017-02-12 23:07:28Z,653
55,11,"
Generally it's not a good idea.
I made very targetted use of this once to good effect when a client's website was being used by a stolen credit-card laundering ring. There were some identifying features shared only by the fraudulent transactions so rather than politely refuse them I would have the site delay the response by a couple of minutes (nobody likes a website that takes minutes to do something) and then return a 500 with the site's standard ""sorry, it's not you, it's me"" message for server errors (it also logged details for passing on to law enforcement). We had three attempts at transactions that got this playing possum response and then never heard from them again.
That though was:

In response to something we knew was an attack rather than being obnoxious to users having a problem.
Not a defence against an attack on the security of the protocol itself, but at the human level above that.
Explicable through other explanations, i.e. we were pretending to have a really sucky website in a situation where that was plausible (there are a lot of sucky websites out there) and we weren't a specific target (the perps would move on to someone else).

Users having a problem should be helped, not abused. ""I can't do that because you aren't correctly authorised"" is refusing to do something in a helpful way. ""I can't do that because you need to use a proxy"" when someone doesn't need to use a proxy is being abusive. Being deliberately unhelpful is only appropriate when you know you're being attacked and then it shouldn't look like an obviously bogus message or you haven't hidden anything (potentially you've actually revealed something if the same bogus status isn't used for every client error).
That said, it is appropriate to take measures to have statuses not leak information. E.g. it's acceptable (an described as such in the standards) for /admin/ to 404 though it would 200 in another case (authorised user, allowed client IP addresses, etc.) Alternatively if /members/my-profile will 200 for an authorised user and 403 or 401 otherwise, then if /members/fdsasafdasfwefaxc would 404 for an authorised user it's a good idea for it to 403 or 401 for the unauthorised user. This isn't so much security by obscurity as considering which URIs relate to resources to be one of the bits of information that is being protected.
",0,2017-02-13 01:57:07Z,654
56,1,"
The bottom line is that you will need to embed a secret into your app. It is an unfortunate truth that DRM (which is more or less what you are trying to achieve) is impossible. A person with access to your app will always be able to recover the embedded secret, no matter what you do to protect it.
That said, there are plenty of things you can do to make your embedded secret very difficult to recover.

Construct it at Runtime - Do not store the secret in a string or configuration file anywhere in your app. Instead derive it from a series of computations at runtime. This stops attackers from simply browsing your app with a hexeditor.
Never Send it Over the Wire - Use a challenge-response system of some kind with a >128bit nonce, that way an attacker cannot MitM the SSL stream (which is easy when he controls the mobile device remember) and see the secret in the clear.

In any case, try to find a tried and tested key-scrambling mechanism and authentication protocol. Do not roll your own.
",23,2013-09-19 16:34:14Z,658
56,2,"
Mostly you cannot check that this is ""your app"": reverse engineering works, so whoever has access to your app (e.g. he can download it on his phone) can pluck at its entrails and emulate it with his PC. @Lynks gives you some hints about how this reverse engineering can be made a more frustrating endeavour, but don't fool yourself: if the attacker is sufficiently motivated, these mechanisms will slow him down by at most a couple of days.

When there is no good answer for a question, it can be worthwhile to shift the ground so that the question changes. Here, the core problem with trying to make sure that ""your app"" is what runs on the client side is that there is such a thing as ""your app"": if somebody opens up the contents of that app, he learns everything that is to learn about all instances of that app everywhere.
So here is an attempt at a solution to your problem. Tag each app instance with a distinct value. ""Tagging"" here means that when the user downloads the app, some token is embedded into the app, and a new token value is created for each download. So that each user gets his own app with token. The idea being that the token will have to be presented to your server for account creation, and if your server sees too many requests with the same token, then it can automatically reject them -- banning that specific token.
Then, to keep on creating many accounts automatically, the attacker would have to also automate the download of fresh app instances (each with its own token value) and retrieval of the token value out of the downloaded app. Assuming that your server is at the other end of the app download, you can also detect such mass-downloading and block it from there.
For this scheme to work, you must be able to create token values on the app-download server, on demand (this might be difficult to setup with the apps installation system on existing smartphones, though), and such that the account creation server can decide whether a token value is genuine or not. HMAC is the right cryptographic tool for that.

Be aware that this kind of solution is nowhere near perfect. These are games and tricks to ""raise the bar"" for the attacker: if you design and install a system like the one I describe, then a successful attacker will have to automate the app download from many distinct IP addresses, to keep under the radar of your heuristics for detecting such things; then also automate the reverse-engineering to recover the token values (use the techniques explained by @Lynks for that); then forward the values to his bots which create accounts. All of this is still theoretically feasible: after all, human users can download the app and create accounts, and there is no way to really make the difference, from the outside, between an average human user, a chimpanzee and a bot.
But if you can make the attack sufficiently complex that the attacker deems it ""not worth the effort"", then you win.
",14,2013-09-19 21:30:42Z,608
56,3,"

If I was writing a website, I would use Captcha to stop my database from being filled up with bogus accounts via script.

So include that in the API. When a user wants to create an account:

The client requests a CAPTCHA.
The server generates a CAPTCHA, stores it, and sends a copy to the client.
The client presents the account creation form, along with the CAPTCHA.
The user fills in the form and answers the CAPTCHA.
The client requests that the new account be created, and sends the CAPTCHA answer with that request.

If the CAPTCHA answer is wrong, the server deletes its copy of the CAPTCHA (so the client can't try again) and responds with an error.


You could also use something like HashCash to make it harder for someone to mount a brute-force attack, just make sure the HashCash can be generated faster than a user could reasonably finish the form.
",3,2013-09-19 22:17:56Z,660
56,4,"
What you need is similar application authentication. It is the same concept as the authentication you do for API calls with sites like Twitter, Facebook and Foursquare. They dont allow anyone just to query their API. Instead, they generate an API key and an API secret for your app. If your application presents those keys to the server, you are allowed to use their APIs.
",0,2013-09-18 05:48:45Z,204
56,5,"
""Having your users authenticated instead of authenticate installed apps"" is more reliable.
I'll generate username/pass at the time they download the app. And ask theme to insert already given username/pass after install it. And save it for next calls to server.
So users have to insert username/pass for the first time they install the app.
In order to get rid of robots, I also get a captcha right before generating username/pass and downloading the app.
",-1,2017-10-05 07:14:16Z,661
